var documenterSearchIndex = {"docs":
[{"location":"mathematical_foundation/#Mathematical-Foundation","page":"Mathematical Foundation","title":"Mathematical Foundation","text":"Theoretical exposition of the analytical framework underlying marginal effects analysis","category":"section"},{"location":"mathematical_foundation/#Unified-Framework-for-Marginal-Effects-Analysis","page":"Mathematical Foundation","title":"Unified Framework for Marginal Effects Analysis","text":"The theoretical foundation of marginal effects analysis rests upon a systematic decomposition of the inferential problem into two fundamental analytical dimensions. This framework addresses the terminological inconsistencies that have historically impeded methodological clarity across econometric and statistical disciplines through the establishment of a unified conceptual structure.\n\nThe analytical framework distinguishes between two orthogonal methodological choices that completely characterize the space of marginal effects approaches. The evaluation context determines whether inference targets population-level parameters or profile-specific estimates, while the analytical target specifies whether the quantity of interest represents marginal effects or adjusted predictions.","category":"section"},{"location":"mathematical_foundation/#Complete-Methodological-Taxonomy","page":"Mathematical Foundation","title":"Complete Methodological Taxonomy","text":" Effects Analysis Predictions Analysis\nProfile Context Marginal Effects at Representative Scenarios Adjusted Predictions at Representative Scenarios\nPopulation Context Average Marginal Effects Average Adjusted Predictions\n\nThis taxonomic structure eliminates the ambiguity inherent in discipline-specific terminological conventions while preserving the essential methodological distinctions that govern appropriate analytical application.","category":"section"},{"location":"mathematical_foundation/#Cross-Disciplinary-Terminological-Unification","page":"Mathematical Foundation","title":"Cross-Disciplinary Terminological Unification","text":"The proposed framework establishes a systematic correspondence between the established terminology employed across econometric, biostatistical, and computational disciplines. This mapping preserves the essential methodological content while eliminating the terminological inconsistencies that have impeded interdisciplinary communication and methodological clarity.","category":"section"},{"location":"mathematical_foundation/#Profile-Context-Methodologies","page":"Mathematical Foundation","title":"Profile Context Methodologies","text":"The profile evaluation context encompasses analytical approaches that evaluate marginal quantities at representative or theoretically motivated points in the covariate space. Traditional terminological variants include Marginal Effects at the Mean and Adjusted Predictions at the Mean, which are unified under the profile effects and profile predictions categories respectively.","category":"section"},{"location":"mathematical_foundation/#Population-Context-Methodologies","page":"Mathematical Foundation","title":"Population Context Methodologies","text":"The population evaluation context encompasses approaches that compute marginal quantities averaged across the empirical distribution of observed covariates. This category unifies Average Marginal Effects, Average Partial Effects, and related population-averaged measures under a coherent methodological framework that emphasizes the distributional basis of the inference.","category":"section"},{"location":"mathematical_foundation/#Formal-Mathematical-Specification","page":"Mathematical Foundation","title":"Formal Mathematical Specification","text":"","category":"section"},{"location":"mathematical_foundation/#Marginal-Effects-and-Adjusted-Predictions","page":"Mathematical Foundation","title":"Marginal Effects and Adjusted Predictions","text":"Conceptual Foundation: Marginal effects quantify the expected change in an outcome variable resulting from a unit change in an explanatory variable, holding all other variables constant. This fundamental concept addresses the analytical question \"How does the dependent variable respond to marginal changes in specific covariates?\" The formal mathematical representation ∂E[Y|X]/∂X captures the instantaneous rate of change in the conditional expectation, providing a rigorous framework for quantifying covariate effects.\n\nMarginal effects analysis concerns the derivative of the conditional expectation function with respect to the covariates of interest. For continuous covariates, this quantity is formally defined as ∂E[Y|X]/∂X, representing the instantaneous rate of change in the expected outcome with respect to marginal changes in the explanatory variable. The interpretation centers on the magnitude of response in the dependent variable per unit increase in the independent variable, with measurement units corresponding to the dependent variable scaled by the units of the explanatory variable.\n\nAdjusted predictions represent the conditional expectation E[Y|X] evaluated at specified covariate configurations. This quantity provides the expected value of the outcome variable conditional on particular covariate realizations, maintaining the same units as the dependent variable. The analytical focus shifts from rates of change to levels of the outcome under specific conditioning scenarios.","category":"section"},{"location":"mathematical_foundation/#Evaluation-Context-Specifications","page":"Mathematical Foundation","title":"Evaluation Context Specifications","text":"Analytical Perspective: The choice between profile and population approaches reflects two different ways to approach marginal effects analysis. Profile analysis examines effects for representative individuals or scenarios (analogous to asking \"What happens for a typical 35-year-old college graduate?\"), while population analysis characterizes average effects across the entire sample distribution (analogous to asking \"What happens on average across all individuals in our dataset?\"). This distinction determines both the interpretive scope and the computational approach of the marginal effects analysis.\n\nProfile approaches evaluate marginal quantities at predetermined points in the covariate space, most commonly at sample means X̄ or at theoretically motivated scenario specifications. This approach yields concrete, interpretable estimates for specific covariate combinations, facilitating clear communication of results and policy implications for particular demographic or economic profiles. The limitation lies in the potential lack of representativeness relative to the broader population distribution.\n\nPopulation approaches compute marginal quantities averaged across the empirical distribution of observed covariates, weighting each observation according to its sample frequency. This methodology yields population-averaged parameters that reflect the heterogeneity present in the data generating process, providing estimates that characterize the broader population represented by the sample. The limitation concerns the potential difficulty in interpreting results that may not correspond to any particular individual or realistic scenario within the population.","category":"section"},{"location":"mathematical_foundation/#Methodological-Inconsistencies-Across-Disciplines","page":"Mathematical Foundation","title":"Methodological Inconsistencies Across Disciplines","text":"Why This Matters for Users: Different fields use related but slightly different terminology:\n\nEconomics: \"Average Marginal Effects\" (AME) - mean of individual-level derivatives\nBiostatistics: \"Average Partial Effects\" (APE) - often equivalent to AME, with some nuance for discrete variables\nMachine Learning: \"Partial Dependence\" - conceptually similar, typically in visualization contexts\n\nWhile these concepts overlap substantially, each field may have specific conventions for handling categorical variables or estimation approaches. This package provides a unified framework that works consistently across disciplines.\n\nThe absence of standardized terminological conventions across quantitative disciplines has generated substantial methodological confusion that impedes both theoretical development and practical implementation. This inconsistency manifests through the use of discipline-specific acronyms and conceptual frameworks that obscure the underlying mathematical equivalence of analytical approaches.","category":"section"},{"location":"mathematical_foundation/#Disciplinary-Terminological-Variations","page":"Mathematical Foundation","title":"Disciplinary Terminological Variations","text":"Economics employs Marginal Effects at the Mean and Average Marginal Effects as primary analytical categories, while biostatistics utilizes Average Partial Effects to denote methodologically identical procedures. Machine learning contexts frequently employ \"partial effects\" as an umbrella term that may conflate distinct analytical approaches including SHAP values and traditional marginal effects. Statistical software packages compound these inconsistencies through accommodation of diverse terminological preferences across user communities.","category":"section"},{"location":"mathematical_foundation/#Fundamental-Methodological-Impediments","page":"Mathematical Foundation","title":"Fundamental Methodological Impediments","text":"The proliferation of terminological variants generates several significant barriers to methodological advancement. Identical analytical concepts receive different nomenclature across disciplines, while the same terminological constructs may denote different methodological approaches in alternative contexts. The distinction between predictions and effects becomes obscured through inconsistent usage patterns, and the conceptual difference between evaluation at representative points versus population averaging receives inadequate attention.\n\nThese inconsistencies hamper reproducibility across research contexts, create unnecessary learning barriers for practitioners attempting to apply methods across disciplines, and impede productive interdisciplinary collaboration through the introduction of artificial communication barriers.","category":"section"},{"location":"mathematical_foundation/#Statistical-vs-Causal-Interpretation","page":"Mathematical Foundation","title":"Statistical vs Causal Interpretation","text":"Statistical vs Causal Interpretation: The same marginal effects computation supports both descriptive and causal analysis:\n\nDescriptive: \"In our sample, education is associated with higher wages\" \nCausal: \"Education causes higher wages\" (requires identifying assumptions)\n\nThe package provides statistically valid estimates; interpretation depends on research design and identifying assumptions.\n\nThe 2×2 framework applies equally to both descriptive and causal analysis. The mathematical operations are identical, but the interpretation differs:","category":"section"},{"location":"mathematical_foundation/#Descriptive-Interpretation-(Always-Valid)","page":"Mathematical Foundation","title":"Descriptive Interpretation (Always Valid)","text":"Question: \"How does Y vary with X in the observed data?\"\nRequirements: Only requires correct model specification\nExample: \"In our sample, each additional year of education is associated with 8% higher wages\"","category":"section"},{"location":"mathematical_foundation/#Causal-Interpretation-(Requires-Additional-Assumptions)","page":"Mathematical Foundation","title":"Causal Interpretation (Requires Additional Assumptions)","text":"Question: \"What would happen to Y if we intervened to change X?\"\nRequirements: Exogeneity, correct functional form, no omitted variables, etc.\nExample: \"Increasing education by one year would cause 8% higher wages\"\n\nImportant: The choice of profile vs population is orthogonal to causal identification. Both MEM and AME can be interpreted causally or descriptively, depending on research design.","category":"section"},{"location":"mathematical_foundation/#When-Profile-Population","page":"Mathematical Foundation","title":"When Profile ≠ Population","text":"The choice between profile and population approaches matters most when the link function is non-identity:","category":"section"},{"location":"mathematical_foundation/#Linear-Models-with-Identity-Link","page":"Mathematical Foundation","title":"Linear Models with Identity Link","text":"Profile = Population: MEM = AME for effects (predictions differ only by constants)\nPractical implication: Choice mainly affects interpretation, not numerical results","category":"section"},{"location":"mathematical_foundation/#GLMs-with-Non-Identity-Links","page":"Mathematical Foundation","title":"GLMs with Non-Identity Links","text":"Profile ≠ Population: All quantities can differ substantially\nWith interactions: Profile vs Population can yield opposite conclusions  \nHeterogeneous samples: Larger differences between approaches","category":"section"},{"location":"mathematical_foundation/#Example:-Logistic-Regression","page":"Mathematical Foundation","title":"Example: Logistic Regression","text":"Consider education effects on probability of employment in a logistic model:\n\nProfile (MEM): \"For someone with average characteristics, +1 year education → +0.08 probability of employment\"\nPopulation (AME): \"On average across the sample, +1 year education → +0.05 probability of employment\"\n\nThe difference arises because the logistic function is nonlinear, so the derivative at the mean differs from the mean of derivatives.\n\nFor the computational implications of this choice, see Computational Architecture and Performance Guide.","category":"section"},{"location":"mathematical_foundation/#Elasticities-and-Semi-Elasticities","page":"Mathematical Foundation","title":"Elasticities and Semi-Elasticities","text":"Elasticities are transformations of marginal effects that follow the same 2×2 framework:","category":"section"},{"location":"mathematical_foundation/#Definitions","page":"Mathematical Foundation","title":"Definitions","text":"Elasticity: % change in Y per % change in X = (∂Y/∂X) × (X/Y)\nSemi-elasticity (X): % change in Y per unit change in X = (∂Y/∂X) × (1/Y)  \nSemi-elasticity (Y): Unit change in Y per % change in X = (∂Y/∂X) × X","category":"section"},{"location":"mathematical_foundation/#Framework-Application","page":"Mathematical Foundation","title":"Framework Application","text":"Profile elasticities: Calculate (∂Y/∂X) × (X̄/Ȳ) at representative values\nPopulation elasticities: Average (∂Y/∂X) × (Xᵢ/Yᵢ) across sample observations\n\nIn GLMs with non-identity links, profile elasticity ≠ average elasticity, following the same logic as marginal effects.\n\nFor detailed elasticity examples and applications to policy analysis, see Advanced Features.","category":"section"},{"location":"mathematical_foundation/#Implementation-in-Margins.jl","page":"Mathematical Foundation","title":"Implementation in Margins.jl","text":"The package implements this framework through two main functions:","category":"section"},{"location":"mathematical_foundation/#population_margins()","page":"Mathematical Foundation","title":"population_margins()","text":"Computes population-level analysis (AME/AAP equivalent):\n\n# Population average marginal effects\npopulation_margins(model, data; type=:effects)\n\n# Population average predictions  \npopulation_margins(model, data; type=:predictions)\n\n# Population average elasticities\npopulation_margins(model, data; type=:effects, measure=:elasticity)","category":"section"},{"location":"mathematical_foundation/#profile_margins()","page":"Mathematical Foundation","title":"profile_margins()","text":"Computes profile analysis (MEM/APM equivalent):\n\n# Effects at sample means\nprofile_margins(model, data, means_grid(data); type=:effects)\n\n# Effects at specific scenarios\nage_grid = cartesian_grid(age=[25, 45, 65])\nprofile_margins(model, data, age_grid; type=:effects)\n\n# Predictions at representative points\nprofile_margins(model, data, means_grid(data); type=:predictions)","category":"section"},{"location":"mathematical_foundation/#Practical-Guidelines","page":"Mathematical Foundation","title":"Practical Guidelines","text":"","category":"section"},{"location":"mathematical_foundation/#Choose-Profile-When:","page":"Mathematical Foundation","title":"Choose Profile When:","text":"Understanding specific, concrete scenarios\nCommunicating results to non-technical audiences  \nSample is relatively homogeneous\nPolicy targets specific demographic profiles","category":"section"},{"location":"mathematical_foundation/#Choose-Population-When:","page":"Mathematical Foundation","title":"Choose Population When:","text":"Estimating true population parameters\nHeterogeneity across sample is important\nBroad policy applications affecting diverse groups\nExternal validity to similar populations is goal","category":"section"},{"location":"mathematical_foundation/#The-Trade-off","page":"Mathematical Foundation","title":"The Trade-off","text":"Profile approaches: More concrete and interpretable, but may not represent population\nPopulation approaches: True average effects, but may not describe any individual well\n\n\n\nThis mathematical foundation anchors all marginal analysis in Margins.jl. For implementation details, see API Reference. For computational architecture and performance implications, see Computational Architecture and Performance Guide. For advanced applications including elasticities and robust inference, see Advanced Features.","category":"section"},{"location":"backend_selection/#Backend-Selection-Guide","page":"Backend Selection","title":"Backend Selection Guide","text":"Understanding AD and FD backends for marginal effects computation","category":"section"},{"location":"backend_selection/#Recommendation:-Use-AD-(Automatic-Differentiation)","page":"Backend Selection","title":"Recommendation: Use AD (Automatic Differentiation)","text":"TL;DR: Use backend=:ad (the default) for all marginal effects computations. It provides:\n\nZero allocation performance after warmup\nMachine precision accuracy\nDomain safety for log(), sqrt(), and other sensitive functions\n3-5x faster than FD in most cases\nAll numeric types supported (Int8, Int16, Float64, etc.)\n\n# Recommended (AD is the default):\nresult = population_margins(model, data; type=:effects)\n\n# Explicit AD specification (equivalent):\nresult = population_margins(model, data; type=:effects, backend=:ad)","category":"section"},{"location":"backend_selection/#When-Finite-Differences-(FD)-Exists","page":"Backend Selection","title":"When Finite Differences (FD) Exists","text":"The :fd backend exists for:\n\nHistorical compatibility - Legacy code using FD\nDebugging - Comparing AD vs FD results to validate correctness\nEdge cases - Rare situations where FD may be preferred\n\nImportant: FD is not recommended for new code. It was developed before efficient AD implementation and is now effectively in maintenance mode.","category":"section"},{"location":"backend_selection/#Quick-Decision-Tree","page":"Backend Selection","title":"Quick Decision Tree","text":"For all applications:\n└── Use backend=:ad (default)\n    ├── Required for: log(x), sqrt(x), 1/x, x^(1/3), etc.\n    ├── Recommended for: all other formulas\n    └── Never fails: domain-safe evaluation\n\nOnly use backend=:fd if:\n├── Maintaining legacy code that explicitly uses FD\n└── Debugging/validation (comparing AD vs FD results)","category":"section"},{"location":"backend_selection/#Critical-Reliability-Differences","page":"Backend Selection","title":"Critical Reliability Differences","text":"","category":"section"},{"location":"backend_selection/#**Domain-Sensitive-Functions:-Always-Use-AD**","page":"Backend Selection","title":"Domain-Sensitive Functions: Always Use AD","text":"Functions that require backend=:ad:\n\n# Log transformations - FD can push values below zero\nmodel = lm(@formula(y ~ log(x)), data)\npopulation_margins(model, data; backend=:ad)  # Required\n\n# Square root functions - FD can push values negative  \nmodel = lm(@formula(y ~ sqrt(x)), data)  \npopulation_margins(model, data; backend=:ad)  # Required\n\n# Inverse functions near zero - FD can create division issues\nmodel = lm(@formula(y ~ 1/x), data)\npopulation_margins(model, data; backend=:ad)  # Recommended\n\n# Fractional powers - Similar domain sensitivity as sqrt\nmodel = lm(@formula(y ~ x^(1/3)), data)\npopulation_margins(model, data; backend=:ad)  # Recommended\n\nWhy FD fails: Finite difference computation f(x+h) - f(x) can push arguments outside valid domains:\n\nlog(x+h) where x+h < 0 → DomainError  \nsqrt(x+h) where x+h < 0 → DomainError\n1/(x+h) where x+h ≈ 0 → numerical instability\n\nWhy AD succeeds: Automatic differentiation computes exact derivatives without domain-violating function evaluations.","category":"section"},{"location":"backend_selection/#**Functions-Safe-for-Either-Backend**","page":"Backend Selection","title":"Functions Safe for Either Backend","text":"# Linear relationships - both backends equivalent\nmodel = lm(@formula(y ~ x + z), data)  \npopulation_margins(model, data; backend=:fd)   # Efficient performance\npopulation_margins(model, data; backend=:ad)   # Equivalent results\n\n# Polynomial functions - both work well\nmodel = lm(@formula(y ~ x + x^2), data)\npopulation_margins(model, data; backend=:fd)   # Choose based on performance needs\npopulation_margins(model, data; backend=:ad)   # Same statistical results\n\n# Simple transformations - no domain issues\nmodel = lm(@formula(y ~ x/10 + z*2), data)\npopulation_margins(model, data; backend=:fd)   # Zero allocation","category":"section"},{"location":"backend_selection/#Performance-Characteristics","page":"Backend Selection","title":"Performance Characteristics","text":"","category":"section"},{"location":"backend_selection/#Memory-Usage-Analysis","page":"Backend Selection","title":"Memory Usage Analysis","text":"Both backends now achieve zero allocation performance:\n\n# FD: Zero allocation after warmup\n@allocated population_margins(model, data_100; backend=:fd)    # 0 bytes\n@allocated population_margins(model, data_1000; backend=:fd)   # 0 bytes  \n@allocated population_margins(model, data_5000; backend=:fd)   # 0 bytes\n\n# AD: Zero allocation after warmup\n@allocated population_margins(model, data_100; backend=:ad)    # 0 bytes\n@allocated population_margins(model, data_1000; backend=:ad)   # 0 bytes\n@allocated population_margins(model, data_5000; backend=:ad)   # 0 bytes\n\nMemory Usage Decision:\n\nAll dataset sizes: Both backends achieve zero allocation performance\nChoice based on reliability and accuracy: AD provides superior domain handling\nConstruction cost: AD requires slightly more memory during evaluator setup (amortized over many evaluations)","category":"section"},{"location":"backend_selection/#Speed-Performance","page":"Backend Selection","title":"Speed Performance","text":"Both backends achieve excellent performance, with AD providing 3-5x improvements:\n\n# Typical performance ranges (varies by system and model complexity)\n# Small problems (n=100-1000)  \n@btime population_margins($model, $data; backend=:fd)  # 0.1-10ms (baseline)\n@btime population_margins($model, $data; backend=:ad)  # 0.05-5ms (3-5x faster!)\n\n# Large problems (n=10000+)\n@btime population_margins($model, $large_data; backend=:fd)  # Scales linearly with n\n@btime population_margins($model, $large_data; backend=:ad)  # Scales linearly, but with better constant factors\n\nKey insight: With zero-allocation AD, the performance differences now favor AD in most cases, while maintaining superior numerical properties.","category":"section"},{"location":"backend_selection/#Numerical-Accuracy","page":"Backend Selection","title":"Numerical Accuracy","text":"","category":"section"},{"location":"backend_selection/#Both-Backends-Provide-Equivalent-Accuracy","page":"Backend Selection","title":"Both Backends Provide Equivalent Accuracy","text":"For well-conditioned problems, both backends produce statistically equivalent results:\n\n# Linear models - identical to machine precision\nfd_result = population_margins(model, data; backend=:fd)\nad_result = population_margins(model, data; backend=:ad)\n\nDataFrame(fd_result).estimate ≈ DataFrame(ad_result).estimate  # rtol=1e-12 PASS\n\n# GLM models - equivalent within appropriate tolerances  \nfd_glm = population_margins(glm_model, data; backend=:fd)\nad_glm = population_margins(glm_model, data; backend=:ad)\n\nDataFrame(fd_glm).estimate ≈ DataFrame(ad_glm).estimate  # rtol=1e-10 PASS","category":"section"},{"location":"backend_selection/#AD-May-Be-More-Accurate-For","page":"Backend Selection","title":"AD May Be More Accurate For","text":"Complex function compositions\nFunctions with steep gradients  \nNear-boundary evaluations\nModels with numerical conditioning issues","category":"section"},{"location":"backend_selection/#Production-Recommendations","page":"Backend Selection","title":"Production Recommendations","text":"","category":"section"},{"location":"backend_selection/#Backend-Selection-Policy","page":"Backend Selection","title":"Backend Selection Policy","text":"No :auto mode is provided.\nNo implicit backend fallbacks are performed.\nSelect backend explicitly. Use :ad by default; use :fd only when explicitly intended and theoretically safe.","category":"section"},{"location":"backend_selection/#Backend-Selection-by-Use-Case","page":"Backend Selection","title":"Backend Selection by Use Case","text":"Use Case Backend Rationale\nDomain-sensitive functions (log, sqrt, 1/x) :ad Required - FD fails with DomainError\nGeneral production workflows :ad Zero allocation + reliability + 3-5x faster\nLarge datasets (>10k observations) :ad Zero allocation + superior performance\nDevelopment/testing :ad Higher reliability + machine precision\nHigh-precision requirements :ad Exact derivatives vs numerical approximation\nLegacy code maintenance :fd Only if existing code explicitly uses FD\nDebugging/validation Both Compare results to verify correctness","category":"section"},{"location":"backend_selection/#Production-Configuration-Guidance","page":"Backend Selection","title":"Production Configuration Guidance","text":"Default to backend=:ad for reliability and accuracy (also zero allocation).\nUse backend=:fd only for simple, well-conditioned formulas and when you explicitly want FD.\nFor domain-sensitive functions (log, sqrt, 1/x near 0), always use :ad.","category":"section"},{"location":"backend_selection/#Troubleshooting-Backend-Issues","page":"Backend Selection","title":"Troubleshooting Backend Issues","text":"","category":"section"},{"location":"backend_selection/#Common-Error-Patterns","page":"Backend Selection","title":"Common Error Patterns","text":"","category":"section"},{"location":"backend_selection/#DomainError-with-FD-Backend","page":"Backend Selection","title":"DomainError with FD Backend","text":"# Error: DomainError with -1.23e-6: log was called with a negative real number\nresult = population_margins(model, data; backend=:fd)  #  Fails\n\n# Solution: Use AD backend for log functions\nresult = population_margins(model, data; backend=:ad)  # Works","category":"section"},{"location":"backend_selection/#Memory-Pressure-with-AD-Backend-(Obsolete)","page":"Backend Selection","title":"~~Memory Pressure with AD Backend~~ (Obsolete)","text":"Note: This troubleshooting section is obsolete as of v2.0. Both AD and FD achieve zero allocation performance, so there is no memory efficiency difference between backends. If you encounter memory issues, they are likely related to dataset size or model complexity, not the backend choice.","category":"section"},{"location":"backend_selection/#Backend-Validation-Testing","page":"Backend Selection","title":"Backend Validation Testing","text":"# Test both backends for new functions\nfunction test_backend_compatibility(model, data)\n    try_fd = try population_margins(model, data; backend=:fd) catch nothing end\n    try_ad = try population_margins(model, data; backend=:ad) catch nothing end\n    \n    if try_fd === nothing && try_ad !== nothing\n        @warn \"Function requires AD backend - FD fails with domain error\"\n        return :ad_required\n    elseif try_fd !== nothing && try_ad !== nothing\n        # Compare results for consistency\n        fd_est = DataFrame(try_fd).estimate\n        ad_est = DataFrame(try_ad).estimate\n        \n        if fd_est ≈ ad_est rtol=1e-10\n            @info \"Both backends produce consistent results\"\n            return :either_ok\n        else\n            @warn \"Backends produce different results - investigate numerical issues\"\n            return :inconsistent\n        end\n    else\n        @error \"Both backends failed\"\n        return :both_failed\n    end\nend\n\n# Usage\ncompatibility = test_backend_compatibility(model, data)","category":"section"},{"location":"backend_selection/#Advanced-Topics","page":"Backend Selection","title":"Advanced Topics","text":"","category":"section"},{"location":"backend_selection/#FormulaCompiler-Integration","page":"Backend Selection","title":"FormulaCompiler Integration","text":"Both backends leverage FormulaCompiler.jl's optimized evaluation:\n\n# FD: Uses finite difference approximation with compiled evaluators\n# - Zero allocation after warmup\n# - Reuses pre-allocated buffers\n# - Scalar operations avoid broadcast allocations\n\n# AD: Uses dual number arithmetic with compiled evaluators (OPTIMIZED)\n# - Zero allocation after warmup via pre-conversion strategy\n# - Exact derivative computation with machine precision\n# - 3-5x performance improvement over previous AD implementation\n# - Composition via chain rule with type homogeneity","category":"section"},{"location":"backend_selection/#Custom-Tolerance-Settings","page":"Backend Selection","title":"Custom Tolerance Settings","text":"For functions near domain boundaries, you may need custom tolerances:\n\n# Custom finite difference step size (advanced)\n# Note: This is a FormulaCompiler.jl setting, not directly exposed in Margins.jl\n# Contact maintainers if you need custom FD step sizes for specific functions","category":"section"},{"location":"backend_selection/#Summary-Guidelines","page":"Backend Selection","title":"Summary Guidelines","text":"","category":"section"},{"location":"backend_selection/#**Default-Strategy-(Recommended):**","page":"Backend Selection","title":"Default Strategy (Recommended):","text":"Use backend=:ad for everything. The AD backend is now the recommended default for all use cases, providing:\n\nZero allocation performance (equal to FD)\nSuperior speed (3-5x faster than FD)\nDomain safety (handles log, sqrt, 1/x correctly)\nMachine precision accuracy\nStatistical validity","category":"section"},{"location":"backend_selection/#**When-to-Use-FD:**","page":"Backend Selection","title":"When to Use FD:","text":"Only use backend=:fd for:\n\nLegacy compatibility - Maintaining existing code that explicitly uses FD\nValidation - Comparing AD vs FD results for debugging\nVery rare edge cases - Contact maintainers if you believe you need FD for a new use case\n\nImportant: FD is not faster, not more memory-efficient, and less reliable than AD in v2.0+. There is no performance or memory reason to prefer FD for new code.","category":"section"},{"location":"backend_selection/#**Statistical-Guarantees:**","page":"Backend Selection","title":"Statistical Guarantees:","text":"Both backends maintain statistical correctness when they succeed:\n\nSame delta-method standard errors (when computed successfully)\nSame marginal effect estimates (when numerically stable)  \nSame confidence intervals and hypothesis tests\n\nThe reliability difference is in computational robustness, not statistical validity.\n\n\n\nFor performance optimization details, see Performance Guide. For mathematical background, see Mathematical Foundation.","category":"section"},{"location":"comparison/#Margins.jl-in-Context:-A-Comparison-with-Other-Marginal-Effects-Packages","page":"Package Comparison","title":"Margins.jl in Context: A Comparison with Other Marginal Effects Packages","text":"This document provides a comprehensive assessment of how Margins.jl compares to other marginal effects implementations across languages. It serves as both a technical comparison and a migration guide for researchers familiar with other tools.\n\nKey sections:\n\nStata Migration Guide - Direct command translations and examples for Stata users\nFeature Comparisons - Detailed capability comparisons across packages\nPerformance Analysis - Computational approach differences and implications\nTechnical Details - Mathematical rigor and implementation approaches","category":"section"},{"location":"comparison/#Stata-Migration-Guide","page":"Package Comparison","title":"Stata Migration Guide","text":"For researchers familiar with Stata's margins command, Margins.jl provides equivalent functionality with a clean conceptual mapping. The key difference is that Margins.jl separates the choice of where to evaluate (population vs profile) from what to compute (effects vs predictions).","category":"section"},{"location":"comparison/#Basic-Command-Translation","page":"Package Comparison","title":"Basic Command Translation","text":"Stata Command Margins.jl Equivalent Notes\nmargins, dydx(*) population_margins(model, data; type=:effects) Average marginal effects (AME)\nmargins, at(means) dydx(*) profile_margins(model, data, means_grid(data); type=:effects) Marginal effects at means (MEM)\nmargins population_margins(model, data; type=:predictions) Average adjusted predictions\nmargins, at(means) profile_margins(model, data, means_grid(data); type=:predictions) Adjusted predictions at means","category":"section"},{"location":"comparison/#Advanced-Command-Translation","page":"Package Comparison","title":"Advanced Command Translation","text":"Stata Command Margins.jl Equivalent Notes\nmargins, at(x=0 x=1 x=2) profile_margins(model, data, cartesian_grid(x=[0,1,2]); type=:predictions) Multiple evaluation points\nmargins, at(x=0 z=1) at(x=1 z=2) profile_margins(model, data, DataFrame(x=[0,1], z=[1,2]); type=:predictions) Custom scenarios\nmargins, over(group) population_margins(model, data; groups=:group) Subgroup analysis\nmargins, dydx(x) at(z=(0 1)) profile_margins(model, data, cartesian_grid(z=[0,1]); type=:effects, vars=[:x]) Specific variables","category":"section"},{"location":"comparison/#Elasticity-Commands","page":"Package Comparison","title":"Elasticity Commands","text":"Stata Command Margins.jl Equivalent Notes\nmargins, eyex(*) population_margins(model, data; type=:effects, measure=:elasticity) Elasticities\nmargins, eydx(*) population_margins(model, data; type=:effects, measure=:semielasticity_eydx) Y semi-elasticity\nmargins, dyex(*) population_margins(model, data; type=:effects, measure=:semielasticity_dyex) X semi-elasticity","category":"section"},{"location":"comparison/#Conceptual-Differences","page":"Package Comparison","title":"Conceptual Differences","text":"Stata approach: Single margins command with many options\n\nCombines evaluation point and computation type in complex syntax\nOptions like at(), dydx(), over() modify behavior\n\nMargins.jl approach: Separate functions for different purposes\n\npopulation_margins() for sample-wide averages (AME/AAP)\nprofile_margins() for scenario-specific analysis (MEM/APM)\nClean parameter separation: type (effects/predictions), measure (elasticity type), reference grids for scenarios","category":"section"},{"location":"comparison/#Migration-Example","page":"Package Comparison","title":"Migration Example","text":"Here's a complete example showing equivalent analysis:\n\nStata:\n\n* Fit logistic model\nlogit employed education age c.age#c.age\n\n* Average marginal effects\nmargins, dydx(*)\n\n* Effects at means\nmargins, at(means) dydx(*)\n\n* Effects at specific ages\nmargins, at(age=(25 35 45 55)) dydx(education)\n\nMargins.jl:\n\nusing Margins, GLM, DataFrames\n\n# Fit logistic model  \nmodel = glm(@formula(employed ~ education + age + age^2), data, Binomial(), LogitLink())\n\n# Average marginal effects\name = population_margins(model, data; type=:effects)\nDataFrame(ame)\n\n# Effects at means\nmem = profile_margins(model, data, means_grid(data); type=:effects) \nDataFrame(mem)\n\n# Effects at specific ages\nage_effects = profile_margins(model, data, \n    cartesian_grid(age=[25, 35, 45, 55]); \n    vars=[:education], \n    type=:effects\n)\nDataFrame(age_effects)","category":"section"},{"location":"comparison/#Migration-Considerations","page":"Package Comparison","title":"Migration Considerations","text":"Performance characteristics: Different computational approaches may affect performance, particularly for:\n\nLarge datasets (>10k observations)\nComplex profile specifications\nRepeated analysis workflows\n\nConceptual differences: The population vs profile distinction provides an alternative way to think about marginal analysis:\n\npopulation_margins() characterizes sample-wide patterns\nprofile_margins() examines specific scenarios\n\nEcosystem integration: Julia's statistical ecosystem provides:\n\nMixed models via MixedModels.jl\nRobust standard errors via CovarianceMatrices.jl\nCustom model types via StatsAPI","category":"section"},{"location":"comparison/#Design-Philosophy","page":"Package Comparison","title":"Design Philosophy","text":"Margins.jl was designed with several key principles:\n\nPerformance first: Built on FormulaCompiler.jl for zero-allocation computation paths\nConceptual clarity: Population vs Profile framework instead of statistical acronyms\nEcosystem integration: Works seamlessly with Julia's statistical ecosystem via StatsAPI\nMathematical rigor: Proper gradient computation and delta-method standard errors throughout","category":"section"},{"location":"comparison/#Feature-Comparison","page":"Package Comparison","title":"Feature Comparison","text":"","category":"section"},{"location":"comparison/#Core-Marginal-Effects-Functionality","page":"Package Comparison","title":"Core Marginal Effects Functionality","text":"Feature Margins.jl Effects.jl R margins Stata margins Python statsmodels\nPopulation marginal effects (AME)  ✗   \nProfile marginal effects (MEM/MER)     (at='mean' only)\nElasticities (3 types) ✗ ✗ (basic) ✗\nFlexible profile specification (Dict + table) (kwargs) (at=) (at()) ✗\nCategorical contrasts (baseline/pairwise) (basic) (basic)  (basic)\nGrouping/stratification (over/within/by) ✗ (basic) (over/by) ✗\nObservation weights  ✗   \nRobust standard errors (via CovarianceMatrices) (via vcov) (sandwich) (vce()) (limited)","category":"section"},{"location":"comparison/#Advanced-Features","page":"Package Comparison","title":"Advanced Features","text":"Feature Margins.jl Effects.jl R margins Stata margins Python statsmodels\nMixed models (automatic)  (manual)  (limited)\nMultiple backends (FD/AD)  ✗ ✗ ✗ ✗\nZero-allocation paths  ✗ ✗ ✗ ✗\nCompiled evaluation  ✗ ✗  ✗\nTable-based profiles  ✗ ✗ ✗ ✗\nMultiple comparison adjustments     (basic)","category":"section"},{"location":"comparison/#Model-Support","page":"Package Comparison","title":"Model Support","text":"Models Margins.jl Effects.jl R margins Stata margins Python statsmodels\nLinear/GLM     \nMixed effects (via StatsAPI)  (some)  (limited)\nSurvival models Future Future   \nCustom models (via StatsAPI) (via StatsAPI) Varies Limited Varies","category":"section"},{"location":"comparison/#Performance-Characteristics","page":"Package Comparison","title":"Performance Characteristics","text":"","category":"section"},{"location":"comparison/#AME-Performance-Background","page":"Package Comparison","title":"AME Performance Background","text":"Average marginal effects (AME) have traditionally been slow because most implementations evaluate the fitted model once per observation to compute numerical derivatives. This creates O(n) complexity with substantial per-evaluation overhead. For large datasets, this computational cost often led researchers to use marginal effects at means (MEM) as a faster alternative.","category":"section"},{"location":"comparison/#Computational-Approach","page":"Package Comparison","title":"Computational Approach","text":"Margins.jl: Uses compiled formula evaluation with dual FD/AD backends. In indicative benchmark results, achieves approximately 50ns per marginal effect with zero allocations after warmup via FormulaCompiler.jl. Computes derivatives directly from compiled formulas rather than repeated model prediction calls.\n\nEffects.jl: Uses automatic differentiation (ForwardDiff.jl) for gradient computation with standard Julia model prediction. Clean implementation but allocates ~400-500 bytes per gradient computation.\n\nR margins: Interpretation-based approach requiring repeated predict() calls per observation. Generally slower for large datasets due to evaluation overhead (Leeper, 2017).\n\nStata margins: Compiled implementation with good performance, but still uses interpretation-based approach. Closed-source implementation limits optimization possibilities.\n\nPython statsmodels: Mixed approach with some optimizations, but limited by Python overhead for large-scale AME computation.","category":"section"},{"location":"comparison/#Performance-Differences","page":"Package Comparison","title":"Performance Differences","text":"The key difference is computational approach:\n\nTraditional approach: n model evaluations × interpretation overhead  \nMargins.jl approach: Compiled formula evaluation (~7ns) + finite difference (~40ns) + delta-method SE (~10ns) = ~57ns per observation with zero allocations\n\nThese architectural differences may become more noticeable with larger datasets.","category":"section"},{"location":"comparison/#Benchmarking-Context","page":"Package Comparison","title":"Benchmarking Context","text":"While formal cross-language benchmarks are complex due to different implementations and ecosystems, indicative performance characteristics suggest architectural differences:\n\nSmall problems (n < 1,000): Most packages perform adequately, overhead differences typically negligible\nMedium problems (n ~ 10,000): Different architectural approaches may show varying performance characteristics\nLarge problems (n > 100,000): Architectural differences become more apparent, with allocation patterns affecting scalability\n\nZero-allocation approaches can help AME computation scale with dataset size by avoiding additional memory allocation.","category":"section"},{"location":"comparison/#API-Design-Comparison","page":"Package Comparison","title":"API Design Comparison","text":"","category":"section"},{"location":"comparison/#Conceptual-Framework","page":"Package Comparison","title":"Conceptual Framework","text":"Margins.jl: Uses Population vs Profile framework with orthogonal parameters:\n\npopulation_margins(model, data; type=:effects, measure=:elasticity)\nprofile_margins(model, data, means_grid(data); type=:effects)\n\nEffects.jl: Function-based approach with keyword arguments:\n\neffects(model; x1=0.5, x2=[0, 1])  # At specific values\neffects(model, data)               # Average marginal effects\n\nR margins: Function-based approach with extensive options:\n\nmargins(model, data, at = list(x = c(-1, 0, 1)))\n\nStata margins: Command-based with extensive syntax:\n\nmargins, at(x=(-1 0 1)) dydx(x)\n\nPython statsmodels: Method-based approach:\n\nmarginal_effects = model.get_margeff(at='mean')","category":"section"},{"location":"comparison/#Strengths-and-Tradeoffs","page":"Package Comparison","title":"Strengths and Tradeoffs","text":"Margins.jl characteristics:\n\nConceptual separation of population vs profile approaches\nType-stable, composable design\nInterface consistent across model types\nElasticity support across multiple measures\nFocus on zero-allocation performance\n\nEffects.jl characteristics:\n\nSimple, focused API\nAutomatic differentiation integration\nLightweight implementation\nActive development\n\nMargins.jl limitations:\n\nNewer package with smaller user base\nJulia ecosystem required\nSome Stata-specific syntax conveniences not implemented\n\nEffects.jl limitations:\n\nMore limited feature set (no observation weights, limited grouping)\nLess performance optimization\nFewer advanced statistical features","category":"section"},{"location":"comparison/#Ecosystem-Integration","page":"Package Comparison","title":"Ecosystem Integration","text":"","category":"section"},{"location":"comparison/#Language-Ecosystems","page":"Package Comparison","title":"Language Ecosystems","text":"Margins.jl: Integrates with Julia's statistical ecosystem via StatsAPI. Models that implement the standard interface work with the package, including mixed models and custom models.\n\nEffects.jl: Also uses StatsAPI for integration, works well with standard Julia statistical models. More lightweight approach with fewer dependencies.\n\nR margins: Good integration with R's modeling ecosystem, though sometimes requires package-specific implementations.\n\nStata margins: Excellent integration within Stata's ecosystem, but limited extensibility.\n\nPython statsmodels: Part of the statsmodels ecosystem with good integration there, more limited beyond.","category":"section"},{"location":"comparison/#Mathematical-Rigor","page":"Package Comparison","title":"Mathematical Rigor","text":"","category":"section"},{"location":"comparison/#Gradient-Computation","page":"Package Comparison","title":"Gradient Computation","text":"Margins.jl: Uses gradient computation with delta-method standard errors. Provides both finite differences and automatic differentiation backends.\n\nOther packages: Use various approaches to derivative computation, with implementation details varying by package and language ecosystem.","category":"section"},{"location":"comparison/#Standard-Error-Computation","page":"Package Comparison","title":"Standard Error Computation","text":"Most packages implement delta-method standard errors, with different approaches to optimization and numerical accuracy.","category":"section"},{"location":"comparison/#Margins.jl-Approach","page":"Package Comparison","title":"Margins.jl Approach","text":"Margins.jl brings several distinctive features to marginal effects analysis:\n\nComputational approach: Uses compiled formula evaluation with zero-allocation performance paths, which in indicative benchmarks can be beneficial for large-scale analysis and repeated computations.\n\nConceptual framework: Separates the question of where to evaluate (population vs profile) from what to compute (effects vs predictions), providing conceptual clarity in analysis design.\n\nElasticity support: Provides comprehensive elasticity computation including standard elasticities, x-semi-elasticities, and y-semi-elasticities across both population and profile approaches.\n\nFor population-level counterfactuals, see Population Scenarios. For profile evaluation at specified covariate combinations, see Reference Grids.\n\nEcosystem integration: Works within Julia's statistical ecosystem, supporting mixed models, robust standard errors, and custom model types through common interfaces.","category":"section"},{"location":"comparison/#Future-Development","page":"Package Comparison","title":"Future Development","text":"Margins.jl development benefits from Julia's evolving ecosystem. Relevant factors for future development include:\n\nLanguage characteristics: Julia's design enables certain types of optimizations\nEcosystem integration: Statistical methods can integrate via common interfaces\nDevelopment activity: Both Margins.jl and FormulaCompiler.jl continue active development","category":"section"},{"location":"comparison/#Ecosystem-Maturity-Considerations","page":"Package Comparison","title":"Ecosystem Maturity Considerations","text":"An important factor in package selection is ecosystem maturity and validation in applied settings:\n\nEstablished packages (Stata margins, R margins): These have extensive validation through years of use in econometric research and applied statistics. Their implementations have been tested across diverse research contexts and publication processes.\n\nJulia ecosystem: While Julia's statistical ecosystem is rapidly maturing, it represents a more experimental environment. Packages like Margins.jl and Effects.jl are newer and have smaller user bases compared to established tools in Stata and R.\n\nPractical implications:\n\nFor high-stakes research requiring maximum confidence in implementation, established packages may be preferable\nFor research prioritizing computational performance or new methodological approaches, newer Julia packages may offer advantages\nCross-validation of results across packages can provide additional confidence when using newer implementations","category":"section"},{"location":"comparison/#Conclusion","page":"Package Comparison","title":"Conclusion","text":"Margins.jl provides an alternative approach with different performance characteristics and API design. Being newer than established packages, it represents a different set of tradeoffs in the marginal effects landscape.\n\nPackage selection depends on specific requirements, use cases, and tolerance for ecosystem maturity differences. Margins.jl offers a different approach to marginal effects computation that may be beneficial in certain contexts, while established packages provide proven reliability in applied research settings.","category":"section"},{"location":"reference_grids/#Reference-Grid-Methodology-and-Implementation","page":"Reference Grids","title":"Reference Grid Methodology and Implementation","text":"Reference Grids  A reference grid is simply a table that says \"compute effects for people with these specific characteristics.\" For example:\n\nAge: 30, Education: College → \"What's the effect for 30-year-old college graduates?\"\nAge: 40, Education: High School → \"What's the effect for 40-year-old high school graduates?\"\n\nThe package provides helper functions to create these tables automatically.\n\nReference grid specification constitutes the methodological foundation for covariate scenario definition in profile-based marginal effects analysis. The implementation provides a systematic framework for scenario specification through structured builder functions and direct tabular specification interfaces.","category":"section"},{"location":"reference_grids/#Methodological-Foundation","page":"Reference Grids","title":"Methodological Foundation","text":"The analytical framework employs explicit reference grid specification to ensure transparency and computational precision:\n\nprofile_margins(model, data, reference_grid; type=:effects, ...)\n\nThe reference_grid parameter accepts DataFrame specifications that enumerate the covariate combinations where marginal effects are computed.","category":"section"},{"location":"reference_grids/#Reference-Grid-Builders","page":"Reference Grids","title":"Reference Grid Builders","text":"","category":"section"},{"location":"reference_grids/#1.-Sample-Means-means_grid(data)","page":"Reference Grids","title":"1. Sample Means - means_grid(data)","text":"Creates reference grid with sample means for continuous variables and frequency-weighted mixtures for categorical variables:\n\n# Build grid with realistic defaults\ngrid = means_grid(data)\nresult = profile_margins(model, data, grid; type=:effects)\n\n# Custom typical value function (default is mean)\ngrid = means_grid(data; typical=median)\nresult = profile_margins(model, data, grid; type=:effects)\n\nOutput structure:\n\nContinuous variables: Sample mean (or custom typical function)\nCategorical variables: Frequency-weighted mixture based on actual data distribution\nBool variables: Probability of true (proportion of true values)","category":"section"},{"location":"reference_grids/#2.-Cartesian-Product-cartesian_grid(vars...)","page":"Reference Grids","title":"2. Cartesian Product - cartesian_grid(vars...)","text":"Creates all combinations of specified values across variables:\n\n# 3×2 = 6 scenarios: all combinations of x and education values\ngrid = cartesian_grid(x=[-1, 0, 1], education=[\"High School\", \"College\"])\nresult = profile_margins(model, data, grid; type=:effects)\n\n# Single variable varying, others at typical values\ngrid = cartesian_grid(age=20:10:70)\nresult = profile_margins(model, data, grid; type=:predictions)\n\n# Complex scenarios with multiple variables\ngrid = cartesian_grid(\n    income=[25000, 50000, 75000],\n    education=[\"HS\", \"College\"],\n    region=[\"North\", \"South\"]\n)  # Creates 3×2×2 = 12 scenarios\nresult = profile_margins(model, data, grid; type=:effects)","category":"section"},{"location":"reference_grids/#3.-Balanced-Factorial-balanced_grid(data;-vars...)","page":"Reference Grids","title":"3. Balanced Factorial - balanced_grid(data; vars...)","text":"Creates balanced (equal-weight) mixtures for categorical variables, useful for orthogonal factorial designs:\n\n# Balanced factorial for categorical variables\ngrid = balanced_grid(data; education=:all, region=:all)\nresult = profile_margins(model, data, grid; type=:effects)\n\n# Mixed specification\ngrid = balanced_grid(data; \n    education=:all,           # All levels with equal weight\n    income=mean(data.income)  # Fixed at mean\n)\nresult = profile_margins(model, data, grid; type=:effects)","category":"section"},{"location":"reference_grids/#4.-Quantile-Based-quantile_grid(data;-vars...)","page":"Reference Grids","title":"4. Quantile-Based - quantile_grid(data; vars...)","text":"Uses quantiles of continuous variables:\n\n# Effects at income quartiles\ngrid = quantile_grid(data; income=[0.25, 0.5, 0.75])\nresult = profile_margins(model, data, grid; type=:effects)\n\n# Multiple quantile specifications\ngrid = quantile_grid(data; \n    income=[0.1, 0.5, 0.9],\n    age=[0.25, 0.75]\n)  # Creates 3×2 = 6 scenarios\nresult = profile_margins(model, data, grid; type=:effects)","category":"section"},{"location":"reference_grids/#5.-Hierarchical-Grammar-hierarchical_grid(data,-spec)","page":"Reference Grids","title":"5. Hierarchical Grammar - hierarchical_grid(data, spec)","text":"Creates systematic reference grids using the group nesting grammar (=> operator) for complex multi-dimensional covariate scenario construction:\n\n# Simple hierarchical: region-specific education representatives\nspec = :region => :education\ngrid = hierarchical_grid(data, spec)\nresult = profile_margins(model, data, grid; type=:effects)\n\n# Complex hierarchy with multiple representative types\nspec = :region => [\n    (:income, :quartiles),  # Income quartiles within each region\n    (:age, :mean),          # Mean age within each region  \n    :education              # All education levels within each region\n]\ngrid = hierarchical_grid(data, spec)\nresult = profile_margins(model, data, grid; type=:effects)\n\n# Deep nesting (3+ levels) with automatic safety validation\nspec = :country => (\n    :region => (\n        :education => [(:income, :quartiles), (:age, :mean)]\n    )\n)\ngrid = hierarchical_grid(data, spec; max_depth=4, warn_large=true)\nresult = profile_margins(model, data, grid; type=:effects)\n\nSafety Parameters:\n\nhierarchical_grid() includes built-in safety features to prevent accidental creation of excessively large grids:\n\nmax_depth::Int=5 - Maximum allowed nesting depth\nDefault: 5 levels\nPrevents runaway nesting that could create enormous grids\nError thrown if specification exceeds this depth\nExample: hierarchical_grid(data, deep_spec; max_depth=10) allows up to 10 levels\nwarn_large::Bool=true - Grid size warnings\nDefault: enabled (shows warnings)\nEstimates total grid size before construction\nWarns if grid will exceed 10,000 rows\nHelps catch specification errors before expensive computation\nExample: hierarchical_grid(data, spec; warn_large=false) disables warnings\n\n# Example: deeply nested specification with safety overrides\nvery_deep_spec = :country => (\n    :state => (\n        :county => (\n            :city => [:education, (:income, :quartiles)]\n        )\n    )\n)\n\n# This will error without increasing max_depth (default is 5)\n# grid = hierarchical_grid(data, very_deep_spec)  # Error: exceeds max_depth\n\n# Allow deeper nesting explicitly\ngrid = hierarchical_grid(data, very_deep_spec; max_depth=10, warn_large=true)\n\nAdvanced Representative Types:\n\n# Statistical representatives within hierarchical groups\nspec = :region => [\n    (:income, :mean),           # Mean income per region\n    (:income, :median),         # Median income per region\n    (:income, :quartiles),      # Q1, Q2, Q3, Q4 per region\n    (:income, :quintiles),      # Quintiles per region\n    (:income, :deciles),        # Deciles per region\n    (:income, [0.1, 0.5, 0.9]), # Custom percentiles per region\n    (:age, [25, 45, 65]),       # Fixed representative ages\n    (:score, (:range, 5))       # 5 evenly spaced points from min to max\n]\ngrid = hierarchical_grid(data, spec)\n\nMixture Integration:\n\n# Population-proportion mixtures for realistic scenarios\nspec = :region => [\n    (:education, :mix_proportional),  # Use actual data proportions\n    (:income, :quartiles),\n    (:age, :mean)\n]\ngrid = hierarchical_grid(data, spec)\n\n# Custom mixtures for policy analysis\nusing Margins: mix\nspec = :region => [\n    (:education, mix(\"HS\" => 0.3, \"College\" => 0.7)),  # Policy scenario\n    (:income, :median)\n]\ngrid = hierarchical_grid(data, spec)","category":"section"},{"location":"reference_grids/#Direct-DataFrame-Specification","page":"Reference Grids","title":"Direct DataFrame Specification","text":"For maximum control, create reference grids directly:\n\n# Simple custom grid\nreference_grid = DataFrame(\n    age=[25, 35, 45], \n    education=[\"High School\", \"College\", \"Graduate\"],\n    experience=[2, 8, 15],\n    treated=[true, false, true]\n)\nresult = profile_margins(model, data, reference_grid; type=:effects)\n\n# Grid with categorical mixtures\nusing Margins: mix\n\npolicy_grid = DataFrame(\n    age=[35, 45, 55],\n    education=[\n        mix(\"HS\" => 0.4, \"College\" => 0.6),        # Current composition\n        mix(\"HS\" => 0.2, \"College\" => 0.8),        # Policy scenario 1\n        mix(\"HS\" => 0.1, \"College\" => 0.9)         # Policy scenario 2\n    ]\n)\nresult = profile_margins(model, data, policy_grid; type=:predictions)","category":"section"},{"location":"reference_grids/#Advanced-Patterns","page":"Reference Grids","title":"Advanced Patterns","text":"","category":"section"},{"location":"reference_grids/#Frequency-Weighted-Defaults","page":"Reference Grids","title":"Frequency-Weighted Defaults","text":"When variables are unspecified in builder functions, they use actual data composition:\n\n# Your data composition:\n# - education: 40% HS, 45% College, 15% Graduate  \n# - region: 75% Urban, 25% Rural\n# - treated: 60% true, 40% false\n\n# Builder uses realistic defaults\ngrid = cartesian_grid(income=[30000, 50000, 70000])\n# → income varies as specified\n# → education: mix(\"HS\" => 0.4, \"College\" => 0.45, \"Graduate\" => 0.15)\n# → region: mix(\"Urban\" => 0.75, \"Rural\" => 0.25)  \n# → treated: 0.6 (probability of true)","category":"section"},{"location":"reference_grids/#Hierarchical-Policy-Analysis","page":"Reference Grids","title":"Hierarchical Policy Analysis","text":"Systematic multi-dimensional policy evaluation using hierarchical grids:\n\n# Complex policy analysis across administrative levels\npolicy_spec = :state => (\n    :county => [\n        (:education, :mix_proportional),     # Actual education composition per county\n        (:income, :quintiles),               # Income distribution per county\n        (:age, [25, 45, 65]),               # Key demographic groups\n        (:employment_status, :all)           # All employment categories\n    ]\n)\ngrid = hierarchical_grid(data, policy_spec)\nresult = profile_margins(policy_model, data, grid; vars=[:policy_treatment])\n\n# Comparative scenario analysis\nbaseline_spec = :region => [(:education, :mix_proportional), (:income, :mean)]\nintervention_spec = :region => [(:education, mix(\"HS\" => 0.2, \"College\" => 0.8)), (:income, :mean)]\n\nbaseline_grid = hierarchical_grid(data, baseline_spec)\nintervention_grid = hierarchical_grid(data, intervention_spec)\n\nbaseline_results = profile_margins(model, data, baseline_grid; type=:predictions)\nintervention_results = profile_margins(model, data, intervention_grid; type=:predictions)\n\n# Calculate policy impact\nbaseline_df = DataFrame(baseline_results)\nintervention_df = DataFrame(intervention_results)\npolicy_impact = intervention_df.estimate .- baseline_df.estimate","category":"section"},{"location":"reference_grids/#Scenario-Comparison","page":"Reference Grids","title":"Scenario Comparison","text":"Compare different policy scenarios:\n\n# Current scenario (status quo)\ncurrent_grid = means_grid(data)\ncurrent = profile_margins(model, data, current_grid; type=:predictions)\n\n# Policy scenario (increased education)\npolicy_grid = DataFrame(\n    age=mean(data.age),\n    income=mean(data.income),\n    education=mix(\"HS\" => 0.2, \"College\" => 0.5, \"Graduate\" => 0.3)  # Policy target\n)\nfuture = profile_margins(model, data, policy_grid; type=:predictions)\n\n# Compare outcomes\ncurrent_pred = DataFrame(current).estimate[1]\nfuture_pred = DataFrame(future).estimate[1]\npolicy_impact = future_pred - current_pred","category":"section"},{"location":"reference_grids/#Sequential-Analysis","page":"Reference Grids","title":"Sequential Analysis","text":"Analyze effects along ranges of key variables:\n\n# Effects across age ranges\nage_grid = cartesian_grid(age=25:5:65)\nage_effects = profile_margins(model, data, age_grid; type=:effects, vars=[:education])\n\n# Plot age-varying effects\nusing Plots\nplot(25:5:65, DataFrame(age_effects).estimate, \n     xlabel=\"Age\", ylabel=\"Education Effect\", \n     title=\"Age-Varying Education Effects\")","category":"section"},{"location":"reference_grids/#Performance-Considerations","page":"Reference Grids","title":"Performance Considerations","text":"","category":"section"},{"location":"reference_grids/#Grid-Size-and-Efficiency","page":"Reference Grids","title":"Grid Size and Efficiency","text":"Reference grid size affects performance linearly, but is independent of dataset size:\n\n# Small grid: 3 scenarios\nsmall_grid = cartesian_grid(x=[0, 1, 2])\n@time profile_margins(model, huge_data, small_grid)  # ~150μs\n\n# Large grid: 27 scenarios  \nlarge_grid = cartesian_grid(x=[0,1,2], y=[0,1,2], z=[0,1,2])\n@time profile_margins(model, huge_data, large_grid)  # ~400μs\n\n# Dataset size doesn't matter\n@time profile_margins(model, small_data, large_grid)  # Still ~400μs","category":"section"},{"location":"reference_grids/#Hierarchical-Grid-Performance","page":"Reference Grids","title":"Hierarchical Grid Performance","text":"Hierarchical grids provide automatic size estimation and safety validation:\n\n# Automatic grid size warnings for large combinations\nlarge_spec = :country => (:region => (:education => (:income, :deciles)))\n# Warning: Estimated grid size ~50,000 combinations may impact performance\ngrid = hierarchical_grid(data, large_spec; warn_large=true)\n\n# Depth protection prevents excessive nesting\ndeep_spec = :a => (:b => (:c => (:d => (:e => (:f => :g)))))\n# Error: Nesting depth 7 exceeds maximum allowed depth 5\ngrid = hierarchical_grid(data, deep_spec; max_depth=5)\n\n# Efficient construction through systematic generation\ncomplex_spec = :region => [(:income, :quartiles), (:age, :mean), :education]\n@time hierarchical_grid(data, complex_spec)  # ~50μs regardless of data size","category":"section"},{"location":"reference_grids/#Memory-Management","page":"Reference Grids","title":"Memory Management","text":"Builder functions are optimized for memory efficiency:\n\n# Efficient: builders avoid unnecessary allocations\ngrid = means_grid(large_data)  # O(1) memory for typical values\n\n# Less efficient: explicit grids require full materialization  \nexplicit_grid = DataFrame(\n    x1=fill(mean(large_data.x1), 1000),  # O(n) memory\n    x2=fill(mean(large_data.x2), 1000)\n)","category":"section"},{"location":"reference_grids/#Validation-and-Error-Handling","page":"Reference Grids","title":"Validation and Error Handling","text":"Reference grids are validated automatically:\n\n# Error: Missing model variables\nincomplete_grid = DataFrame(x1=[0, 1])  # Missing x2 from model\nprofile_margins(model, data, incomplete_grid)  \n# → ArgumentError: Missing model variables: x2\n\n# Error: Invalid categorical levels\ninvalid_grid = DataFrame(\n    x1=[0, 1], \n    group=[\"InvalidLevel\", \"AnotherInvalid\"]  # Not in original data\n)\nprofile_margins(model, data, invalid_grid)\n# → ArgumentError: Invalid levels for categorical variable 'group'\n\n# Warning: Large grid size\nhuge_grid = cartesian_grid(x=1:100, y=1:100)  # 10,000 scenarios\nprofile_margins(model, data, huge_grid)\n# → Warning: Large reference grid (10000 scenarios) may impact performance","category":"section"},{"location":"reference_grids/#Statistical-Properties","page":"Reference Grids","title":"Statistical Properties","text":"","category":"section"},{"location":"reference_grids/#Delta-Method-Standard-Errors","page":"Reference Grids","title":"Delta-Method Standard Errors","text":"Standard errors are computed consistently across all reference grid types:\n\n# Same statistical rigor regardless of grid construction method\ngrid1 = means_grid(data)\ngrid2 = DataFrame(age=mean(data.age), education=mode(data.education))\ngrid3 = cartesian_grid(age=[mean(data.age)])\n\n# All use identical delta-method computation\nresult1 = profile_margins(model, data, grid1; type=:effects)\nresult2 = profile_margins(model, data, grid2; type=:effects)  \nresult3 = profile_margins(model, data, grid3; type=:effects)\n\n# Standard errors are mathematically equivalent\nall(DataFrame(result1).se .≈ DataFrame(result2).se .≈ DataFrame(result3).se)  # true","category":"section"},{"location":"reference_grids/#Categorical-Mixture-Handling","page":"Reference Grids","title":"Categorical Mixture Handling","text":"Categorical mixtures are handled natively throughout the system:\n\n# Fractional specifications work seamlessly\nmixed_grid = DataFrame(\n    age=[35, 45],\n    treated=[0.3, mix(0 => 0.6, 1 => 0.4)]  # Mix of scalar and mixture\n)\nresult = profile_margins(model, data, mixed_grid; type=:predictions)\n\n# Standard errors account for mixture uncertainty automatically\nDataFrame(result)  # Includes proper SEs for mixed scenarios","category":"section"},{"location":"reference_grids/#Migration-Guide","page":"Reference Grids","title":"Migration Guide","text":"","category":"section"},{"location":"reference_grids/#From-Old-at-Parameter-Syntax","page":"Reference Grids","title":"From Old at Parameter Syntax","text":"# OLD (deprecated):\nprofile_margins(model, data; at=:means)\nprofile_margins(model, data; at=Dict(:x => [0,1,2]))\nprofile_margins(model, data; at=[Dict(:x => 0), Dict(:x => 1)])\n\n# NEW (current):\nprofile_margins(model, data, means_grid(data))\nprofile_margins(model, data, cartesian_grid(x=[0,1,2]))\n\nexplicit_grid = DataFrame(x=[0, 1])\nprofile_margins(model, data, explicit_grid)","category":"section"},{"location":"reference_grids/#Builder-Function-Evolution","page":"Reference Grids","title":"Builder Function Evolution","text":"# OLD (deprecated internal names):\nrefgrid_means(data)\nrefgrid_cartesian(specs, data)\n\n# NEW (exported public API):\nmeans_grid(data)\ncartesian_grid(vars...)\nbalanced_grid(data; vars...)\nquantile_grid(data; vars...)","category":"section"},{"location":"reference_grids/#Best-Practices","page":"Reference Grids","title":"Best Practices","text":"Start with means_grid() for basic analysis\nUse cartesian_grid() for systematic exploration\nUse balanced_grid() for orthogonal factorial designs\nUse quantile_grid() for distributional analysis\nUse hierarchical_grid() for complex multi-dimensional policy analysis\nUse explicit DataFrame for maximum custom control\nValidate grids with small examples before scaling up\nConsider grid size vs computational requirements\nLeverage frequency weighting for realistic defaults\nUse mixture specifications for policy counterfactual analysis\n\nSee also: profile_margins for the main function interface.","category":"section"},{"location":"performance/#Performance-Guide","page":"Performance Guide","title":"Performance Guide","text":"Computational characteristics and optimization strategies","category":"section"},{"location":"performance/#Conceptual-Framework","page":"Performance Guide","title":"Conceptual Framework","text":"","category":"section"},{"location":"performance/#Performance-Design-Principles","page":"Performance Guide","title":"Performance Design Principles","text":"Margins.jl achieves computational efficiency through architectural design that respects the fundamental mathematical structure of marginal effects analysis. Performance optimization preserves statistical correctness while exploiting the distinct computational requirements of population versus profile analysis.","category":"section"},{"location":"performance/#Algorithmic-Complexity-Characteristics","page":"Performance Guide","title":"Algorithmic Complexity Characteristics","text":"Profile Analysis: O(1) constant-time complexity independent of dataset size\nPopulation Analysis: O(n) linear scaling with optimized per-observation computational cost\nStatistical Integrity: All performance optimizations maintain mathematical validity","category":"section"},{"location":"performance/#Implementation-Performance","page":"Performance Guide","title":"Implementation Performance","text":"","category":"section"},{"location":"performance/#Performance-Characteristics","page":"Performance Guide","title":"Performance Characteristics","text":"","category":"section"},{"location":"performance/#Profile-Analysis:-O(1)-Constant-Time","page":"Performance Guide","title":"Profile Analysis: O(1) Constant Time","text":"Profile margins achieve constant-time performance regardless of dataset size:\n\nusing BenchmarkTools, Margins\n\n# Performance is independent of dataset size\n@btime profile_margins($model, $data_1k, means_grid($data_1k); type=:effects)     # constant time\n@btime profile_margins($model, $data_100k, means_grid($data_100k); type=:effects) # same complexity\n@btime profile_margins($model, data_1M, means_grid(data_1M); type=:effects)       # same complexity\n\n# Complex scenarios also O(1)\nscenarios = cartesian_grid(x1=[0,1,2], x2=[10,20,30], group=[\"A\",\"B\"])  # 18 profiles\n@btime profile_margins($model, $huge_data, scenarios)                       # still constant time\n\nWhy this matters: Profile analysis cost is independent of sample size, making it efficient for large-scale econometric analysis.","category":"section"},{"location":"performance/#Population-Analysis:-Optimized-O(n)-Scaling","page":"Performance Guide","title":"Population Analysis: Optimized O(n) Scaling","text":"Population margins scale linearly with optimized per-row costs:\n\n# Linear scaling with low per-row computational cost\n@btime population_margins($model, $data_1k)    # scales with dataset size\n@btime population_margins($model, $data_10k)   # with efficient per-row processing  \n@btime population_margins($model, $data_100k)  # minimal allocation overhead\n\n# Memory footprint remains zero (both backends)\n@allocated population_margins(model, data_1k; backend=:fd)    # 0 bytes\n@allocated population_margins(model, data_10k; backend=:fd)   # 0 bytes\n@allocated population_margins(model, data_100k; backend=:fd)  # 0 bytes\n\n@allocated population_margins(model, data_1k; backend=:ad)    # 0 bytes\n@allocated population_margins(model, data_10k; backend=:ad)   # 0 bytes\n@allocated population_margins(model, data_100k; backend=:ad)  # 0 bytes\n\nWhy this matters: Population analysis maintains constant allocation footprint while delivering consistent per-row performance.","category":"section"},{"location":"performance/#Dataset-Size-Guidelines","page":"Performance Guide","title":"Dataset Size Guidelines","text":"","category":"section"},{"location":"performance/#Performance-Expectations-by-Scale","page":"Performance Guide","title":"Performance Expectations by Scale","text":"Dataset Size Population Margins Profile Margins Recommended Workflow\n< 1k Fast Constant time Use either approach freely\n1k-10k Fast Constant time Profile preferred for scenarios\n10k-100k Scales linearly Constant time Profile for exploration, population for final analysis\n100k-1M Scales appropriately Constant time Profile strongly preferred\n> 1M Scales with dataset size Constant time Profile analysis, selective population","category":"section"},{"location":"performance/#Backend-Selection-by-Use-Case","page":"Performance Guide","title":"Backend Selection by Use Case","text":"For detailed backend selection guidance including domain-sensitive functions and reliability considerations, see Backend Selection Guide.\n\nQuick summary:\n\n:ad - Recommended default; machine-precision accuracy, zero allocation, handles all functions\n:fd - Alternative; zero allocation, numerical approximation, good for simple formulas\n\n# Recommended configuration\npopulation_margins(model, data; backend=:ad, scale=:response)\n\n# Profile analysis (also uses :ad by default)\nprofile_margins(model, data, means_grid(data); backend=:ad, scale=:response)\n\n# Domain-sensitive functions (log, sqrt) - AD recommended\npopulation_margins(model, data; backend=:ad)  # Recommended for log(x), sqrt(x)","category":"section"},{"location":"performance/#Optimization-Principles","page":"Performance Guide","title":"Optimization Principles","text":"","category":"section"},{"location":"performance/#Core-Performance-Philosophy","page":"Performance Guide","title":"Core Performance Philosophy","text":"Statistical Correctness First: Performance optimizations maintain statistical validity\n\nDelta-method standard errors use full covariance matrix\nAll gradient computations maintain mathematical precision\nBootstrap validation ensures statistical accuracy\nNever change estimators, gradients, or SE math to \"optimize\"\n\nZero-Allocation Patterns: Eliminate unnecessary memory allocations\n\nPre-allocated buffers reused across computations\nFormulaCompiler.jl provides zero-allocation evaluation primitives\nConstant memory footprint regardless of dataset size\nO(1) allocations in production paths: constant allocation count w.r.t. sample size\n\nComputational Efficiency: Optimize hot paths without changing methodology\n\nCompiled formula evaluation with caching\nEfficient gradient accumulation patterns\nScalar operations over broadcast temporaries\nZero dynamic growth: avoid push! in hot paths; size outputs up-front","category":"section"},{"location":"performance/#Backend-Performance-Characteristics","page":"Performance Guide","title":"Backend Performance Characteristics","text":"","category":"section"},{"location":"performance/#Automatic-Differentiation-(:ad)-**RECOMMENDED-DEFAULT**","page":"Performance Guide","title":"Automatic Differentiation (:ad) - RECOMMENDED DEFAULT","text":"# Zero allocation after warmup\npopulation_margins(model, data; backend=:ad)  # 0 bytes allocated\n\nAdvantages:\n\nZero allocation after warmup\nMachine precision accuracy (exact derivatives)\nRobust domain handling (handles log, sqrt, 1/x safely)\nSuitable for complex formulas\n\nUse cases: Most applications - provides good performance and reliability","category":"section"},{"location":"performance/#Finite-Differences-(:fd)","page":"Performance Guide","title":"Finite Differences (:fd)","text":"# Zero allocation after warmup\npopulation_margins(model, data; backend=:fd)  # 0 bytes allocated\n\nAdvantages:\n\nZero allocation in production paths\nSimple numerical implementation\nGood accuracy for well-conditioned functions\n\nUse cases: Simple linear formulas where marginal speed differences matter","category":"section"},{"location":"performance/#Memory-Management","page":"Performance Guide","title":"Memory Management","text":"","category":"section"},{"location":"performance/#Allocation-Patterns","page":"Performance Guide","title":"Allocation Patterns","text":"Margins.jl achieves zero-allocation performance for computational workflows:\n\n# Profile margins: constant allocation regardless of data size\n@allocated profile_margins(model, small_data, means_grid(small_data))  # small constant allocation\n@allocated profile_margins(model, large_data, means_grid(large_data))  # same allocation pattern\n\n# Population margins: zero allocation after warmup (both backends)\n@allocated population_margins(model, data_1k; backend=:fd)   # 0 bytes\n@allocated population_margins(model, data_10k; backend=:fd)  # 0 bytes\n@allocated population_margins(model, data_1k; backend=:ad)   # 0 bytes\n@allocated population_margins(model, data_10k; backend=:ad)  # 0 bytes","category":"section"},{"location":"performance/#Memory-Efficiency-Best-Practices","page":"Performance Guide","title":"Memory Efficiency Best Practices","text":"","category":"section"},{"location":"performance/#For-Large-Datasets","page":"Performance Guide","title":"For Large Datasets","text":"# Use profile analysis for exploration (O(1) memory)\nscenarios = cartesian_grid(x1=[-1, 0, 1], treatment=[0, 1])\nresults = profile_margins(model, large_data, scenarios)\n\n# Use population analysis with zero-allocation backends\nkey_effects = population_margins(model, large_data; vars=[:treatment], backend=:ad)  # Recommended\n# OR\nkey_effects = population_margins(model, large_data; vars=[:treatment], backend=:fd)  # Also zero allocation","category":"section"},{"location":"performance/#For-Batch-Processing","page":"Performance Guide","title":"For Batch Processing","text":"# Process multiple models with zero allocation\nmodels = [model1, model2, model3]\nresults = []\n\nfor model in models\n    # Each call has zero allocation (either backend)\n    result = population_margins(model, data; backend=:ad)  # Recommended: zero allocation\n    push!(results, DataFrame(result))\nend","category":"section"},{"location":"performance/#Performance-Best-Practices","page":"Performance Guide","title":"Performance Best Practices","text":"","category":"section"},{"location":"performance/#High-Performance-Usage-Patterns","page":"Performance Guide","title":"High-Performance Usage Patterns","text":"For optimal performance in production environments, follow these proven patterns:","category":"section"},{"location":"performance/#Compilation-and-Caching","page":"Performance Guide","title":"Compilation and Caching","text":"# Good: Compile once, use multiple times\ncompiled = FormulaCompiler.compile_formula(model, data)  # Expensive, do once\nde = FormulaCompiler.build_derivative_evaluator(compiled, data; vars=vars)  # Do once\n\n# Multiple analysis calls reuse compiled objects automatically\nresult1 = population_margins(model, data; type=:effects)     \nresult2 = profile_margins(model, data, means_grid(data); type=:effects)\n\n# Avoid: Forcing recompilation in loops\nfor subset in data_subsets\n    # Each call may recompile unnecessarily\n    result = population_margins(fit_model(subset), subset)  \nend","category":"section"},{"location":"performance/#Memory-Efficient-Data-Processing","page":"Performance Guide","title":"Memory-Efficient Data Processing","text":"# Good: Pre-allocate result structures for known sizes\nn_effects = length(vars) * length(scenarios)\nresult_buffer = DataFrame(\n    term = Vector{String}(undef, n_effects),\n    estimate = Vector{Float64}(undef, n_effects),\n    se = Vector{Float64}(undef, n_effects)\n)\n\n# Good: Use scalar operations in hot paths\nfor i in eachindex(estimates)\n    μ = GLM.linkinv(link, η[i])      # Scalar operation\n    se[i] = sqrt(gradients[i]' * Σ * gradients[i])\nend\n\n# Avoid: Growing DataFrames with push! in loops\nresults = DataFrame()\nfor scenario in scenarios\n    result = population_margins(model, scenario_data)\n    push!(results, DataFrame(result))  # Expensive growth\nend","category":"section"},{"location":"performance/#FormulaCompiler-Integration-Patterns","page":"Performance Guide","title":"FormulaCompiler Integration Patterns","text":"# Good: Let FormulaCompiler handle the optimization\n# Use built-in primitives for zero-allocation paths\npopulation_margins(model, data; backend=:fd)  # Uses optimized accumulation\n\n# Good: Cache compiled objects for batch processing\nmodels = [model1, model2, model3]\ncached_compilations = Dict()\n\nfor model in models\n    # Compilation is cached automatically by model signature\n    result = population_margins(model, data; backend=:ad)\nend","category":"section"},{"location":"performance/#Performance-Validation","page":"Performance Guide","title":"Performance Validation","text":"","category":"section"},{"location":"performance/#Checking-Allocation-Patterns","page":"Performance Guide","title":"Checking Allocation Patterns","text":"# Verify zero-allocation performance\nusing BenchmarkTools\n\n# Both backends should show 0 allocation after warmup\n@allocated population_margins(model, data; backend=:ad)  # Expected: 0 bytes  \n@allocated population_margins(model, data; backend=:fd)  # Expected: 0 bytes\n\n# Profile margins should have constant allocation regardless of data size\n@allocated profile_margins(model, small_data, means_grid(small_data))  # Small constant\n@allocated profile_margins(model, large_data, means_grid(large_data))  # Same constant","category":"section"},{"location":"performance/#Performance-Monitoring","page":"Performance Guide","title":"Performance Monitoring","text":"# Production monitoring pattern\nfunction monitored_margins(model, data; max_alloc_kb=10, kwargs...)\n    alloc_before = Base.gc_num().poolalloc\n    \n    result = population_margins(model, data; kwargs...)\n    \n    alloc_after = Base.gc_num().poolalloc\n    alloc_kb = (alloc_after - alloc_before) / 1024\n    \n    if alloc_kb > max_alloc_kb\n        @warn \"Excessive allocation detected: $(alloc_kb)KB\"\n    end\n    \n    return result\nend","category":"section"},{"location":"performance/#Troubleshooting-Performance-Issues","page":"Performance Guide","title":"Troubleshooting Performance Issues","text":"","category":"section"},{"location":"performance/#Diagnostic-Tools","page":"Performance Guide","title":"Diagnostic Tools","text":"","category":"section"},{"location":"performance/#Memory-Allocation-Checking","page":"Performance Guide","title":"Memory Allocation Checking","text":"using BenchmarkTools\n\n# Check allocation patterns\n@allocated population_margins(model, data)  # Should be constant across dataset sizes\n\n# Benchmark performance\n@btime population_margins($model, $data)     # Timing analysis","category":"section"},{"location":"performance/#Performance-Profiling","page":"Performance Guide","title":"Performance Profiling","text":"# Profile hot paths (advanced)\nusing Profile\n\n@profile for i in 1:100\n    population_margins(model, data; backend=:fd)\nend\nProfile.print()","category":"section"},{"location":"performance/#Common-Issues-and-Solutions","page":"Performance Guide","title":"Common Issues and Solutions","text":"","category":"section"},{"location":"performance/#Issue:-Profile-margins-slower-than-expected","page":"Performance Guide","title":"Issue: Profile margins slower than expected","text":"Diagnosis: Reference grid specification or DataFrame dispatch issues Solution: Use proper reference grid builders\n\n# Correct: O(1) performance with reference grids\nprofile_margins(model, data, means_grid(data); type=:effects)\nprofile_margins(model, data, cartesian_grid(x=[0,1,2]); type=:effects)\n\n# Avoid: Improper reference grid specification\n# Always use reference grid builders or explicit DataFrames","category":"section"},{"location":"performance/#Issue:-Population-margins-allocating-excessively","page":"Performance Guide","title":"Issue: Population margins allocating excessively","text":"Diagnosis: Hot loop allocation patterns or data format issues Solution: Check backend and use efficient data formats\n\n# Efficient: Both backends should be zero allocation\nresult = population_margins(model, data; backend=:ad)  # Recommended\nresult = population_margins(model, data; backend=:fd)  # Also zero allocation\n\n# Efficient data format\ndata_nt = Tables.columntable(data)  # Convert once for multiple analyses\nresult = population_margins(model, data_nt; backend=:ad)","category":"section"},{"location":"performance/#Issue:-Inconsistent-performance-across-runs","page":"Performance Guide","title":"Issue: Inconsistent performance across runs","text":"Diagnosis: Compilation effects, memory pressure, or GC interference Solution: Proper warmup and consistent configuration\n\n# Proper benchmarking protocol\n# 1. Warmup run\npopulation_margins(model, small_sample)  \n\n# 2. Clear compilation effects  \nGC.gc()\n\n# 3. Consistent benchmark\n@btime population_margins($model, $data; backend=:ad)  ","category":"section"},{"location":"performance/#Issue:-Memory-allocation-growing-with-dataset-size","page":"Performance Guide","title":"Issue: Memory allocation growing with dataset size","text":"Diagnosis: O(n) allocation pattern indicating performance regression Solution: Verify zero-allocation backends and check for loops\n\n# Expected: constant allocation across dataset sizes\n@allocated population_margins(model, data_1k; backend=:ad)    # Should be 0 bytes\n@allocated population_margins(model, data_10k; backend=:ad)   # Should be 0 bytes  \n@allocated population_margins(model, data_100k; backend=:ad)  # Should be 0 bytes\n\n# If allocations grow with n:\n# 1. Check backend selection (:ad and :fd both should be zero allocation)  \n# 2. Verify data format (Tables.jl-compatible)\n# 3. Check for custom vcov functions that may allocate","category":"section"},{"location":"performance/#Issue:-Slow-compilation-on-first-run","page":"Performance Guide","title":"Issue: Slow compilation on first run","text":"Diagnosis: Normal FormulaCompiler compilation overhead Solution: Accept first-run cost, subsequent runs benefit from caching\n\n# Expected pattern:\n@time population_margins(model, data)        # Slower (compilation)\n@time population_margins(model, data)        # Faster (cached)\n\n# For production: accept compilation cost or precompile key models","category":"section"},{"location":"performance/#FormulaCompiler.jl-Integration","page":"Performance Guide","title":"FormulaCompiler.jl Integration","text":"","category":"section"},{"location":"performance/#Zero-Allocation-Foundations","page":"Performance Guide","title":"Zero-Allocation Foundations","text":"Margins.jl achieves performance through tight integration with FormulaCompiler.jl:","category":"section"},{"location":"performance/#Compiled-Formula-Evaluation","page":"Performance Guide","title":"Compiled Formula Evaluation","text":"# Single compilation, multiple evaluations\ncompiled = FormulaCompiler.compile_formula(model, data)  # Once\n# Reused across all margin computations - zero allocation per evaluation","category":"section"},{"location":"performance/#Derivative-Computation","page":"Performance Guide","title":"Derivative Computation","text":"# Pre-built derivative evaluators\nde = FormulaCompiler.build_derivative_evaluator(compiled, data; vars=vars)  # Once  \n# Reused for all marginal effects - zero allocation per derivative","category":"section"},{"location":"performance/#Buffer-Management","page":"Performance Guide","title":"Buffer Management","text":"# Pre-allocated buffers prevent runtime allocation\nη_buf = Vector{Float64}(undef, n_profiles)      # Linear predictor buffer\ng_buf = Vector{Float64}(undef, n_vars)          # Gradient buffer  \ngβ_accumulator = Vector{Float64}(undef, n_coef) # Parameter gradient buffer","category":"section"},{"location":"performance/#Advanced-Performance-Patterns","page":"Performance Guide","title":"Advanced Performance Patterns","text":"","category":"section"},{"location":"performance/#Caching-Strategies","page":"Performance Guide","title":"Caching Strategies","text":"# FormulaCompiler artifacts are cached automatically\n# Multiple margin calls on same model/data reuse compilation\nresult1 = population_margins(model, data; type=:effects)      # Compiles\nresult2 = profile_margins(model, data, means_grid(data); type=:effects)  # Reuses compilation","category":"section"},{"location":"performance/#Batch-Processing-Optimization","page":"Performance Guide","title":"Batch Processing Optimization","text":"# Process multiple scenarios efficiently\nscenarios = cartesian_grid(x1=[0,1,2], group=[\"A\",\"B\",\"C\"])  # 9 profiles\n\n# Single compilation, multiple scenario evaluations\nresults = profile_margins(model, data, scenarios; type=:effects)  # Efficient","category":"section"},{"location":"performance/#Production-Deployment-Guidelines","page":"Performance Guide","title":"Production Deployment Guidelines","text":"","category":"section"},{"location":"performance/#Recommended-Configuration","page":"Performance Guide","title":"Recommended Configuration","text":"# High-performance production settings\nresult = population_margins(\n    model, data;\n    backend = :ad,           # Recommended: zero allocation, exact derivatives\n    scale = :response,       # Response scale\n    type = :effects          # Core functionality\n)","category":"section"},{"location":"performance/#Monitoring-and-Validation","page":"Performance Guide","title":"Monitoring and Validation","text":"# Performance monitoring in production\nfunction production_margins(model, data; kwargs...)\n    # Allocation monitoring\n    alloc_before = Base.gc_num().poolalloc\n    \n    result = population_margins(model, data; backend=:ad, kwargs...)\n    \n    alloc_after = Base.gc_num().poolalloc\n    alloc_diff = alloc_after - alloc_before\n    \n    # Log excessive allocations\n    if alloc_diff > 10000  # 10KB threshold\n        @warn \"Excessive allocation detected\" alloc_diff\n    end\n    \n    return result\nend","category":"section"},{"location":"performance/#Error-Handling","page":"Performance Guide","title":"Error Handling","text":"# Robust production wrapper\n# Note: No implicit backend fallbacks. Select `backend` explicitly.\n\n\n\nThis performance guide ensures you can leverage Margins.jl's full computational potential while maintaining statistical rigor in production environments. For conceptual background on why Population vs Profile matters for performance, see Mathematical Foundation. For comprehensive API usage, see API Reference.","category":"section"},{"location":"weights/#Weights-in-Population-Analysis","page":"Weights","title":"Weights in Population Analysis","text":"This guide explains how to use observation weights in population_margins, how weighted averaging and delta‑method standard errors are computed, and how weights interact with groups and scenarios.","category":"section"},{"location":"weights/#Scope-and-Policy","page":"Weights","title":"Scope and Policy","text":"Weights are supported in population_margins via the weights keyword.\nprofile_margins does not accept weights — profiles evaluate scenarios at reference points without averaging over the sample.\nStatistical correctness: Weighted quantities use proper normalization and delta‑method SEs use the averaged gradient with the model’s full covariance matrix Σ.","category":"section"},{"location":"weights/#Supported-Forms","page":"Weights","title":"Supported Forms","text":"weights = nothing (default): Unweighted analysis.\nweights = :colname (Symbol): Column in data with weights (sampling or frequency).\nweights = vector::AbstractVector{<:Real}: Vector of weights with length == nrow(data).","category":"section"},{"location":"weights/#Weighted-Computation","page":"Weights","title":"Weighted Computation","text":"Let w_i ≥ 0 be weights for observation i in the current context (after grouping filters). Then:\n\nWeighted mean effect: AME = (∑ w_i · Δ_i) / (∑ w_i)\nWeighted averaged gradient: ḡ = (∑ w_i · g_i) / (∑ w_i)\nStandard error (delta method): se = sqrt(ḡ' · Σ · ḡ)\n\nWhere Δ_i is the per‑row effect (continuous derivative or categorical contrast) and g_i is the corresponding per‑row parameter gradient; Σ is the model covariance matrix.\n\nThese formulas are used consistently in:\n\nUngrouped population effects and predictions\nGrouped analyses (applied within each subgroup)\nScenario analyses (applied within each scenario × subgroup context)","category":"section"},{"location":"weights/#Examples","page":"Weights","title":"Examples","text":"using Random\nusing DataFrames, CategoricalArrays, GLM, Margins\n\nRandom.seed!(123)\nn = 200\ndf = DataFrame(\n    y = randn(n),\n    x = randn(n),\n    z = randn(n),\n    group = categorical(rand([\"A\",\"B\"], n)),\n    samp_w = rand(0.5:0.1:2.0, n),             # sampling weights\n    freq_w = rand([1,2,3,4], n)                 # frequency weights\n)\n\nmodel = lm(@formula(y ~ x + z + group), df)\n\n# 1) Unweighted population AME\name_unw = population_margins(model, df; type=:effects, vars=[:x, :z])\n\n# 2) Sampling weights via column name\name_samp = population_margins(model, df; type=:effects, vars=[:x, :z], weights=:samp_w)\n\n# 3) Frequency weights via column name\name_freq = population_margins(model, df; type=:effects, vars=[:x, :z], weights=:freq_w)\n\n# 4) Explicit weight vector\nwvec = Float64.(df.samp_w)\name_vec = population_margins(model, df; type=:effects, vars=[:x, :z], weights=wvec)\n\n# 5) Grouped weighted analysis\ngrp_samp = population_margins(model, df; type=:effects, vars=[:x], groups=:group, weights=:samp_w)\n\n# 6) Scenarios with weights (counterfactual z values)\nscen_w = population_margins(model, df; type=:effects, vars=[:x], scenarios=(z=[-1.0, 0.0, 1.0]), weights=:samp_w)\n\nAll results use weighted averaging with proper normalization by the total weight in each context and delta‑method SEs computed from the averaged gradient and full covariance Σ.","category":"section"},{"location":"weights/#Best-Practices","page":"Weights","title":"Best Practices","text":"Provide non‑negative weights; zero weights effectively drop observations.\nFor grouped analyses, ensure the weight column/vector aligns with the original data (the implementation indexes weights by original row indices).\nConfirm units/interpretation: sampling vs frequency weights may yield different magnitudes depending on the empirical distribution they imply.\nUse stable data types (Float64 for weight vectors) to avoid implicit conversions.","category":"section"},{"location":"weights/#Error-Handling","page":"Weights","title":"Error Handling","text":"Length mismatch for weights::Vector vs nrow(data) → error.\nInvalid weight column name → error.\nUsing a variable as both a weight and a simultaneous effect variable or grouping key may error if it creates an internal contradiction; prefer distinct columns.","category":"section"},{"location":"profile_margins/#Profile-Specific-Marginal-Effects-Analysis","page":"Profile Analysis","title":"Profile-Specific Marginal Effects Analysis","text":"Profile-specific marginal effects analysis evaluates marginal quantities at predetermined points within the covariate space, providing inference for representative scenarios and policy counterfactuals. This methodological approach facilitates concrete interpretation of marginal effects through evaluation at theoretically motivated or practically relevant covariate combinations.","category":"section"},{"location":"profile_margins/#Reference-Grid-Methodology","page":"Profile Analysis","title":"Reference Grid Methodology","text":"The implementation employs an explicit reference grid specification to ensure methodological transparency and computational flexibility. The core analytical function utilizes the following signature specification:\n\nprofile_margins(model, data, reference_grid; type=:effects, vars=nothing, ...)","category":"section"},{"location":"profile_margins/#Reference-Grid-Construction-Framework","page":"Profile Analysis","title":"Reference Grid Construction Framework","text":"The package implements systematic reference grid construction through specialized builder functions that accommodate diverse analytical requirements:","category":"section"},{"location":"profile_margins/#1.-Sample-Means-means_grid()","page":"Profile Analysis","title":"1. Sample Means - means_grid()","text":"The canonical approach evaluates marginal quantities at empirical sample means for continuous variables while incorporating frequency-weighted probability mixtures for categorical variables.\n\n# Marginal effects at sample means (MEM)\nresult = profile_margins(model, data, means_grid(data); type=:effects)\n\n# Adjusted predictions at sample means (APM)\nresult = profile_margins(model, data, means_grid(data); type=:predictions)","category":"section"},{"location":"profile_margins/#2.-Cartesian-Product-cartesian_grid()","page":"Profile Analysis","title":"2. Cartesian Product - cartesian_grid()","text":"Systematic construction of reference grids through Cartesian product enumeration of specified covariate values across multiple dimensions:\n\n# Complete factorial design: 3×2 = 6 scenarios\nresult = profile_margins(model, data, \n    cartesian_grid(x=[-1, 0, 1], education=[\"High School\", \"College\"]); \n    type=:effects)\n\n# Single-variable sensitivity analysis with typical values for remaining covariates\nresult = profile_margins(model, data,\n    cartesian_grid(age=20:10:70);\n    type=:predictions)\n\nCompleting Partial Grids with complete_reference_grid()\n\nWhen creating reference grids with cartesian_grid(), you may specify only a subset of model variables. The complete_reference_grid() function automatically fills in typical values for any missing variables:\n\n# Create partial grid focusing on variables of interest\npartial_grid = cartesian_grid(\n    x1=[0, 1, 2],      # Only specify x1 and x2\n    x2=[10, 20]\n)\n# Result: 3×2 = 6 rows, but missing x3, x4, etc. from model\n\n# Complete the grid with typical values for missing variables\ncomplete_grid = complete_reference_grid(partial_grid, model, data)\n# Result: 6 rows with x1, x2 specified AND x3=mean(data.x3), x4=mean(data.x4), etc.\n\n# Use directly with profile_margins\nresult = profile_margins(model, data, complete_grid; type=:effects)\n\n# Customize the typical value function (default is mean)\ncomplete_grid = complete_reference_grid(partial_grid, model, data; typical=median)\n# Now uses median instead of mean for missing variables\n\nUse Cases:\n\nFocused Analysis: Specify only key variables of interest, let the function handle the rest\nSensitivity Analysis: Vary 1-2 variables while holding others at representative values\nPresentation: Simplify grid specification for cleaner, more maintainable code\nCustom Typical Values: Use median, mode, or custom functions for missing variables\n\nImportant: This function only works with partial grids. If your grid already specifies all model variables, complete_reference_grid() simply returns it unchanged.","category":"section"},{"location":"profile_margins/#3.-Balanced-Factorial-balanced_grid()","page":"Profile Analysis","title":"3. Balanced Factorial - balanced_grid()","text":"Construction of balanced factorial designs utilizing equal-weight probability mixtures for categorical variable specifications:\n\n# Balanced factorial design for comprehensive categorical analysis\nresult = profile_margins(model, data,\n    balanced_grid(data; education=:all, region=:all); \n    type=:effects)","category":"section"},{"location":"profile_margins/#4.-Quantile-Based-quantile_grid()","page":"Profile Analysis","title":"4. Quantile-Based - quantile_grid()","text":"Reference grid construction based on empirical quantiles of continuous variable distributions:\n\n# Marginal effects evaluated at income distribution quartiles\nresult = profile_margins(model, data,\n    quantile_grid(data; income=[0.25, 0.5, 0.75]); \n    type=:effects)","category":"section"},{"location":"profile_margins/#5.-Hierarchical-Grammar-hierarchical_grid()","page":"Profile Analysis","title":"5. Hierarchical Grammar - hierarchical_grid()","text":"Systematic reference grid construction using the group nesting grammar (=> operator) for complex multi-dimensional covariate scenario analysis:\n\n# Complex hierarchical specification with multiple representative types\nreference_spec = :region => [\n    (:income, :quartiles),  # Income quartiles within each region\n    (:age, :mean),          # Mean age within each region\n    :education              # All education levels within each region\n]\nresult = profile_margins(model, data, \n    hierarchical_grid(data, reference_spec); \n    type=:effects)\n\n# Deep hierarchical nesting for comprehensive policy analysis\npolicy_spec = :country => (:region => (:education => [(:income, :quintiles), (:age, :mean)]))\nresult = profile_margins(model, data,\n    hierarchical_grid(data, policy_spec; max_depth=4); \n    type=:predictions)","category":"section"},{"location":"profile_margins/#Direct-DataFrame-Specification-(Complete-Analytical-Control)","page":"Profile Analysis","title":"Direct DataFrame Specification (Complete Analytical Control)","text":"Maximum analytical flexibility is achieved through direct DataFrame specification of reference grid points:\n\n# Custom reference grid with explicit covariate specifications\nreference_grid = DataFrame(\n    age=[25, 35, 45], \n    education=[\"High School\", \"College\", \"Graduate\"],\n    experience=[2, 8, 15]\n)\nresult = profile_margins(model, data, reference_grid; type=:effects)","category":"section"},{"location":"profile_margins/#Advanced-Methodological-Features","page":"Profile Analysis","title":"Advanced Methodological Features","text":"","category":"section"},{"location":"profile_margins/#Population-Representative-Categorical-Composition","page":"Profile Analysis","title":"Population-Representative Categorical Composition","text":"The implementation addresses the methodological limitation of arbitrary baseline category selection through empirical frequency-weighted categorical mixtures that reflect actual population composition:\n\n# Data characteristics: education = 40% HS, 45% College, 15% Graduate\n#                      region = 75% Urban, 25% Rural\n\n# Sample means incorporating realistic categorical composition\nresult = profile_margins(model, data, means_grid(data); type=:effects)\n# → age: empirical sample mean\n# → education: frequency-weighted mixture (40% HS, 45% College, 15% Graduate)  \n# → region: frequency-weighted mixture (75% Urban, 25% Rural)","category":"section"},{"location":"profile_margins/#Fractional-Categorical-Specifications","page":"Profile Analysis","title":"Fractional Categorical Specifications","text":"Policy counterfactual analysis utilizes fractional categorical specifications through the categorical mixture interface:\n\nusing Margins: mix\n\n# Policy scenario incorporating specific treatment probability distributions\nreference_grid = DataFrame(\n    age=[35, 45, 55],\n    treated=[mix(0 => 0.3, 1 => 0.7)]  # 70% treatment probability\n)\nresult = profile_margins(model, data, reference_grid; type=:predictions)","category":"section"},{"location":"profile_margins/#Profile-Specific-Elasticity-Analysis","page":"Profile Analysis","title":"Profile-Specific Elasticity Analysis","text":"Elasticity computation at predetermined covariate profiles enables sensitivity analysis across representative scenarios:\n\n# Elasticities evaluated at sample means\nresult = profile_margins(model, data, means_grid(data); \n    type=:effects, measure=:elasticity)\n\n# Semi-elasticities across income distribution quantiles\nresult = profile_margins(model, data,\n    cartesian_grid(income=[25000, 50000, 75000]); \n    type=:effects, measure=:semielasticity_dyex)","category":"section"},{"location":"profile_margins/#Computational-Performance-Analysis","page":"Profile Analysis","title":"Computational Performance Analysis","text":"Profile-specific marginal effects analysis exhibits constant-time computational complexity with execution time independent of dataset dimensionality:\n\n# Computational cost remains invariant to dataset size\n@time profile_margins(model, small_data, means_grid(small_data))   # ~100μs\n@time profile_margins(model, large_data, means_grid(large_data))   # ~100μs\n\n# Complex factorial designs maintain constant-time scaling properties\nscenarios = cartesian_grid(x1=[0,1,2], x2=[10,20,30], group=[\"A\",\"B\"])  # 18 profiles\n@time profile_margins(model, huge_data, scenarios)  # Maintains ~100μs complexity","category":"section"},{"location":"profile_margins/#Migration-from-Old-API","page":"Profile Analysis","title":"Migration from Old API","text":"If you have code using the deprecated at parameter:\n\n# OLD (deprecated):\nprofile_margins(model, data; at=:means, type=:effects)\nprofile_margins(model, data; at=Dict(:x => [0,1,2]), type=:effects)\n\n# NEW (current):\nprofile_margins(model, data, means_grid(data); type=:effects)  \nprofile_margins(model, data, cartesian_grid(x=[0,1,2]); type=:effects)","category":"section"},{"location":"profile_margins/#Statistical-Notes","page":"Profile Analysis","title":"Statistical Notes","text":"Standard errors: Computed via delta method using full model covariance matrix\nCategorical effects: Use baseline contrasts vs reference levels at each profile\nProfile interpretation: More concrete than population averages, ideal for policy communication\nComputational efficiency: Single compilation per analysis, reused across all profiles","category":"section"},{"location":"profile_margins/#Output-Structure-and-Result-Multiplicity","page":"Profile Analysis","title":"Output Structure and Result Multiplicity","text":"","category":"section"},{"location":"profile_margins/#Result-Row-Generation-Pattern","page":"Profile Analysis","title":"Result Row Generation Pattern","text":"The number of output rows depends on variable types and categorical levels:\n\nContinuous variables: 1 reference grid row → 1 output row (derivative)\nCategorical variables: 1 reference grid row → Multiple output rows (baseline contrasts)","category":"section"},{"location":"profile_margins/#Example-Output-Multiplicity","page":"Profile Analysis","title":"Example Output Multiplicity","text":"# Reference grid with 2 profiles\ngrid = DataFrame(\n    age = [30, 40],                    # Continuous variable\n    education = [\"A\", \"B\", \"C\"]        # Categorical with 3 levels\n)\n\n# Results: 2 profiles × (1 continuous + 2 categorical contrasts) = 6 total rows:\nresult = profile_margins(model, data, grid; type=:effects, vars=[:age, :education])\n\n# Output structure:\n# Row 1: age at (age=30, education=\"A,B,C\")\n# Row 2: education: B - A at (age=30, education=\"A,B,C\")\n# Row 3: education: C - A at (age=30, education=\"A,B,C\")\n# Row 4: age at (age=40, education=\"A,B,C\")\n# Row 5: education: B - A at (age=40, education=\"A,B,C\")\n# Row 6: education: C - A at (age=40, education=\"A,B,C\")","category":"section"},{"location":"profile_margins/#Categorical-Baseline-Contrasts","page":"Profile Analysis","title":"Categorical Baseline Contrasts","text":"For a categorical variable with K levels, each reference grid row generates:\n\nK-1 contrast rows comparing each non-baseline level to the baseline\nTerm names follow pattern: \"variable: level - baseline\"\nExample: 4-level categorical produces 3 contrasts (B - A, C - A, D - A)\n\nThis multiplicative pattern means that reference grids with multiple categorical variables can generate substantial numbers of result rows, enabling comprehensive categorical effect analysis at each profile.","category":"section"},{"location":"profile_margins/#Examples","page":"Profile Analysis","title":"Examples","text":"","category":"section"},{"location":"profile_margins/#Basic-Workflow","page":"Profile Analysis","title":"Basic Workflow","text":"using DataFrames, GLM, Margins\n\n# Fit model\nmodel = lm(@formula(y ~ x1 + x2 + group), data)\n\n# Effects at sample means\nmem_results = profile_margins(model, data, means_grid(data); type=:effects)\nDataFrame(mem_results)\n\n# Predictions at specific scenarios\nscenarios = cartesian_grid(x1=[0, 1, 2], group=[\"A\", \"B\"])\npredictions = profile_margins(model, data, scenarios; type=:predictions)\nDataFrame(predictions)","category":"section"},{"location":"profile_margins/#Policy-Analysis","page":"Profile Analysis","title":"Policy Analysis","text":"# Current scenario: actual data composition\ncurrent = profile_margins(model, data, means_grid(data); type=:predictions)\n\n# Policy scenario: increased education levels\npolicy_grid = DataFrame(\n    x1=mean(data.x1),\n    education=mix(\"High School\" => 0.2, \"College\" => 0.5, \"Graduate\" => 0.3)\n)\nfuture = profile_margins(model, data, policy_grid; type=:predictions)\n\n# Compare scenarios\npolicy_effect = DataFrame(future).estimate[1] - DataFrame(current).estimate[1]\n\nSee also: population_margins for population-averaged analysis.","category":"section"},{"location":"second_differences/#Second-Differences-(Interaction-Effects)","page":"Second Differences","title":"Second Differences (Interaction Effects)","text":"Quantifying how marginal effects vary across moderator levels","category":"section"},{"location":"second_differences/#Conceptual-Foundation","page":"Second Differences","title":"Conceptual Foundation","text":"","category":"section"},{"location":"second_differences/#What-Are-Second-Differences?","page":"Second Differences","title":"What Are Second Differences?","text":"Second differences measure whether the marginal effect of one variable differs across levels of another variable. In generalized linear models, where effects are inherently nonlinear, second differences provide the natural way to quantify interaction effects on the predicted outcome scale.\n\nDefinition: For two variables X (focal variable) and Z (moderator), the second difference is defined as:\n\n$\n\n\\Delta^2 = [E(Y|X=1,Z=1) - E(Y|X=0,Z=1)] - [E(Y|X=1,Z=0) - E(Y|X=0,Z=0)] $\n\nThis represents the difference in marginal effects, a \"difference of differences\", capturing how the effect of X changes when Z changes.","category":"section"},{"location":"second_differences/#Relationship-to-Interaction-Terms","page":"Second Differences","title":"Relationship to Interaction Terms","text":"Model Type Linear Model Nonlinear Model (GLM)\nSimple effect Coefficient on X Marginal effect of X\nInteraction Coefficient on X times Z Second difference\nScale Coefficient scale = outcome scale Must compute from predicted probabilities/means\n\nIn linear models, the interaction coefficient directly represents the second difference. In nonlinear models (logit, probit, Poisson), the interaction coefficient does not directly represent the interaction on the predicted scale, necessitating explicit computation of second differences from marginal effects.","category":"section"},{"location":"second_differences/#Methodological-Context","page":"Second Differences","title":"Methodological Context","text":"Second differences extend the conceptual framework of Margins.jl by addressing a fundamental question: \"Does the effect of X depend on Z?\" This complements the two core questions of marginal effects analysis:\n\nEffects: \"How much does Y change when I change X?\"\nPredictions: \"What value of Y should I expect?\"\n\nSecond differences answer: \"Does the answer to question 1 change depending on Z?\"","category":"section"},{"location":"second_differences/#Computational-Framework","page":"Second Differences","title":"Computational Framework","text":"","category":"section"},{"location":"second_differences/#Two-Approaches-to-Second-Differences","page":"Second Differences","title":"Two Approaches to Second Differences","text":"Margins.jl implements second differences through two complementary approaches:","category":"section"},{"location":"second_differences/#1.-Discrete-Contrast-Approach-(Population-Based)","page":"Second Differences","title":"1. Discrete Contrast Approach (Population-Based)","text":"The discrete contrast approach uses pre-computed population-level marginal effects (AMEs) via functions like second_differences(), second_differences_pairwise(), and second_differences_all_contrasts(). This approach provides:\n\nPopulation representativeness: Effects averaged over the sample distribution\nStatistical validity: Proper delta-method standard errors accounting for full covariance structure\nFlexibility: Works with binary, categorical, and continuous moderators\nUse case: Comparing AMEs across discrete moderator levels","category":"section"},{"location":"second_differences/#2.-Local-Derivative-Approach-(Profile-Based)","page":"Second Differences","title":"2. Local Derivative Approach (Profile-Based)","text":"The local derivative approach uses second_differences_at() to compute derivatives at specific evaluation points via finite differences. This approach provides:\n\nProfile-specific analysis: Evaluate how effects vary at representative covariate combinations\nContinuous moderation: Direct derivative ∂AME/∂modifier at specific points\nScenario control: Hold other variables constant while varying only the modifier\nUse case: Understanding effect heterogeneity at particular policy-relevant profiles\nRequirement: Modifier must be continuous/numeric (focal variable can be continuous or categorical)","category":"section"},{"location":"second_differences/#Statistical-Inference","page":"Second Differences","title":"Statistical Inference","text":"Standard errors for second differences employ rigorous delta-method computation. For a second difference comparing two AMEs (textAME_1 and textAME_2) with parameter gradients g_1 and g_2:\n\n$\n\n\\text{SE}(\\Delta^2) = \\sqrt{(g2 - g1)' \\Sigma (g2 - g1)} $\n\nwhere Sigma is the model's parameter covariance matrix. This approach properly accounts for:\n\nCovariance between the two AME estimates\nFull uncertainty propagation from model parameters\nAppropriate test statistics and confidence intervals\n\nThe delta-method computation ensures that hypothesis tests maintain appropriate Type I error rates and confidence intervals achieve nominal coverage probabilities, meeting the statistical validity standards required for econometric research.","category":"section"},{"location":"second_differences/#Function-Reference","page":"Second Differences","title":"Function Reference","text":"","category":"section"},{"location":"second_differences/#Main-Interface","page":"Second Differences","title":"Main Interface","text":"","category":"section"},{"location":"second_differences/#second_differences()","page":"Second Differences","title":"second_differences()","text":"Unified interface for computing second differences across all modifier types. This is the recommended entry point that automatically handles binary, categorical, and continuous moderators.\n\nSignature:\n\nsecond_differences(\n    ame_result::EffectsResult,\n    variables::Union{Symbol, Vector{Symbol}},\n    modifier::Symbol,\n    vcov::Matrix{Float64};\n    contrast::String = \"derivative\",\n    modifier_type::Symbol = :auto,\n    all_contrasts::Bool = true\n)\n\nArguments:\n\name_result: Result from population_margins() with scenarios over the modifier\nvariables: Focal variable(s) to analyze (single Symbol or Vector{Symbol})\nmodifier: The moderating variable\nvcov: Parameter covariance matrix from the model (via vcov(model))\n\nKeyword Arguments:\n\ncontrast: Focal variable contrast for categorical variables (default: \"derivative\")\nmodifier_type: Modifier classification (:auto, :binary, :categorical, :continuous)\nall_contrasts: Compute all focal variable contrasts when applicable (default: true)\n\nReturns: DataFrame with columns:\n\nvariable: Focal variable name\nmodifier: Modifier variable name\ncontrast: Contrast description (for categorical focal variables)\nmodifier_level1, modifier_level2: Levels being compared\nsecond_diff: Second difference estimate (or slope for continuous)\nse: Standard error (delta method)\nz_stat: Z-statistic\np_value: P-value for H₀: second difference = 0\name_at_level1, ame_at_level2: AME values at each level\nmodifier_type: Detected modifier type\nsignificant: Boolean indicator (p < 0.05)","category":"section"},{"location":"second_differences/#Specialized-Functions","page":"Second Differences","title":"Specialized Functions","text":"","category":"section"},{"location":"second_differences/#second_differences_pairwise()","page":"Second Differences","title":"second_differences_pairwise()","text":"Computes all pairwise modifier comparisons for a single focal variable contrast.\n\nUse case: When analyzing a categorical or continuous moderator with multiple levels, and you want all pairwise comparisons for one specific contrast.\n\nSignature:\n\nsecond_differences_pairwise(\n    ame_result::EffectsResult,\n    variable::Symbol,\n    modifier::Symbol,\n    vcov::Matrix{Float64};\n    contrast::String = \"derivative\",\n    modifier_type::Symbol = :auto\n)\n\nReturns: DataFrame with one row per pairwise modifier comparison.","category":"section"},{"location":"second_differences/#second_differences_all_contrasts()","page":"Second Differences","title":"second_differences_all_contrasts()","text":"Computes second differences for all contrasts of a categorical focal variable.\n\nUse case: When your focal variable is categorical with multiple contrasts (e.g., pairwise comparisons across education levels) and you want to see how each contrast varies across modifier levels.\n\nSignature:\n\nsecond_differences_all_contrasts(\n    ame_result::EffectsResult,\n    variable::Symbol,\n    modifier::Symbol,\n    vcov::Matrix{Float64};\n    modifier_type::Symbol = :auto\n)\n\nReturns: DataFrame with one row per (focal contrast × modifier pair) combination.","category":"section"},{"location":"second_differences/#second_differences_at()","page":"Second Differences","title":"second_differences_at()","text":"Computes local derivatives of marginal effects with respect to a continuous modifier at specified evaluation points using finite differences.\n\nUse case: Profile-based analysis where you want to understand how effects vary at specific covariate combinations, holding other variables constant.\n\nImportant: The modifier must be numeric/continuous because the function uses finite differences (perturbing the modifier by ±δ). For categorical moderators, use second_differences() or second_differences_pairwise() instead.\n\nSignature:\n\nsecond_differences_at(\n    model::RegressionModel,\n    data,\n    variables::Union{Symbol, Vector{Symbol}},\n    modifier::Symbol,\n    vcov;\n    at::Union{Symbol, Real, Vector, NamedTuple} = :mean,\n    profile::NamedTuple = NamedTuple(),\n    delta::Union{Symbol, Real} = :auto,\n    scale::Symbol = :response\n)\n\nArguments:\n\nmodel: Fitted regression model\ndata: Data frame used to fit the model\nvariables: Focal variable(s) to compute derivatives for (can be continuous or categorical)\nmodifier: Continuous modifier variable (differentiate with respect to this - must be numeric)\nvcov: Parameter covariance matrix or function\n\nKeyword Arguments:\n\nat: Where to evaluate the derivative\n:mean: At mean(modifier) [default]\n:median: At median(modifier)\nNumeric value: At specified modifier value\nVector: Multiple evaluation points\nNamedTuple: Full explicit profile including modifier\nprofile: Additional variables to hold fixed (scalar or vector values)\ndelta: Step size for finite difference (:auto or numeric value)\nscale: Prediction scale (:link or :response)\n\nReturns: DataFrame with columns:\n\nvariable: Focal variable name\ncontrast: Contrast description\nmodifier: Modifier variable name\neval_point: Evaluation point for modifier\nderivative: ∂AME/∂modifier (per unit change in modifier)\nse: Standard error of derivative\nz_stat, p_value: Statistical inference\ndelta_used: Actual step size used\nAdditional columns for profile variables\n\nExamples:\n\n# Continuous focal variable, continuous modifier\nsd = second_differences_at(model, data, :income, :age, vcov(model))\n# → 1 row: derivative of income effect w.r.t. age\n\n# Categorical focal variable, continuous modifier\n# (education is categorical: \"hs\", \"college\", \"grad\")\nsd = second_differences_at(model, data, :education, :income, vcov(model))\n# → Multiple rows: one per education contrast\n# → e.g., \"college - hs\", \"grad - hs\", \"grad - college\"\n# → Shows how each education gap varies with income\n\n# At specific profile with other variables held constant\nsd = second_differences_at(model, data, :education, :income, vcov(model);\n                          at=50000,           # Income = $50k\n                          profile=(age=40,    # Hold age constant\n                                  region=\"north\"))  # Hold region constant\n\n# Multiple variables across evaluation points\nsd = second_differences_at(model, data, [:x1, :x2], :age, vcov(model);\n                          at=[30, 45, 60])\n# → 2 variables × 3 eval points = 6 rows\n\n# ERROR: Categorical modifier not allowed\nsd = second_differences_at(model, data, :income, :region, vcov(model))\n# → Error: \"Modifier :region must be numeric for second_differences_at()\"\n# → Use second_differences() instead for categorical moderators\n\nVariable Type Requirements:\n\nFocal variable (first argument): Can be continuous or categorical\nContinuous → 1 row per evaluation point\nCategorical → Multiple rows per evaluation point (one per contrast)\nModifier (second argument): Must be continuous/numeric\nFunction uses finite differences: modifier ± δ\nFor categorical moderators, use second_differences() instead\n\nStatistical Notes:\n\nUses two-point symmetric finite difference: (AME(at + δ) - AME(at - δ)) / (2δ)\nDelta-method SE computed from gradient information\nChoice of δ (via delta parameter) trades off approximation bias vs variance","category":"section"},{"location":"second_differences/#Usage-Patterns","page":"Second Differences","title":"Usage Patterns","text":"","category":"section"},{"location":"second_differences/#Basic-Workflow","page":"Second Differences","title":"Basic Workflow","text":"The standard workflow for computing second differences involves three steps:\n\nFit your regression model\nCompute AMEs across modifier levels using population_margins() with scenarios\nCalculate second differences using second_differences()\n\nusing Margins, GLM, DataFrames\n\n# Step 1: Fit model with interaction\nmodel = lm(@formula(y ~ x * treated + z), data)\n\n# Step 2: Compute AMEs at both treatment levels\names = population_margins(model, data;\n                         scenarios=(treated=[0, 1],),\n                         type=:effects)\n\n# Step 3: Calculate second difference\nsd = second_differences(ames, :x, :treated, vcov(model))\nDataFrame(sd)\n\nOutput structure:\n\n1×12 DataFrame\n Row │ variable  modifier  contrast    modifier_level1  modifier_level2  second_diff  se        z_stat   p_value   ame_at_level1  ame_at_level2  significant\n     │ Symbol    Symbol    String      Int64           Int64            Float64      Float64   Float64  Float64   Float64        Float64        Bool\n─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ x         treated   derivative               0                1       0.152    0.0428    3.55    0.0004          0.298          0.450      true\n\nInterpretation: The marginal effect of x is 0.152 units larger when treated=1 compared to treated=0 (p < 0.001), indicating a significant positive interaction.","category":"section"},{"location":"second_differences/#Binary-Moderators","page":"Second Differences","title":"Binary Moderators","text":"Binary moderators produce a single second difference comparing the two levels.\n\nExample: Treatment Effect Heterogeneity\n\n# Does the effect of age vary by treatment status?\nmodel = glm(@formula(outcome ~ age * treated + education),\n            data, Binomial(), LogitLink())\n\names = population_margins(model, data;\n                         scenarios=(treated=[0, 1],),\n                         type=:effects)\n\nsd = second_differences(ames, :age, :treated, vcov(model))\n\nInterpretation contexts:\n\nPositive second difference: Effect is stronger when treated\nNegative second difference: Effect is weaker when treated\nNear-zero second difference: Effect is homogeneous across treatment status","category":"section"},{"location":"second_differences/#Categorical-Moderators","page":"Second Differences","title":"Categorical Moderators","text":"Categorical moderators with K levels produce K(K-1)/2 pairwise comparisons.\n\nExample: Education-Specific Effects\n\n# Does the income effect vary across education levels?\nmodel = lm(@formula(consumption ~ income + education), data)\n\names = population_margins(model, data;\n                         scenarios=(education=[\"hs\", \"college\", \"grad\"],),\n                         type=:effects)\n\nsd = second_differences(ames, :income, :education, vcov(model))\n\nOutput structure (3 education levels → 3 pairwise comparisons):\n\n3×12 DataFrame\n Row │ variable  modifier   modifier_level1  modifier_level2  second_diff  se      z_stat  p_value  significant\n     │ Symbol    Symbol     String          String           Float64      Float64  Float64  Float64  Bool\n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ income    education  hs              college               0.152    0.042    3.62    0.0003   true\n   2 │ income    education  hs              grad                  0.243    0.051    4.76   <1e-5    true\n   3 │ income    education  college         grad                  0.091    0.048    1.90    0.058    false\n\nInterpretation: The marginal effect of income on consumption is 0.152 units higher for college graduates compared to high school graduates (p < 0.001). The effect continues to increase for graduate degree holders, though the college-to-grad increase (0.091) is not statistically significant.","category":"section"},{"location":"second_differences/#Continuous-Moderators","page":"Second Differences","title":"Continuous Moderators","text":"Continuous moderators provide slopes representing the rate of change in marginal effects per unit change in the moderator.\n\nExample: Age-Varying Treatment Effects\n\n# How does the treatment effect change with age?\nmodel = lm(@formula(outcome ~ treatment + age), data)\n\n# Evaluate at three age values\names = population_margins(model, data;\n                         scenarios=(age=[30, 45, 60],),\n                         type=:effects)\n\nsd = second_differences(ames, :treatment, :age, vcov(model);\n                       modifier_type=:continuous)\n\nContinuous modifier interpretation:\n\nsecond_diff: Change in marginal effect per unit increase in moderator\nFor age comparison 30→45: slope = (AME₄₅ - AME₃₀) / 15\nSlopes indicate whether effects strengthen (positive) or weaken (negative) with age","category":"section"},{"location":"second_differences/#Multiple-Focal-Variables","page":"Second Differences","title":"Multiple Focal Variables","text":"Analyze multiple variables simultaneously to compare which effects show strongest moderation.\n\nExample: Comparative Moderation Analysis\n\n# Which demographic effects vary most by treatment?\nmodel = lm(@formula(outcome ~ age + education_yrs + experience + treated), data)\n\names = population_margins(model, data;\n                         scenarios=(treated=[0, 1],),\n                         type=:effects,\n                         vars=[:age, :education_yrs, :experience])\n\nsd = second_differences(ames, [:age, :education_yrs, :experience],\n                       :treated, vcov(model))\n\nOutput structure (3 variables × 1 binary contrast = 3 rows):\n\n3×12 DataFrame\n Row │ variable       modifier  second_diff  se      z_stat  p_value  significant\n     │ Symbol         Symbol    Float64      Float64  Float64  Float64  Bool\n─────┼────────────────────────────────────────────────────────────────────────\n   1 │ age            treated        0.045    0.018    2.50    0.012    true\n   2 │ education_yrs  treated        0.123    0.031    3.97   <1e-4    true\n   3 │ experience     treated        0.012    0.024    0.50    0.617    false\n\nInterpretation: Education effects show the strongest treatment heterogeneity (second diff = 0.123, p < 0.001), followed by age effects (0.045, p = 0.012). Experience effects do not significantly vary by treatment status.","category":"section"},{"location":"second_differences/#Categorical-Focal-Variables","page":"Second Differences","title":"Categorical Focal Variables","text":"When the focal variable is categorical, second differences can be computed for each contrast.\n\nExample: Education Contrasts Across Regions\n\nusing CategoricalArrays\n\n# Prepare categorical variable with explicit ordering\ndata.education = categorical(data.education,\n                            levels=[\"hs\", \"college\", \"grad\"],\n                            ordered=true)\n\nmodel = lm(@formula(income ~ education + region), data)\n\n# Compute pairwise education contrasts across regions\names = population_margins(model, data;\n                         scenarios=(region=[\"north\", \"south\", \"west\"],),\n                         type=:effects,\n                         vars=[:education])\n\n# Get all education contrasts × all region pairs\nsd = second_differences(ames, :education, :region, vcov(model))\n\nOutput dimensions: If education has 3 levels (→ 3 pairwise contrasts) and region has 3 levels (→ 3 pairwise comparisons), the result contains 3 × 3 = 9 rows representing the full matrix of focal contrasts by modifier comparisons.\n\nPractical use: Identify which education gaps (e.g., college - hs) vary most across geographic regions.","category":"section"},{"location":"second_differences/#Local-Derivatives-at-Profiles-(second_differences_at())","page":"Second Differences","title":"Local Derivatives at Profiles (second_differences_at())","text":"The profile-based approach computes how marginal effects change continuously with a modifier at specific evaluation points.\n\nExample: Income Effect Variation Across Age\n\n# Does the income effect on consumption strengthen or weaken with age?\nmodel = lm(@formula(consumption ~ income * age + education), data)\n\n# Evaluate at mean age\nsd = second_differences_at(model, data, :income, :age, vcov(model))\n# Interpretation: derivative shows whether income effect increases/decreases per year of age\n\n# Evaluate at multiple age points\nsd = second_differences_at(model, data, :income, :age, vcov(model);\n                          at=[25, 35, 45, 55, 65])\n# → 5 rows showing how income effect changes across life course\n\n# With fixed profile\nsd = second_differences_at(model, data, :income, :age, vcov(model);\n                          at=40,\n                          profile=(education=\"college\", region=\"urban\"))\n# → \"For urban college graduates, how does income effect vary with age?\"\n\nKey differences from discrete approach:\n\nContinuous moderation: Direct derivative rather than discrete contrasts\nProfile control: Can hold other variables constant at specific values\nInterpretation: Per-unit change in moderator rather than level-to-level comparison\n\nWhen to use each approach:\n\nApproach Use When Modifier Type Example\nsecond_differences()<br>(Discrete) Comparing effects across distinct groups Binary or Categorical \"Does age effect differ for treated vs control?\"\nsecond_differences_at()<br>(Local) Understanding continuous variation at specific profiles Continuous only \"At age=40, how does education effect change per 1k income?\"\n\nVariable Type Compatibility:\n\nFunction Focal Variable Modifier Variable\nsecond_differences() Continuous or Categorical Binary, Categorical, or Continuous\nsecond_differences_at() Continuous or Categorical Continuous only\n\nCommon error: Trying to use second_differences_at() with a categorical modifier will produce:\n\nError: Modifier :region must be numeric for second_differences_at()\n\nSolution: Use second_differences() or second_differences_pairwise() for categorical moderators.","category":"section"},{"location":"second_differences/#Advanced-Analysis-Patterns","page":"Second Differences","title":"Advanced Analysis Patterns","text":"","category":"section"},{"location":"second_differences/#Robust-Standard-Errors","page":"Second Differences","title":"Robust Standard Errors","text":"Second differences support robust standard errors through CovarianceMatrices.jl integration.\n\nusing CovarianceMatrices\n\n# Heteroskedasticity-robust second differences\nvcov_robust = CovarianceMatrices.HC1(model)\nsd = second_differences(ames, :x, :treated, vcov_robust)\n\n# Clustered standard errors\nvcov_clustered = CovarianceMatrices.Clustered(model, :firm_id)\nsd = second_differences(ames, :x, :treated, vcov_clustered)\n\nThe delta-method computation automatically incorporates the robust covariance structure, ensuring appropriate uncertainty quantification under heteroskedasticity or clustering.","category":"section"},{"location":"second_differences/#Multiple-Testing-Considerations","page":"Second Differences","title":"Multiple Testing Considerations","text":"When computing many pairwise comparisons (categorical moderators with many levels), consider adjusting for multiple testing:\n\nusing MultipleTesting\n\n# Compute all pairwise second differences\nsd = second_differences(ames, :x, :education, vcov(model))\n\n# Apply Bonferroni correction\nα = 0.05\nn_tests = nrow(sd)\nsd.significant_bonferroni = sd.p_value .< (α / n_tests)\n\n# Or use false discovery rate control\nsd.significant_fdr = adjust(sd.p_value, BenjaminiHochberg()) .< α","category":"section"},{"location":"second_differences/#Filtering-to-Specific-Contrasts","page":"Second Differences","title":"Filtering to Specific Contrasts","text":"For categorical focal variables with many contrasts, focus on specific contrasts of interest:\n\n# Compute only the \"college - hs\" contrast across regions\nsd = second_differences(ames, :education, :region, vcov(model);\n                       contrast=\"college - hs\",\n                       all_contrasts=false)","category":"section"},{"location":"second_differences/#Visualization-Workflows","page":"Second Differences","title":"Visualization Workflows","text":"Second differences integrate naturally with visualization workflows:\n\nusing StatsPlots\n\n# Plot second differences with confidence intervals\nsd_df = DataFrame(sd)\n@df sd_df scatter(:modifier_level2, :second_diff,\n                  yerr=1.96 .* :se,\n                  xlabel=\"Modifier Level\",\n                  ylabel=\"Second Difference\",\n                  title=\"Effect Heterogeneity Across Moderator\",\n                  legend=false)\nhline!([0], linestyle=:dash, color=:gray)  # Reference line at zero","category":"section"},{"location":"second_differences/#Methodological-Notes","page":"Second Differences","title":"Methodological Notes","text":"","category":"section"},{"location":"second_differences/#Comparison-to-Coefficient-Based-Interactions","page":"Second Differences","title":"Comparison to Coefficient-Based Interactions","text":"In linear models, second differences equal interaction coefficients:\n\n# Linear model: coefficient = second difference\nmodel_linear = lm(@formula(y ~ x * z), data)\ninteraction_coef = coef(model_linear)[end]  # Coefficient on x:z\n\names = population_margins(model_linear, data; scenarios=(z=[0,1],), type=:effects)\nsd = second_differences(ames, :x, :z, vcov(model_linear))\n# sd.second_diff ≈ interaction_coef (within numerical precision)\n\nIn nonlinear models, they diverge:\n\n# Logistic model: coefficient ≠ second difference\nmodel_logit = glm(@formula(y ~ x * z), data, Binomial(), LogitLink())\ninteraction_coef = coef(model_logit)[end]\n\names = population_margins(model_logit, data; scenarios=(z=[0,1],), type=:effects)\nsd = second_differences(ames, :x, :z, vcov(model_logit))\n# sd.second_diff ≠ interaction_coef (second diff is on probability scale)\n\nThis divergence motivates the second differences framework: to express interactions on the interpretable predicted outcome scale rather than the abstract coefficient scale.","category":"section"},{"location":"second_differences/#Relationship-to-Ai-and-Norton-(2003)","page":"Second Differences","title":"Relationship to Ai & Norton (2003)","text":"Ai & Norton (2003) demonstrated that in nonlinear models (logit, probit), the interaction effect:\n\nIs not equal to the interaction coefficient\nVaries across observations\nCan have different signs than the coefficient\n\nSecond differences in Margins.jl operationalize the Ai & Norton framework by:\n\nComputing population-averaged interaction effects (second differences from AMEs)\nProviding proper standard errors via delta method\nEnabling hypothesis tests for interaction significance\n\nReference: Ai, C., & Norton, E. C. (2003). Interaction terms in logit and probit models. Economics Letters, 80(1), 123-129.","category":"section"},{"location":"second_differences/#Population-vs-Profile-Interpretation","page":"Second Differences","title":"Population vs Profile Interpretation","text":"Current second differences use population-averaged marginal effects:\n\nAMEs computed by averaging over the sample distribution at each modifier level\nSecond differences reflect population-level interaction effects\nAppropriate for policy analysis requiring external validity\n\nFor profile-based local derivatives, use second_differences_at():\n\nEvaluate interaction effects at specific covariate combinations\nUseful for scenario-specific analysis or representative case interpretation\nSee the Local Derivatives at Profiles section above for full documentation","category":"section"},{"location":"second_differences/#Significance-Testing","page":"Second Differences","title":"Significance Testing","text":"Hypothesis tests evaluate H₀: second difference = 0 (no interaction on predicted scale).\n\nImportant distinction:\n\nSignificant second difference → interaction exists on predicted outcome scale\nSignificant interaction coefficient → interaction exists on coefficient scale\n\nIn nonlinear models, these are distinct hypotheses. Second differences test the hypothesis most relevant for applied interpretation.","category":"section"},{"location":"second_differences/#Integration-Examples","page":"Second Differences","title":"Integration Examples","text":"","category":"section"},{"location":"second_differences/#With-GLM.jl-Ecosystem","page":"Second Differences","title":"With GLM.jl Ecosystem","text":"using GLM, CategoricalArrays\n\n# Logistic regression with interaction\nmodel = glm(@formula(employed ~ education * experience + age),\n            data, Binomial(), LogitLink())\n\n# Second differences on probability scale\names = population_margins(model, data;\n                         scenarios=(experience=[0, 10, 20],),\n                         type=:effects,\n                         scale=:response)  # Probability scale\n\nsd = second_differences(ames, :education, :experience, vcov(model);\n                       modifier_type=:continuous)\n# Interpretation: change in employment effect of education per year of experience","category":"section"},{"location":"second_differences/#With-MixedModels.jl","page":"Second Differences","title":"With MixedModels.jl","text":"Second differences support mixed-effects models for clustered/hierarchical data:\n\nusing MixedModels\n\n# Linear mixed model with interaction\nmodel = fit(MixedModel,\n           @formula(outcome ~ treatment * time + (1 + time | subject)),\n           data)\n\names = population_margins(model, data;\n                         scenarios=(time=[0, 6, 12],),\n                         type=:effects)\n\nsd = second_differences(ames, :treatment, :time, vcov(model);\n                       modifier_type=:continuous)\n# Interpretation: change in treatment effect over time (time-varying treatment effect)","category":"section"},{"location":"second_differences/#With-DataFrames-Ecosystem","page":"Second Differences","title":"With DataFrames Ecosystem","text":"using DataFrames, Chain, CSV\n\n# Complete analysis pipeline\nresults = @chain begin\n    population_margins(model, data; scenarios=(region=[\"north\",\"south\",\"west\"],), type=:effects)\n    second_differences(_, :income, :region, vcov(model))\n    DataFrame(_)\n    subset(_, :significant => x -> x .== true)  # Filter to significant interactions\n    sort(_, :second_diff, rev=true)  # Sort by effect size\nend\n\n# Export for reporting\nCSV.write(\"significant_interactions.csv\", results)","category":"section"},{"location":"second_differences/#Error-Handling","page":"Second Differences","title":"Error Handling","text":"","category":"section"},{"location":"second_differences/#Common-Error-Patterns","page":"Second Differences","title":"Common Error Patterns","text":"Insufficient modifier levels:\n\n# Error: only one modifier level\names = population_margins(model, data; scenarios=(treated=[1],), type=:effects)\nsecond_differences(ames, :x, :treated, vcov(model))\n# → Error: \"Need at least 2 modifier levels\"\n\nSolution: Ensure scenarios include at least 2 modifier levels.\n\nMissing variable in AME result:\n\n# Error: variable not in AME computation\names = population_margins(model, data; scenarios=(treated=[0,1],),\n                         type=:effects, vars=[:x])\nsecond_differences(ames, :z, :treated, vcov(model))\n# → Error: No rows found for variable z\n\nSolution: Include the focal variable in the original population_margins() call or omit vars parameter to include all continuous variables.\n\nDimension mismatch:\n\n# Error: vcov dimensions don't match gradient dimensions\nwrong_vcov = vcov(different_model)\nsecond_differences(ames, :x, :treated, wrong_vcov)\n# → Error: \"Dimension mismatch: vcov has N parameters but gradients has M\"\n\nSolution: Ensure vcov comes from the same model used to compute AMEs.","category":"section"},{"location":"second_differences/#Validation-Workflow","page":"Second Differences","title":"Validation Workflow","text":"function validate_second_differences(sd_result::DataFrame)\n    # Check for numerical issues\n    if any(isnan.(sd_result.second_diff)) || any(isinf.(sd_result.second_diff))\n        @warn \"NaN or Inf values detected in second differences\"\n    end\n\n    # Check for zero standard errors (indicates no interaction)\n    zero_se = sum(sd_result.se .≈ 0.0)\n    if zero_se > 0\n        @info \"$zero_se second difference(s) have zero SE (likely no interaction)\"\n    end\n\n    # Summary statistics\n    n_significant = sum(sd_result.significant)\n    println(\"Significant interactions: $n_significant / $(nrow(sd_result))\")\n    println(\"Mean absolute second difference: $(mean(abs.(sd_result.second_diff)))\")\n\n    return sd_result\nend\n\n# Usage\nsd = second_differences(ames, :x, :treated, vcov(model))\nvalidated_sd = validate_second_differences(DataFrame(sd))","category":"section"},{"location":"second_differences/#Performance-Considerations","page":"Second Differences","title":"Performance Considerations","text":"","category":"section"},{"location":"second_differences/#Computational-Complexity","page":"Second Differences","title":"Computational Complexity","text":"Second differences computation is extremely efficient:\n\nPrimary cost: Computing the underlying AMEs via population_margins()\nSecond differences calculation: Negligible additional cost (vector operations only)\nScalability: O(1) with respect to number of pairwise comparisons\n\n# Performance example\n@time ames = population_margins(model, large_data;\n                                scenarios=(education=levels,),\n                                type=:effects)\n# → 95% of total computation time\n\n@time sd = second_differences(ames, :income, :education, vcov(model))\n# → <5% of total time, even with many pairwise comparisons","category":"section"},{"location":"second_differences/#Memory-Efficiency","page":"Second Differences","title":"Memory Efficiency","text":"Second differences leverage the gradient information already stored in EffectsResult:\n\nNo additional model evaluations required\nNo dataset traversal\nMinimal additional allocations\n\nFor large-scale applications with many moderator levels, the computational bottleneck remains the AME computation, not the second differences calculation.","category":"section"},{"location":"second_differences/#Literature-and-References","page":"Second Differences","title":"Literature and References","text":"","category":"section"},{"location":"second_differences/#Key-Methodological-Papers","page":"Second Differences","title":"Key Methodological Papers","text":"Ai, C., & Norton, E. C. (2003). Interaction terms in logit and probit models. Economics Letters, 80(1), 123-129.\nSeminal paper demonstrating problems with interpreting interaction coefficients in nonlinear models\nEstablished need for marginal effects-based interaction analysis\nNorton, E. C., Wang, H., & Ai, C. (2004). Computing interaction effects and standard errors in logit and probit models. The Stata Journal, 4(2), 154-167.\nPractical implementation guidance\nStandard error computation for interaction effects\nGreene, W. H. (2010). Testing hypotheses about interaction terms in nonlinear models. Economics Letters, 107(2), 291-296.\nHypothesis testing framework for interactions\nMultiple comparison considerations\nKaraca-Mandic, P., Norton, E. C., & Dowd, B. (2012). Interaction terms in nonlinear models. Health Services Research, 47(1pt1), 255-274.\nApplied examples in health economics\nInterpretation guidance for practitioners","category":"section"},{"location":"second_differences/#Software-Implementation-References","page":"Second Differences","title":"Software Implementation References","text":"Stata: margins command with dydx() operator and at() option\nR: margins package (Leeper et al.) and interactionTest package\nPython: statsmodels.discrete.discrete_model.Logit.get_margeff() with at specification\n\nMargins.jl's second differences implementation follows this established methodological tradition while leveraging Julia's high-performance computational capabilities for efficient large-scale analysis.\n\n\n\nThis documentation provides comprehensive coverage of second differences functionality in Margins.jl. For the underlying marginal effects framework, see Mathematical Foundation. For population-based analysis that produces the AME inputs, see Population Scenarios. For integration with robust standard errors, see Advanced Features.","category":"section"},{"location":"guide/mixtures_and_contrasts/#Categorical-Mixtures-and-Manual-Contrasts","page":"Categorical Mixtures and Manual Contrasts","title":"Categorical Mixtures and Manual Contrasts","text":"","category":"section"},{"location":"guide/mixtures_and_contrasts/#Overview","page":"Categorical Mixtures and Manual Contrasts","title":"Overview","text":"When using categorical mixtures in reference grids, the package follows a clear design principle:\n\nMixtures represent population composition context, not discrete scenarios to contrast.\n\nThis means categorical variables specified as mixtures (either explicit CategoricalMixture objects or implicit frequency-weighted values for Bool variables) are skipped for automatic contrast computation in profile_margins().","category":"section"},{"location":"guide/mixtures_and_contrasts/#Why-This-Design?","page":"Categorical Mixtures and Manual Contrasts","title":"Why This Design?","text":"","category":"section"},{"location":"guide/mixtures_and_contrasts/#Mixtures-are-Context,-Not-Contrast-Targets","page":"Categorical Mixtures and Manual Contrasts","title":"Mixtures are Context, Not Contrast Targets","text":"A mixture specification like:\n\neducation = mix(\"hs\" => 0.4, \"college\" => 0.6)\n\nRepresents: \"40% high school educated, 60% college educated\" — a weighted average population context.\n\nComputing contrasts like \"0.6 college-mixture - 0.4 hs-mixture\" is clearly meaningful. Contrasts require discrete levels: \"college - hs\" is natural, but mixtures are less clearly so.","category":"section"},{"location":"guide/mixtures_and_contrasts/#Bool-Variables-with-Fractional-Values","page":"Categorical Mixtures and Manual Contrasts","title":"Bool Variables with Fractional Values","text":"Similarly, when Bool variables are filled with fractional values during grid completion (e.g., 0.796 representing 79.6% true rate), they are treated as mixture specifications and skipped for contrasts:\n\n# Bool variable filled by grid completion\nref_grid = DataFrame(treatment = [false, true])\n# other_bool_var gets filled with frequency (e.g., 0.65 for 65% true)\nresult = profile_margins(model, df, ref_grid; type=:effects)\n# [ Info: Skipping contrasts for variable other_bool_var: specified as mixture...","category":"section"},{"location":"guide/mixtures_and_contrasts/#Recommended-Workflow:-APMs-Manual-Contrasts","page":"Categorical Mixtures and Manual Contrasts","title":"Recommended Workflow: APMs + Manual Contrasts","text":"For computing contrasts between predictions at specific profiles with mixture context, use this two-step workflow:","category":"section"},{"location":"guide/mixtures_and_contrasts/#Step-1:-Compute-Adjusted-Predictions-at-Profiles-(APMs)","page":"Categorical Mixtures and Manual Contrasts","title":"Step 1: Compute Adjusted Predictions at Profiles (APMs)","text":"Use profile_margins() with type=:predictions to get predictions at specific profile points, with categorical mixtures providing population-weighted context:\n\nusing Margins, GLM, DataFrames, Statistics\n\n# Define profiles with mixture context\nref_grid = DataFrame(\n    treatment = [false, true],\n    age = [mean(df.age), mean(df.age)]\n    # education will be filled with frequency-weighted mixture\n)\n\n# Get predictions at these profiles\nresult = profile_margins(model, df, ref_grid; type=:predictions)\nresult_df = DataFrame(result; include_gradients=true)\n\nKey Point: Include include_gradients=true to enable proper delta-method standard errors for contrasts.","category":"section"},{"location":"guide/mixtures_and_contrasts/#Step-2:-Compute-Contrasts-Manually","page":"Categorical Mixtures and Manual Contrasts","title":"Step 2: Compute Contrasts Manually","text":"Use the contrast() function to compute contrasts between specific rows:\n\n# Contrast row 2 vs row 1 (treatment:true - treatment:false)\ncontrast_result = contrast(result_df, 2, 1, vcov(model))\n\nprintln(\"Treatment effect: \", contrast_result.contrast)\nprintln(\"Standard error: \", contrast_result.se)\nprintln(\"p-value: \", contrast_result.p_value)\nprintln(\"95% CI: [\", contrast_result.ci_lower, \", \", contrast_result.ci_upper, \"]\")","category":"section"},{"location":"guide/mixtures_and_contrasts/#Complete-Example","page":"Categorical Mixtures and Manual Contrasts","title":"Complete Example","text":"using Margins, GLM, DataFrames, StatsModels, CategoricalArrays, Statistics\nusing Random\nRandom.seed!(123)\n\n# Generate data\nn = 200\ndf = DataFrame(\n    education = categorical(rand([\"hs\", \"college\", \"grad\"], n)),\n    treatment = rand(Bool, n),\n    age = rand(25:65, n),\n    y = randn(n) .+ 0.5 .* df.treatment  # True treatment effect\n)\n\n# Fit model\nmodel = lm(@formula(y ~ education + treatment + age), df)\n\n# Step 1: Get predictions with education as population-weighted context\nref_grid = DataFrame(\n    treatment = [false, true],\n    age = [mean(df.age), mean(df.age)]\n)\n\nresult = profile_margins(model, df, ref_grid; type=:predictions)\nresult_df = DataFrame(result; include_gradients=true)\n\n# Step 2: Compute treatment contrast\ntreatment_effect = contrast(result_df, 2, 1, vcov(model))\n\nprintln(\"Treatment Effect (at mean age, population-weighted education):\")\nprintln(\"  Estimate: \", round(treatment_effect.contrast, digits=3))\nprintln(\"  SE: \", round(treatment_effect.se, digits=3))\nprintln(\"  95% CI: [\", round(treatment_effect.ci_lower, digits=3),\n        \", \", round(treatment_effect.ci_upper, digits=3), \"]\")","category":"section"},{"location":"guide/mixtures_and_contrasts/#Alternative-Approaches","page":"Categorical Mixtures and Manual Contrasts","title":"Alternative Approaches","text":"","category":"section"},{"location":"guide/mixtures_and_contrasts/#Option-1:-Population-Average-Contrasts-(AME)","page":"Categorical Mixtures and Manual Contrasts","title":"Option 1: Population Average Contrasts (AME)","text":"If you want contrasts averaged over the entire observed sample distribution:\n\n# Average marginal effects with contrasts\nresult = population_margins(model, df; type=:effects, contrasts=:pairwise)\n\nThis computes contrasts averaged over all observations in your data, not at specific profile points.","category":"section"},{"location":"guide/mixtures_and_contrasts/#Option-2:-Discrete-Levels-in-Reference-Grid","page":"Categorical Mixtures and Manual Contrasts","title":"Option 2: Discrete Levels in Reference Grid","text":"If you want automatic pairwise contrasts at specific profiles without mixtures:\n\n# Specify discrete levels explicitly\nref_grid = DataFrame(\n    treatment = [false, false, true, true],\n    education = categorical([\"hs\", \"college\", \"hs\", \"college\"]),\n    age = [mean(df.age), mean(df.age), mean(df.age), mean(df.age)]\n)\n\nresult = profile_margins(model, df, ref_grid; type=:effects, contrasts=:pairwise)\n\nThis computes contrasts for categorical variables at each discrete profile point.","category":"section"},{"location":"guide/mixtures_and_contrasts/#Option-3:-Quantile-or-Balanced-Grids","page":"Categorical Mixtures and Manual Contrasts","title":"Option 3: Quantile or Balanced Grids","text":"For systematic exploration of categorical × continuous interactions:\n\n# Balanced grid with all categorical levels\nref_grid = balanced_grid(df; education=:all, age=[25, 45, 65])\nresult = profile_margins(model, df, ref_grid; type=:predictions)\n\n# Then manually contrast specific rows of interest","category":"section"},{"location":"guide/mixtures_and_contrasts/#When-to-Use-Each-Approach","page":"Categorical Mixtures and Manual Contrasts","title":"When to Use Each Approach","text":"Goal Recommended Method\nContrasts at specific profiles with mixture context APM + contrast()\nPopulation-average contrasts (AME) population_margins() with type=:effects\nAutomatic pairwise contrasts at discrete profiles profile_margins() with discrete levels, contrasts=:pairwise\nComplex custom contrasts APM/MEM + contrast() for full control","category":"section"},{"location":"guide/mixtures_and_contrasts/#Design-Rationale","page":"Categorical Mixtures and Manual Contrasts","title":"Design Rationale","text":"This design follows statistical correctness principles:\n\nStatistical Validity is Paramount: Prevents meaningless \"mixture vs mixture\" contrasts\nError-First Policy: Skips invalid operations rather than approximating\nTransparency: Clear info messages guide users toward valid workflows\nPublication-Grade Standards: Manual contrast() uses proper delta-method with full covariance matrix\n\nThe APM + manual contrast() workflow provides:\n\n✅ Full statistical validity (proper delta-method standard errors)\n✅ Maximum flexibility (contrast any rows you want)\n✅ Clear interpretability (contrasts between specific profile points)\n✅ Population-weighted context (mixtures provide realistic backdrop)","category":"section"},{"location":"guide/mixtures_and_contrasts/#See-Also","page":"Categorical Mixtures and Manual Contrasts","title":"See Also","text":"API Reference - See contrast() function documentation\nReference Grid Guide\nProfile Margins","category":"section"},{"location":"advanced/#Advanced-Features","page":"Advanced Features","title":"Advanced Features","text":"Elasticities, robust standard errors, and specialized analysis techniques","category":"section"},{"location":"advanced/#Elasticities-and-Semi-Elasticities","page":"Advanced Features","title":"Elasticities and Semi-Elasticities","text":"Margins.jl provides comprehensive elasticity support through the measure parameter, following the same Population vs Profile framework as standard marginal effects. For conceptual background on the 2×2 framework, see Mathematical Foundation.","category":"section"},{"location":"advanced/#Types-of-Elasticity-Measures","page":"Advanced Features","title":"Types of Elasticity Measures","text":"","category":"section"},{"location":"advanced/#Standard-Elasticity","page":"Advanced Features","title":"Standard Elasticity","text":"Definition: Percent change in Y per percent change in X Formula: (∂Y/∂X) × (X/Y) Interpretation: \"A 1% increase in X leads to an ε% change in Y\"\n\n# Population average elasticities\npopulation_margins(model, data; type=:effects, measure=:elasticity)\n\n# Elasticities at sample means\nprofile_margins(model, data, means_grid(data); type=:effects, measure=:elasticity)","category":"section"},{"location":"advanced/#Semi-Elasticity-with-respect-to-X","page":"Advanced Features","title":"Semi-Elasticity with respect to X","text":"Definition: Percent change in Y per unit change in X Formula: (∂Y/∂X) × (1/Y) Interpretation: \"A 1-unit increase in X leads to a (100×ε)% change in Y\"\n\n# Population average semi-elasticities (X)\npopulation_margins(model, data; type=:effects, measure=:semielasticity_dyex)\n\n# Semi-elasticities at specific scenarios\nprofile_margins(model, data, cartesian_grid(x1=[0,1,2]); type=:effects, measure=:semielasticity_dyex)","category":"section"},{"location":"advanced/#Semi-Elasticity-with-respect-to-Y","page":"Advanced Features","title":"Semi-Elasticity with respect to Y","text":"Definition: Unit change in Y per percent change in X   Formula: (∂Y/∂X) × X Interpretation: \"A 1% increase in X leads to an ε-unit change in Y\"\n\n# Population average semi-elasticities (Y)\npopulation_margins(model, data; type=:effects, measure=:semielasticity_eydx)","category":"section"},{"location":"advanced/#Elasticity-Framework-Application","page":"Advanced Features","title":"Elasticity Framework Application","text":"Elasticities follow the same Population vs Profile distinction as marginal effects (see Mathematical Foundation for detailed framework explanation):\n\nMeasure Population Approach Profile Approach\nElasticity Average of (∂Y/∂X) × (Xᵢ/Yᵢ) across sample (∂Y/∂X) × (X̄/Ȳ) at representative values\nSemi-elasticity (X) Average of (∂Y/∂X) × (1/Yᵢ) across sample (∂Y/∂X) × (1/Ȳ) at representative values\nSemi-elasticity (Y) Average of (∂Y/∂X) × Xᵢ across sample (∂Y/∂X) × X̄ at representative values","category":"section"},{"location":"advanced/#Practical-Example:-Wage-Elasticities","page":"Advanced Features","title":"Practical Example: Wage Elasticities","text":"using Margins, DataFrames, GLM\n\n# Economic data: wages, education, experience\ndf = DataFrame(\n    log_wage = randn(1000) .+ 2.5, # Log wages\n    education = rand(12:20, 1000), # Years of education  \n    experience = rand(0:30, 1000), # Years of experience\n    age = rand(25:55, 1000)\n)\n\nmodel = lm(@formula(log_wage ~ education + experience + age), df)\n\n# Education elasticity of wages (population average)\nedu_elasticity = population_margins(model, df; \n                                   vars=[:education], \n                                   measure=:elasticity)\nprintln(\"Population average education elasticity: \", DataFrame(edu_elasticity))\n\n# Education elasticity at different experience levels (profile analysis)  \nexp_scenarios = profile_margins(\n    model, df,\n    cartesian_grid(experience=[0, 10, 20, 30]);\n    vars = [:education],\n    measure = :elasticity\n)\nprintln(\"Education elasticity by experience level:\")\nprintln(DataFrame(exp_scenarios))\n\n# Semi-elasticity: percent wage change per year of education\nedu_semielast = population_margins(\n    model, df;\n    vars = [:education],\n    measure = :semielasticity_dyex\n)\nprintln(\"Education semi-elasticity: \", DataFrame(edu_semielast))","category":"section"},{"location":"advanced/#When-Profile-Population-for-Elasticities","page":"Advanced Features","title":"When Profile ≠ Population for Elasticities","text":"In GLMs with non-identity links, population and profile elasticities can differ substantially:\n\n# Logistic model example\nlogit_model = glm(@formula(employed ~ education + experience), df, Binomial(), LogitLink())\n\n# Population average employment elasticity w.r.t. education\npop_elastic = population_margins(logit_model, df; vars=[:education], measure=:elasticity)\n\n# Employment elasticity at sample means\nprof_elastic = profile_margins(logit_model, df, means_grid(df); vars=[:education], measure=:elasticity)\n\n# These will differ because logistic function is nonlinear\nprintln(\"Population elasticity: \", DataFrame(pop_elastic).estimate[1])\nprintln(\"Profile elasticity: \", DataFrame(prof_elastic).estimate[1])","category":"section"},{"location":"advanced/#Robust-Standard-Errors","page":"Advanced Features","title":"Robust Standard Errors","text":"Margins.jl integrates CovarianceMatrices.jl for robust standard error computation.","category":"section"},{"location":"advanced/#Basic-Robust-Standard-Errors","page":"Advanced Features","title":"Basic Robust Standard Errors","text":"","category":"section"},{"location":"advanced/#Heteroskedasticity-Robust-(White/Huber-White)","page":"Advanced Features","title":"Heteroskedasticity-Robust (White/Huber-White)","text":"using CovarianceMatrices\n\n# Apply robust covariance via vcov parameter\nrobust_effects = population_margins(\n    model, data; vcov=HC1(), type=:effects\n)","category":"section"},{"location":"advanced/#Available-Robust-Estimators","page":"Advanced Features","title":"Available Robust Estimators","text":"# Different heteroskedasticity-robust variants\nHC0()  # Basic White estimator\nHC1()  # Degrees-of-freedom adjusted (most common)\nHC2()  # Leverage-adjusted  \nHC3()  # Jackknife-type\nHC4()  # High-leverage robust\nHC5()  # Outlier-robust\n\n# Example with HC3\nresult = population_margins(model, data; vcov=HC3())","category":"section"},{"location":"advanced/#Clustered-Standard-Errors","page":"Advanced Features","title":"Clustered Standard Errors","text":"","category":"section"},{"location":"advanced/#Single-Level-Clustering","page":"Advanced Features","title":"Single-Level Clustering","text":"# Cluster by firm ID\nclustered_effects = population_margins(model, data;\n    vcov=Clustered(:firm_id), type=:effects)","category":"section"},{"location":"advanced/#Multi-Level-Clustering","page":"Advanced Features","title":"Multi-Level Clustering","text":"# Two-way clustering (firm and year)\nresult = population_margins(model, data; vcov=Clustered([:firm_id, :year]))","category":"section"},{"location":"advanced/#HAC-(Heteroskedasticity-and-Autocorrelation-Consistent)-Standard-Errors","page":"Advanced Features","title":"HAC (Heteroskedasticity and Autocorrelation Consistent) Standard Errors","text":"# Newey-West HAC estimator\neffects_hac = population_margins(model, data;\n    vcov=HAC(Bartlett()), type=:effects)","category":"section"},{"location":"advanced/#Custom-Covariance-Providers","page":"Advanced Features","title":"Custom Covariance Providers","text":"","category":"section"},{"location":"advanced/#Function-Based-Covariance","page":"Advanced Features","title":"Function-Based Covariance","text":"# Custom covariance function (must return an AbstractMatrix)\nfunction my_robust_vcov(model)\n    # ... compute covariance from model ...\n    return Σ::AbstractMatrix\nend\n\n# Use custom function directly\nresult = population_margins(model, data; vcov=my_robust_vcov)","category":"section"},{"location":"advanced/#Robust-Standard-Errors-with-Elasticities","page":"Advanced Features","title":"Robust Standard Errors with Elasticities","text":"Robust standard errors work seamlessly with all elasticity measures:\n\n# Robust elasticity estimates\nrobust_elasticities = population_margins(model, data;\n    vcov=HC1(),\n    measure=:elasticity, type=:effects)\n\n# Profile elasticities with clustered SEs\nprofile_elasticities = profile_margins(model, data,\n    means_grid(data); vcov = Clustered(:cluster_var),\n    measure = :elasticity)","category":"section"},{"location":"advanced/#Standard-Errors-for-Elasticities:-Delta-Method-vs-Bootstrap","page":"Advanced Features","title":"Standard Errors for Elasticities: Delta Method vs Bootstrap","text":"warning: Conditional vs Unconditional Inference\nStandard errors for elasticity measures computed via the delta method (default) represent conditional inference; they assume the observed data (X, Y) are fixed and only account for uncertainty in parameter estimates β̂. Bootstrap standard errors represent unconditional inference and may be larger because they also account for sampling variation in the data.","category":"section"},{"location":"advanced/#Understanding-the-Difference","page":"Advanced Features","title":"Understanding the Difference","text":"For elasticity measures like ε = (x̄/ȳ) × (∂y/∂x), the transformation involves sample moments (x̄, ȳ) that are treated differently by different inference methods:\n\nDelta Method (Default):\n\nAssumption: Observed data X and Y are fixed constants\nUncertainty source: Only parameter estimates β̂\nVariance: Var[ε(β̂) | X, Y]\nAdvantages: Fast, analytically exact (given the conditional assumption)\nImplementation: Uses the quotient rule to account for mean(ŷ) depending on β\n\nBootstrap:\n\nAssumption: Data are sampled from a population\nUncertainty sources: Both β̂ and the sample moments x̄, ȳ\nVariance: Var[ε(β̂, x̄, ȳ)]\nAdvantages: Accounts for full sampling variation\nTrade-off: Computationally intensive (requires refitting model many times)","category":"section"},{"location":"advanced/#Why-They-Differ","page":"Advanced Features","title":"Why They Differ","text":"The key distinction is that elasticity formulas involve ratios of sample statistics:\n\n# For elasticity: ε = (x̄ / mean(ŷ)) × AME\n# Where:\n#   x̄ = sample mean of predictor (varies across bootstrap samples)\n#   mean(ŷ) = sample mean of predictions (varies with both β and resampled data)\n#   AME = average marginal effect (varies with β)\n\nWhen you bootstrap:\n\nDifferent observations are sampled → x̄ changes\nModel is refit → β̂ changes → AME changes\nPredictions are recomputed → mean(ŷ) changes\n\nThe delta method only captures variation from (2), treating (1) and parts of (3) as fixed.\n\nnote: This is Not a Bug\nThis behavior matches other statistical software:R's marginaleffects: Documentation states \"For nonlinear models, the delta method is only an approximation\" and recommends bootstrap for transformations\nStata's margins: Documents that delta method \"assumes that the values at which the covariates are evaluated are fixed\"","category":"section"},{"location":"advanced/#Example:-Comparing-Methods","page":"Advanced Features","title":"Example: Comparing Methods","text":"using Margins, GLM, DataFrames, Bootstrap\n\n# Fit model\nmodel = lm(@formula(y ~ x + z), data)\n\n# Delta method SEs (default - fast, conditional)\ndelta_result = population_margins(model, data;\n    vars=[:x],\n    measure=:elasticity)\nprintln(\"Delta method SE: \", DataFrame(delta_result).se[1])\n\n# Bootstrap SEs (slower, unconditional)\n# Note: Built-in bootstrap support coming soon\n# For now, use manual bootstrap:\nfunction boot_elasticity(data, indices)\n    boot_data = data[indices, :]\n    boot_model = lm(@formula(y ~ x + z), boot_data)\n    boot_result = population_margins(boot_model, boot_data;\n        vars=[:x], measure=:elasticity)\n    return DataFrame(boot_result).estimate[1]\nend\n\nbs = bootstrap(boot_elasticity, data, BasicSampling(1000))\nboot_se = std(bs.t[1])\nprintln(\"Bootstrap SE: \", boot_se)\n\n# Bootstrap SE will typically be larger, especially in small samples","category":"section"},{"location":"advanced/#Technical-Details","page":"Advanced Features","title":"Technical Details","text":"Margins.jl implements the full quotient rule for elasticity derivatives:\n\nfracpartial varepsilonpartial beta = fracbarxbary fracpartial textAMEpartial beta - fracvarepsilonbary fracpartial barypartial beta\n\nHere, the second term accounts for mean(ŷ) depending on β.\n\ninfo: References\nKrinsky, I., & Robb, A. L. (1986). \"On Approximating the Statistical Properties of Elasticities.\" Review of Economics and Statistics, 68(4), 715-719.\nArel-Bundock, V. (2023). \"marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests.\" R package. Documentation on inference\nGreene, W. H. (2018). Econometric Analysis (8th ed.), Section 3.6 on the Delta Method.","category":"section"},{"location":"advanced/#Standardized-Predictors","page":"Advanced Features","title":"Standardized Predictors","text":"Margins.jl seamlessly integrates with StandardizedPredictors.jl for models fit with standardized (z-scored) variables. Marginal effects are automatically reported on the original (raw) scale, requiring no manual back-transformation.","category":"section"},{"location":"advanced/#Why-Standardize-Predictors?","page":"Advanced Features","title":"Why Standardize Predictors?","text":"Standardization transforms variables to have mean 0 and standard deviation 1:\n\nx_std = (x - mean(x)) / std(x)","category":"section"},{"location":"advanced/#Integration-with-Margins.jl","page":"Advanced Features","title":"Integration with Margins.jl","text":"using Margins, GLM, StandardizedPredictors, DataFrames\n\n# Fit model with standardized income\ndf = DataFrame(\n    sales = randn(1000) .* 10000 .+ 50000,\n    income = randn(1000) .* 20000 .+ 50000,  # mean ≈ \\$50k, std ≈ \\$20k\n    age = randn(1000) .* 10 .+ 40\n)\n\nmodel = lm(@formula(sales ~ income + age), df,\n           contrasts = Dict(:income => ZScore(), :age => ZScore()))\n\n# Marginal effects are automatically on ORIGINAL scale\nresult = population_margins(model, df; type=:effects, vars=[:income, :age])\nDataFrame(result)\n\n# income effect: change in sales per \\$1 increase in income (not per SD!)\n# age effect: change in sales per 1-year increase in age (not per SD!)","category":"section"},{"location":"advanced/#How-Automatic-Back-Transformation-Works","page":"Advanced Features","title":"How Automatic Back-Transformation Works","text":"When computing marginal effects, Margins.jl uses FormulaCompiler.jl's derivative system, which automatically applies the chain rule through the standardization transformation:\n\nMathematical detail (with ZScore):\n\nModel uses: x_std = (x - μ) / σ\nDerivative computation: ∂η/∂x_raw = ∂η/∂x_std × ∂x_std/∂x_raw = β × (1/σ)\nResult: Marginal effect per unit of original variable\n\nBoth finite differences (FD) and automatic differentiation (AD) backends handle this automatically:\n\nFD: Perturbs raw values → standardization applied during evaluation → derivative includes 1/σ\nAD: Dual arithmetic propagates through (x - μ)/σ → derivative includes 1/σ","category":"section"},{"location":"advanced/#Comparison:-Raw-vs-Standardized-Models","page":"Advanced Features","title":"Comparison: Raw vs Standardized Models","text":"# Fit both raw and standardized models\nmodel_raw = lm(@formula(sales ~ income), df)\nmodel_std = lm(@formula(sales ~ income), df, contrasts=Dict(:income => ZScore()))\n\n# Marginal effects are IDENTICAL (both on original scale)\nme_raw = population_margins(model_raw, df; vars=[:income])\nme_std = population_margins(model_std, df; vars=[:income])\n\n# Both give same result: effect per dollar of income\n@assert DataFrame(me_raw).estimate ≈ DataFrame(me_std).estimate\n\nWhy they match:\n\nRaw model: ∂sales/∂income_dollars = β₁\nStandardized model: ∂sales/∂income_dollars = β₁_std / σ_income\nThe σ in the denominator is automatically included by the chain rule","category":"section"},{"location":"advanced/#Elasticities-with-Standardized-Predictors","page":"Advanced Features","title":"Elasticities with Standardized Predictors","text":"Elasticities are invariant to standardization because they are scale-free:\n\n# Elasticity with standardized predictors\nmodel = lm(@formula(sales ~ income + age), df,\n           contrasts = Dict(:income => ZScore()))\n\n# Elasticity uses raw values of X and Y\nresult = population_margins(model, df; vars=[:income], measure=:elasticity)\n\n# Interpretation: % change in sales per % change in income\n# (same whether predictors are standardized or not)","category":"section"},{"location":"advanced/#Profile-Analysis-with-Standardized-Predictors","page":"Advanced Features","title":"Profile Analysis with Standardized Predictors","text":"Reference grids work directly with raw values:\n\n# Specify scenarios in original units\nusing Margins: cartesian_grid\n\ngrid = cartesian_grid(\n    income = [40000, 60000, 80000],  # Raw dollar amounts\n    age = [30, 40, 50]                # Raw years\n)\n\nresult = profile_margins(model, df, grid; type=:effects)\n\n# Effects are per dollar of income, per year of age\n# Standardization is handled automatically during evaluation","category":"section"},{"location":"advanced/#Technical-Notes","page":"Advanced Features","title":"Technical Notes","text":"Model coefficients (coef(model)) are on the standardized scale\nβ₁ represents effect per SD change in x\nJacobian from FormulaCompiler is on the raw scale\nIncludes 1/σ factor from chain rule automatically\nMarginal effects: g = J' × β\nThe 1/σ in J combines with standardized β to give raw-scale effects\n\nThis behavior is validated with tests that compare the raw and standardized models, ensuring that both produce identical marginal effects on the original measurement scales.","category":"section"},{"location":"advanced/#Categorical-Mixtures-for-Policy-Analysis","page":"Advanced Features","title":"Categorical Mixtures for Policy Analysis","text":"Margins.jl supports categorical mixtures for scenario analysis, which enables the specification of population compositions as an alternative to (the observed) category levels.","category":"section"},{"location":"advanced/#Motivation:-Realistic-Population-Scenarios","page":"Advanced Features","title":"Motivation: Realistic Population Scenarios","text":"Marginal effects often use arbitrary categorical values (e.g., \"set all observations to treatment=1\"). Categorical mixtures enable the specification of typical values:\n\nusing CategoricalArrays, Margins\n\n# Instead of: \"All treated\" (unrealistic)\nunrealistic = profile_margins(\n    model, data, cartesian_grid(treatment = [1]); type = :predictions\n)\n\n# Use: Realistic treatment rate  \nrealistic = profile_margins(\n    model, data, \n    DataFrame(treatment=[mix(0 => 0.3, 1 => 0.7)])\n) # 70% treatment rate","category":"section"},{"location":"advanced/#Frequency-Weighted-Categorical-Defaults","page":"Advanced Features","title":"Frequency-Weighted Categorical Defaults","text":"When categorical variables are unspecified in profiles, Margins.jl uses actual sample frequencies rather than arbitrary first levels:\n\n# Data composition:\n#     education = 40% HS, 45% College, 15% Graduate\n#     region = 60% Urban, 40% Rural\n\n# Effects \"at means\" uses realistic composition\nresult = profile_margins(model, data, means_grid(data); type = :effects)\n# → Continuous vars: sample means\n# → education: mix(\"HS\" => 0.40, \"College\" => 0.45, \"Graduate\" => 0.15)  \n# → region: mix(\"Urban\" => 0.60, \"Rural\" => 0.40)","category":"section"},{"location":"advanced/#Scenario-Analysis","page":"Advanced Features","title":"Scenario Analysis","text":"","category":"section"},{"location":"advanced/#Demographic-Transition-Scenarios","page":"Advanced Features","title":"Demographic Transition Scenarios","text":"# Current population composition\ncurrent_scenario = profile_margins(model, data,\n    DataFrame(education=[mix(\"HS\" => 0.40, \"College\" => 0.45, \"Graduate\" => 0.15)]);\n    type=:predictions)\n\n# Future scenario: increased college graduation\nfuture_scenario = profile_margins(model, data,\n    DataFrame(education=[mix(\"HS\" => 0.25, \"College\" => 0.60, \"Graduate\" => 0.15)]);\n    type=:predictions)\n\n# Policy impact\nimpact = DataFrame(future_scenario).estimate[1] - DataFrame(current_scenario).estimate[1]","category":"section"},{"location":"advanced/#Treatment-Effect-Heterogeneity","page":"Advanced Features","title":"Treatment Effect Heterogeneity","text":"# Treatment effects across population compositions\ntreatment_scenarios = DataFrame([\n    (treatment = 0, education = mix(\"HS\" => 0.5, \"College\" => 0.5)),\n    (treatment = 1, education = mix(\"HS\" => 0.5, \"College\" => 0.5)),\n    (treatment = 0, education = mix(\"HS\" => 0.2, \"College\" => 0.8)),  \n    (treatment = 1, education = mix(\"HS\" => 0.2, \"College\" => 0.8))\n])\n\nresults = profile_margins(\n    model, data, treatment_scenarios; type = :predictions\n)\ntreatment_effects_df = DataFrame(results)","category":"section"},{"location":"advanced/#Advanced-Grouping-and-Stratification","page":"Advanced Features","title":"Advanced Grouping and Stratification","text":"Margins.jl provides a comprehensive grouping framework for population-based marginal effects analysis, supporting hierarchical stratification patterns that extend far beyond traditional approaches.","category":"section"},{"location":"advanced/#Hierarchical-Grouping-Framework","page":"Advanced Features","title":"Hierarchical Grouping Framework","text":"","category":"section"},{"location":"advanced/#Basic-Grouping-Patterns","page":"Advanced Features","title":"Basic Grouping Patterns","text":"# Simple categorical grouping\ndemographic_effects = population_margins(\n    model, data;\n    type = :effects, vars = [:income], groups = :education\n)\n\n# Cross-tabulated grouping\ncross_effects = population_margins(\n    model, data;\n    type = :effects,\n    vars = [:income], \n    groups = [:education, :region]\n)","category":"section"},{"location":"advanced/#Nested-Hierarchical-Grouping","page":"Advanced Features","title":"Nested Hierarchical Grouping","text":"# Hierarchical nesting: region → education within each region\nnested_effects = population_margins(\n    model, data;\n    type = :effects,\n    vars = [:income],\n    groups = :region => :education\n)\n\n# Deep nesting: region → urban → education\ndeep_nested = population_margins(\n    model, data;\n    type = :effects,\n    groups = :region => (:urban => :education)\n)","category":"section"},{"location":"advanced/#Continuous-Variable-Binning","page":"Advanced Features","title":"Continuous Variable Binning","text":"# Quartile analysis\nincome_quartiles = population_margins(\n    model, data;\n    type = :effects,\n    groups = (:income, 4) # Q1, Q2, Q3, Q4\n)\n\n# Custom policy-relevant thresholds\npolicy_thresholds = population_margins(\n    model, data;\n    type = :effects,\n    groups = (:income, [25000, 50000, 75000])\n)\n\n# Mixed categorical and continuous\nmixed_groups = population_margins(\n    model, data;\n    type = :effects,\n    groups = [:education, (:income, 4)]\n)","category":"section"},{"location":"advanced/#Counterfactual-Scenario-Analysis","page":"Advanced Features","title":"Counterfactual Scenario Analysis","text":"","category":"section"},{"location":"advanced/#Policy-Scenarios-with-Population-Override","page":"Advanced Features","title":"Policy Scenarios with Population Override","text":"# Binary policy scenarios\npolicy_analysis = population_margins(\n    model, data;\n    type = :effects,\n    vars = [:outcome],\n    scenarios = (:policy => [0, 1])\n)\n\n# Multi-variable scenarios\ncomplex_scenarios = population_margins(\n    model, data;\n    type = :effects,\n    scenarios = (:treatment => [0, 1], \n    :policy => [\"current\", \"reform\"])\n)","category":"section"},{"location":"advanced/#Combined-Grouping-and-Scenarios","page":"Advanced Features","title":"Combined Grouping and Scenarios","text":"# Comprehensive policy analysis: demographics × policy scenarios\nfull_analysis = population_margins(\n    model, data;\n    type = :effects,\n    vars = [:outcome],\n    groups = [:education, :region],\n    scenarios = (:treatment => [0, 1])\n)","category":"section"},{"location":"advanced/#Complex-Nested-Patterns","page":"Advanced Features","title":"Complex Nested Patterns","text":"","category":"section"},{"location":"advanced/#Parallel-Grouping-Within-Hierarchy","page":"Advanced Features","title":"Parallel Grouping Within Hierarchy","text":"# Region → (education levels + income quartiles separately)\nparallel_groups = population_margins(\n    model, data;\n    type = :effects,\n    groups = :region => [:education, (:income, 4)]\n)","category":"section"},{"location":"advanced/#Advanced-Policy-Applications","page":"Advanced Features","title":"Advanced Policy Applications","text":"# Healthcare policy analysis\nhealthcare_analysis = population_margins(\n    health_model, health_data;\n    type = :effects,\n    groups = :state => (:urban => [:insurance_type, (:income, 3)]),\n    scenarios = (:policy_reform => [0, 1], :funding_level => [0.8, 1.2])\n)\n\n# Results: State × Urban/Rural × (Insurance×Income-Tertiles) × Policy×Funding scenarios","category":"section"},{"location":"advanced/#Second-Differences-(Interaction-Effects)","page":"Advanced Features","title":"Second Differences (Interaction Effects)","text":"For comprehensive coverage of second differences—interaction effects on the predicted outcome scale—see the dedicated Second Differences guide. Second differences quantify how marginal effects vary across moderator levels, extending the Margins.jl framework to address effect heterogeneity questions.\n\nQuick reference:\n\n# Compute AMEs across modifier levels\names = population_margins(\n    model, data; scenarios = (treated=[0,1],), type = :effects\n)\n\n# Calculate second differences\nsd = second_differences(ames, :age, :treated, vcov(model))\n\nAvailable functions: second_differences(), second_difference(), second_differences_pairwise(), second_differences_all_contrasts().","category":"section"},{"location":"advanced/#Error-Handling-and-Diagnostics","page":"Advanced Features","title":"Error Handling and Diagnostics","text":"","category":"section"},{"location":"advanced/#Robust-Error-Detection","page":"Advanced Features","title":"Robust Error Detection","text":"# Check for statistical validity issues\nfunction validate_margins_result(result::MarginsResult)\n    df = DataFrame(result)\n    \n    # Check for excessive standard errors (potential issues)\n    large_se = df[df.se .> 10 * abs.(df.estimate), :]\n    if nrow(large_se) > 0\n        @warn \"Large standard errors detected - potential statistical issues\"\n        println(large_se)\n    end\n    \n    # Check for missing values\n    missing_results = df[ismissing.(df.estimate) .| ismissing.(df.se), :]\n    if nrow(missing_results) > 0\n        @warn \"Missing values in results - check model specification\"\n    end\n    \n    return df\nend\n\n# Usage\nresult = population_margins(model, data)\nvalidated_df = validate_margins_result(result)","category":"section"},{"location":"advanced/#Covariance-Matrix-Diagnostics","page":"Advanced Features","title":"Covariance Matrix Diagnostics","text":"# Check covariance matrix properties\nfunction diagnose_vcov(model)\n    Σ = vcov(model)\n    \n    # Check positive definiteness\n    eigenvals = eigvals(Σ)\n    if any(eigenvals .< 1e-12)\n        @warn \"Covariance matrix near-singular - standard errors may be unreliable\"\n    end\n    \n    # Check condition number\n    cond_num = cond(Σ)\n    if cond_num > 1e12\n        @warn \"Poorly conditioned covariance matrix - numerical issues possible\"\n    end\n    \n    return (eigenvals=eigenvals, condition_number=cond_num)\nend","category":"section"},{"location":"caching/#Caching-System-(Internal-Architecture)","page":"Caching System (Internal Architecture)","title":"Caching System (Internal Architecture)","text":"Note: This document describes internal implementation details of the Margins.jl caching system. The caching mechanism is fully automatic and requires no user intervention. This documentation is provided for developers, contributors, and users interested in understanding performance characteristics.","category":"section"},{"location":"caching/#User-Perspective:-Automatic-Caching","page":"Caching System (Internal Architecture)","title":"User Perspective: Automatic Caching","text":"TL;DR for Users: Margins.jl automatically caches compiled formula evaluators. The first call to population_margins() or profile_margins() with a given model compiles the evaluator (~1-10ms overhead), and subsequent calls reuse the cached version (microsecond overhead). No user action required.\n\n# First call: includes compilation overhead\n@time result1 = population_margins(model, data; type=:effects)  # ~5ms\n\n# Second call: uses cached evaluator\n@time result2 = population_margins(model, data; type=:effects)  # ~0.3ms (15x faster!)\n\n# Different parameters create new cache entry\n@time result3 = population_margins(model, data; type=:predictions)  # ~5ms (new compilation)\n\n","category":"section"},{"location":"caching/#Implementation-Details","page":"Caching System (Internal Architecture)","title":"Implementation Details","text":"","category":"section"},{"location":"caching/#Introduction","page":"Caching System (Internal Architecture)","title":"Introduction","text":"The computation of marginal effects involves repeated evaluation of compiled formula representations and their derivatives. To minimize computational overhead, Margins.jl implements a caching mechanism that preserves compiled evaluators and their associated memory buffers across multiple invocations. This section describes the architecture and implementation of this caching system.","category":"section"},{"location":"caching/#System-Architecture","page":"Caching System (Internal Architecture)","title":"System Architecture","text":"","category":"section"},{"location":"caching/#Cache-Structure","page":"Caching System (Internal Architecture)","title":"Cache Structure","text":"The package maintains a global dictionary that maps configuration hashes to pre-compiled engine instances:\n\nconst ENGINE_CACHE = Dict{UInt64, MarginsEngine}()\n\nEach MarginsEngine encapsulates a compiled formula evaluator from FormulaCompiler.jl along with pre-allocated buffers necessary for computation. The cache key uniquely identifies the computational context through a hash of relevant parameters.","category":"section"},{"location":"caching/#Configuration-Identification","page":"Caching System (Internal Architecture)","title":"Configuration Identification","text":"The cache key encompasses all parameters that affect the computational procedure:\n\ncache_key = hash((\n    usage,                      # Computational pattern (population or profile)\n    deriv,                      # Derivative requirement specification\n    model,                      # Statistical model with fitted parameters\n    keys(data_nt),             # Data structure specification\n    vars,                      # Variables requiring derivative computation\n    typeof(model),             # Model type for method dispatch\n    fieldnames(typeof(model)), # Model structure specification\n    vcov                       # Variance-covariance specification\n))\n\nThis comprehensive key ensures that distinct computational contexts maintain separate cache entries, preventing inadvertent reuse of incompatible compiled structures.","category":"section"},{"location":"caching/#Retrieval-Mechanism","page":"Caching System (Internal Architecture)","title":"Retrieval Mechanism","text":"The cache employs a standard memoization pattern through Julia's get! function:\n\nfunction get_or_build_engine(usage, deriv, model, data_nt, vars, vcov)\n    cache_key = hash(...)\n\n    return get!(ENGINE_CACHE, cache_key) do\n        build_engine(usage, deriv, model, data_nt, vars, vcov)\n    end\nend\n\nWhen a configuration is encountered for the first time, the system constructs a new engine instance. Subsequent requests with identical configurations retrieve the existing instance without recompilation.","category":"section"},{"location":"caching/#Computational-Implications","page":"Caching System (Internal Architecture)","title":"Computational Implications","text":"","category":"section"},{"location":"caching/#Compilation-Overhead-Reduction","page":"Caching System (Internal Architecture)","title":"Compilation Overhead Reduction","text":"The construction of formula evaluators involves several computationally intensive operations:\n\nFormula parsing and algebraic expansion\nCategorical variable encoding and level extraction\nInteraction term construction and indexing\nDerivative graph construction for automatic differentiation\nMemory buffer allocation and sizing\n\nThrough caching, these operations occur once per unique configuration rather than for each marginal effects computation. Consider the following timing comparison:\n\n# Initial computation requires compilation\n@time result1 = population_margins(model, data; vars=[:x1, :x2])\n# 0.003456 seconds\n\n# Subsequent computation uses cached evaluator\n@time result2 = population_margins(model, data; vars=[:x1, :x2])\n# 0.000234 seconds\n\nThe order-of-magnitude reduction in computation time reflects the elimination of compilation overhead.","category":"section"},{"location":"caching/#Memory-Buffer-Management","page":"Caching System (Internal Architecture)","title":"Memory Buffer Management","text":"Each cached engine maintains pre-allocated buffers sized according to its usage pattern:\n\nstruct MarginsEngine{L, U, D}\n    compiled::FormulaCompiler.UnifiedCompiled\n    de::Union{FormulaCompiler.DerivativeEvaluator, Nothing}\n\n    # Pre-allocated computation buffers\n    g_buf::Vector{Float64}           # Gradient storage\n    gβ_accumulator::Vector{Float64}  # Accumulator for averaged effects\n    η_buf::Vector{Float64}           # Linear predictor values\n    row_buf::Vector{Float64}         # Design matrix row storage\n\n    # Model parameters and metadata\n    model::Any\n    β::Vector{Float64}\n    Σ::Matrix{Float64}\n    link::L\n    vars::Vector{Symbol}\n    data_nt::NamedTuple\nend\n\nThe buffer sizing strategy differs between usage patterns:\n\nPopulation analysis: Buffers sized for single-row operations to minimize memory footprint\nProfile analysis: Larger buffers accommodate multiple profile evaluations simultaneously","category":"section"},{"location":"caching/#Type-Specialization","page":"Caching System (Internal Architecture)","title":"Type Specialization","text":"The caching system leverages Julia's type system to eliminate runtime dispatch overhead. The DerivativeSupport type parameter enables compile-time specialization:\n\n# Continuous variables requiring derivatives\nengine = get_or_build_engine(PopulationUsage, HasDerivatives, ...)\n# Returns: MarginsEngine{..., PopulationUsage, HasDerivatives}\n\n# Categorical variables without derivatives\nengine = get_or_build_engine(ProfileUsage, NoDerivatives, ...)\n# Returns: MarginsEngine{..., ProfileUsage, NoDerivatives}\n\nThis parametric typing eliminates Union type overhead in computational kernels.","category":"section"},{"location":"caching/#Practical-Considerations","page":"Caching System (Internal Architecture)","title":"Practical Considerations","text":"","category":"section"},{"location":"caching/#Cache-Entry-Differentiation","page":"Caching System (Internal Architecture)","title":"Cache Entry Differentiation","text":"The caching system creates distinct entries for configurations that differ in any computationally relevant aspect:\n\nModel variation: Different fitted models, even with identical formulas, maintain separate cache entries due to distinct coefficient values.\nCovariance specification: Each variance-covariance estimator (e.g., robust, clustered) requires its own cache entry as it affects standard error computation.\nVariable selection: Different sets of variables for marginal effects analysis necessitate distinct derivative evaluators.\nUsage patterns: Population and profile analyses employ different buffer sizing strategies and thus maintain separate cache entries.","category":"section"},{"location":"caching/#Memory-Management","page":"Caching System (Internal Architecture)","title":"Memory Management","text":"The memory footprint of cached engines scales with the number of unique configurations encountered during a session. For a model with p predictors, each engine requires approximately O(p²) memory, dominated by the variance-covariance matrix storage.\n\nTypical usage patterns exhibit bounded cache growth:\n\nStandard analysis sessions: 1-10 unique configurations\nComplex comparative studies: 10-100 configurations\nLong-running services: Periodic cache clearing may be warranted","category":"section"},{"location":"caching/#Internal-Cache-Management-(Not-Exported)","page":"Caching System (Internal Architecture)","title":"Internal Cache Management (Not Exported)","text":"For advanced users and developers, the package provides internal utilities for cache management. These are not exported and require qualified access:\n\n# Query cache statistics (internal function)\nstats = Margins.get_cache_stats()\n# Returns: (entries = n, memory_estimate = bytes, keys = [...])\n\n# Clear cache to reclaim memory (internal function)\nMargins.clear_engine_cache!()\n\nNote: These functions are primarily for debugging and development. Most users will never need to call them directly, as Julia's garbage collection handles memory management automatically when MarginsEngine instances are no longer referenced.","category":"section"},{"location":"caching/#Performance-Characteristics","page":"Caching System (Internal Architecture)","title":"Performance Characteristics","text":"Empirical measurements demonstrate the performance impact of caching for a generalized linear model with 10 predictors and 100,000 observations:\n\nComputation Type Initial (ms) Cached (ms) Ratio\nPopulation margins 45 3 15:1\nProfile margins (10 profiles) 8 0.5 16:1\nBootstrap standard errors (1000 iterations) 45,000 3,000 15:1\n\nThese measurements reflect the elimination of compilation overhead while preserving computational correctness.","category":"section"},{"location":"caching/#Implementation-Details-2","page":"Caching System (Internal Architecture)","title":"Implementation Details","text":"","category":"section"},{"location":"caching/#Thread-Safety-Considerations","page":"Caching System (Internal Architecture)","title":"Thread Safety Considerations","text":"The current implementation does not provide thread-safe cache access. Concurrent access from multiple threads may result in race conditions. For parallel computation, two strategies are available:\n\nPre-populate the cache before initiating parallel computation\nImplement thread-local caches (not currently supported)","category":"section"},{"location":"caching/#Cache-Invalidation","page":"Caching System (Internal Architecture)","title":"Cache Invalidation","text":"The cache does not automatically detect changes to model coefficients that occur outside the standard fitting procedures. Manual coefficient modification requires explicit cache clearing:\n\n# Fit model and compute margins\nmodel = lm(@formula(y ~ x), data)\nresult1 = population_margins(model, data)\n\n# Manual coefficient update (atypical scenario)\nmodel.pp.beta0 .= new_coefficients\n\n# Clear cache to ensure consistency (internal function)\nMargins.clear_engine_cache!()\nresult2 = population_margins(model, data)","category":"section"},{"location":"caching/#Backward-Compatibility","page":"Caching System (Internal Architecture)","title":"Backward Compatibility","text":"The system provides automatic derivative support detection for legacy code:\n\n# Explicit specification (recommended)\nget_or_build_engine(PopulationUsage, HasDerivatives, model, data_nt, vars, vcov)\n\n# Automatic detection (backward compatible)\nget_or_build_engine(PopulationUsage, model, data_nt, vars, vcov)\n\nThe automatic detection examines the data types to determine whether derivative computation is required.","category":"section"},{"location":"caching/#Theoretical-Foundation","page":"Caching System (Internal Architecture)","title":"Theoretical Foundation","text":"The caching system exploits the mathematical property that marginal effects computation for a given model configuration involves deterministic transformations of the design matrix and coefficient vector. For a model with linear predictor η = Xβ, the marginal effect computation:\n\n∂E[y|X]/∂xⱼ = g'(η) × βⱼ\n\ndepends only on the model structure, coefficients, and link function. By caching the compiled representation of these transformations, the system avoids redundant symbolic and numerical preprocessing while preserving exact numerical equivalence.","category":"section"},{"location":"caching/#Future-Directions","page":"Caching System (Internal Architecture)","title":"Future Directions","text":"Several enhancements to the caching system are under consideration:\n\nLeast-recently-used eviction: Automatic cache size management through LRU policies\nThread-local storage: Safe concurrent access through thread-specific caches\nWeak references: Enable garbage collection of unused engines while preserving active ones\nPersistent caching: Serialization of compiled engines for cross-session reuse\n\nThese enhancements would extend the applicability of the caching system to more diverse computational contexts while maintaining the current guarantees of numerical correctness.","category":"section"},{"location":"caching/#Related-Documentation","page":"Caching System (Internal Architecture)","title":"Related Documentation","text":"Computational Architecture: FormulaCompiler.jl integration and evaluation strategies\nPerformance Guide: Computational complexity analysis and optimization strategies\nAPI Reference: Cache management function specifications","category":"section"},{"location":"dev/performance/#Developer-Performance-Guide","page":"Developer Performance Guide","title":"Developer Performance Guide","text":"This page summarizes best practices to ensure O(1) allocations in production paths without compromising statistical correctness. It mirrors PERFORMANCEBESTPRACTICES.md at the repo root for convenient browsing.\n\nSee also: ../../PERFORMANCEBESTPRACTICES.md (same content).","category":"section"},{"location":"dev/performance/#Core-Principles","page":"Developer Performance Guide","title":"Core Principles","text":"Preserve statistical validity (delta-method, full Σ) at all times\nAim for constant allocations w.r.t. sample size in production code\nPreallocate once; reuse buffers and result tables\nAvoid dynamic growth in hot paths","category":"section"},{"location":"dev/performance/#Patterns-That-Work","page":"Developer Performance Guide","title":"Patterns That Work","text":"Compile/cache FormulaCompiler artifacts outside loops\nMove hot loops into helpers that take concrete arguments (compiled, row_buf, β, link, de)\nReuse row_buf, η_buf, g_buf, gβ_accumulator; avoid temporary vectors\nPrefer scalar loops over broadcasts that allocate\nPreallocate DataFrame columns and assign by index","category":"section"},{"location":"dev/performance/#Validation","page":"Developer Performance Guide","title":"Validation","text":"Use validate_allocations.jl; rely on tags:\n[PROD]: production code paths — must be O(1)\n[TEST]: diagnostic loops — may allocate by design for isolation\n\nExpected (validated): _population_predictions, population_margins (pred/effects) show constant allocations.","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"Comprehensive technical specification for Margins.jl functions and types","category":"section"},{"location":"api/#Conceptual-Foundation","page":"API Reference","title":"Conceptual Foundation","text":"","category":"section"},{"location":"api/#Two-Function-Architecture","page":"API Reference","title":"Two-Function Architecture","text":"The package implements a systematic two-function API that operationalizes the unified analytical framework through distinct computational pathways for population-level and profile-specific marginal effects analysis.","category":"section"},{"location":"api/#Analysis-Type-Distinction","page":"API Reference","title":"Analysis Type Distinction","text":"Population Analysis: Integration over empirical covariate distributions\nProfile Analysis: Evaluation at specified covariate combinations","category":"section"},{"location":"api/#Function-Specifications","page":"API Reference","title":"Function Specifications","text":"","category":"section"},{"location":"api/#Population-Analysis","page":"API Reference","title":"Population Analysis","text":"","category":"section"},{"location":"api/#population_margins","page":"API Reference","title":"population_margins","text":"Computes population-level marginal effects or adjusted predictions through integration over the empirical distribution of observed covariates.\n\nThe function implements population-averaged inference by computing marginal quantities for each observation in the sample and subsequently averaging these quantities according to the empirical distribution. This approach yields population parameters that reflect the heterogeneity present in the data generating process while providing appropriate standard errors through delta-method computation with full covariance matrix integration.\n\nMethodological Applications: Population analysis provides unbiased estimates of population parameters suitable for policy evaluation requiring external validity to similar populations. The approach proves particularly valuable when sample heterogeneity represents important features of the underlying population, and when analytical applications affect diverse demographic or economic groups requiring representative inference.\n\nComputational Characteristics: Linear scaling with respect to sample size while maintaining minimal per-observation computational overhead through optimized implementations. Detailed performance analysis and computational complexity comparisons are provided in the Performance Guide\n\nSee also: Population Scenarios for counterfactual analysis and Weights for sampling/frequency weights.","category":"section"},{"location":"api/#Profile-Analysis","page":"API Reference","title":"Profile Analysis","text":"","category":"section"},{"location":"api/#profile_margins","page":"API Reference","title":"profile_margins","text":"Computes marginal effects or adjusted predictions evaluated at specified covariate combinations within the covariate space.\n\nThe function implements profile-specific inference through evaluation of marginal quantities at predetermined points in the covariate space, typically at sample means or theoretically motivated scenario specifications. This approach yields concrete, interpretable estimates for specific covariate combinations while maintaining appropriate uncertainty quantification through delta-method standard error computation.\n\nMethodological Applications:   Profile analysis provides representative case inference suitable for policy targeting specific demographic or economic profiles. The approach facilitates clear communication of quantitative results through concrete scenario interpretation, making it particularly valuable for stakeholder communication and policy applications requiring specific target group analysis.\n\nComputational Characteristics: Constant-time complexity independent of dataset size through optimized evaluation algorithms that avoid full dataset traversal. Comprehensive performance benchmarking and memory allocation analysis are detailed in the Performance Guide","category":"section"},{"location":"api/#Result-Type-Specifications","page":"API Reference","title":"Result Type Specifications","text":"","category":"section"},{"location":"api/#Type-Safe-Result-System-(v2.0)","page":"API Reference","title":"Type-Safe Result System (v2.0)","text":"Margins.jl implements a specialized type system that provides type safety and optimized DataFrame formatting through distinct result containers for different analysis types.","category":"section"},{"location":"api/#EffectsResult","page":"API Reference","title":"EffectsResult","text":"Structured container for marginal effects analysis (AME, MEM, MER) implementing the Tables.jl interface protocol.\n\nThe EffectsResult type encapsulates computed marginal effects along with associated statistical inference quantities including standard errors, confidence intervals, and hypothesis test statistics. The type contains variable identification fields (variables, terms) essential for effects interpretation and supports multiple DataFrame formatting options.\n\nFields:\n\nestimates::Vector{Float64} - Point estimates of marginal effects\nstandard_errors::Vector{Float64} - Delta-method standard errors\nvariables::Vector{String} - The \"x\" in dy/dx (which variable each row represents)\nterms::Vector{String} - Contrast descriptions (e.g., \"dy/dx\", \"treated - control\")\nprofile_values::Union{Nothing, NamedTuple} - Reference grid values (for profile effects MEM/MER; nothing for population effects AME)\ngroup_values::Union{Nothing, NamedTuple} - Grouping variable values (when using groups parameter; nothing otherwise)\ngradients::Matrix{Float64} - Parameter gradients (G matrix) for delta-method computation\nmetadata::Dict{Symbol, Any} - Analysis metadata (model info, options used, sample size, etc.)\n\nKey Features:\n\nMultiple DataFrame formats: :standard, :compact, :confidence, :profile, :stata\nAuto-detects appropriate format based on analysis type\nprofile_values populated only for profile_margins() (MEM/MER)\ngroup_values populated only when using groups parameter","category":"section"},{"location":"api/#PredictionsResult","page":"API Reference","title":"PredictionsResult","text":"Streamlined container for predictions analysis (AAP, APM, APR) implementing the Tables.jl interface protocol.\n\nThe PredictionsResult type focuses specifically on predicted values without variable/contrast concepts, providing a clean interface optimized for predictions analysis. The streamlined design reflects that predictions represent \"fitted values at scenarios\" rather than \"effects of variables.\"\n\nFields:\n\nestimates::Vector{Float64} - Point estimates (predicted values)\nstandard_errors::Vector{Float64} - Delta-method standard errors\nprofile_values::Union{Nothing, NamedTuple} - Reference grid values (for profile predictions APM/APR; nothing for population predictions AAP)\ngroup_values::Union{Nothing, NamedTuple} - Grouping variable values (when using groups parameter; nothing otherwise)\ngradients::Matrix{Float64} - Parameter gradients (G matrix) for delta-method computation\nmetadata::Dict{Symbol, Any} - Analysis metadata (model info, options used, sample size, etc.)\n\nKey Features:\n\nOmits variable/contrast fields (not applicable to predictions - predictions don't have \"x\" or \"dy/dx\" concepts)\nSingle optimized DataFrame format for predictions display\nClean tabular output focused on prediction values and statistics\nprofile_values populated only for profile_margins() (APM/APR)\ngroup_values populated only when using groups parameter\n\nData Integration Framework:\n\n# Type-specific result containers with Tables.jl protocol\neffects_result = population_margins(model, data; type=:effects)  # Returns EffectsResult\npredictions_result = population_margins(model, data; type=:predictions)  # Returns PredictionsResult\n\n# Accessing fields directly\neffects_result.estimates          # Vector{Float64} of marginal effects\neffects_result.standard_errors    # Vector{Float64} of standard errors\neffects_result.variables          # Vector{String} of variable names\neffects_result.profile_values     # Nothing (for population) or NamedTuple (for profile)\neffects_result.group_values       # Nothing (no groups) or NamedTuple (with groups)\neffects_result.metadata           # Dict{Symbol, Any} with analysis info\n\n# Profile margins have profile_values populated\nprofile_result = profile_margins(model, data, means_grid(data); type=:effects)\nprofile_result.profile_values     # NamedTuple(x1=[...], x2=[...], ...)\n\n# Grouped analysis has group_values populated\ngrouped_result = population_margins(model, data; type=:effects, groups=:region)\ngrouped_result.group_values       # NamedTuple(region=[\"North\", \"South\", ...])\n\n# Type-specific DataFrame conversion\neffects_df = DataFrame(effects_result)  # Includes variable/contrast columns\npredictions_df = DataFrame(predictions_result)  # Streamlined predictions format\n\n# Multiple format options for effects\nDataFrame(effects_result; format=:compact)  # Minimal columns\nDataFrame(effects_result; format=:stata)    # Stata-style column names\n\n# Compatible with all Tables.jl-compliant output formats\nCSV.write(\"effects.csv\", effects_result)\nCSV.write(\"predictions.csv\", predictions_result)","category":"section"},{"location":"api/#Second-Differences-(Interaction-Effects)","page":"API Reference","title":"Second Differences (Interaction Effects)","text":"Margins.jl provides comprehensive support for computing second differences—interaction effects on the predicted outcome scale. Second differences quantify how marginal effects vary across levels of a moderating variable, addressing the fundamental question: \"Does the effect of X depend on Z?\"","category":"section"},{"location":"api/#Quick-Start","page":"API Reference","title":"Quick Start","text":"# Step 1: Compute AMEs across modifier levels\names = population_margins(model, data;\n                         scenarios=(treated=[0, 1],),\n                         type=:effects)\n\n# Step 2: Calculate second differences\nsd = second_differences(ames, :age, :treated, vcov(model))\nDataFrame(sd)","category":"section"},{"location":"api/#Available-Functions","page":"API Reference","title":"Available Functions","text":"Discrete Contrast Approach (Population-based):\n\nsecond_differences(): Unified interface (recommended) - handles binary, categorical, and continuous moderators\nsecond_difference(): Binary moderators only (backward compatibility)\nsecond_differences_pairwise(): All pairwise modifier comparisons\nsecond_differences_all_contrasts(): All focal contrasts × all modifier pairs\n\nLocal Derivative Approach (Profile-based):\n\nsecond_differences_at(): Compute ∂AME/∂modifier at specific evaluation points via finite differences\n\nFor comprehensive coverage including methodological foundation, usage patterns, and interpretation guidance, see Second Differences.","category":"section"},{"location":"api/#Extended-Analytical-Capabilities","page":"API Reference","title":"Extended Analytical Capabilities","text":"","category":"section"},{"location":"api/#Categorical-Mixture-Specifications","page":"API Reference","title":"Categorical Mixture Specifications","text":"The package implements sophisticated categorical mixture functionality to enable realistic policy scenario analysis through fractional category specifications. The CategoricalMixture type facilitates the specification of probability-weighted categorical distributions that reflect realistic population compositions rather than arbitrary baseline categories.\n\nPolicy Counterfactual Analysis:\n\n# Current population educational composition (predictions at a mixture)\nbaseline_grid = DataFrame(education=[mix(\"HS\" => 0.4, \"College\" => 0.4, \"Graduate\" => 0.2)])\nbaseline = profile_margins(model, data, baseline_grid; type=:predictions)\n\n# Policy counterfactual: educational attainment improvement (new mixture)\nintervention_grid = DataFrame(education=[mix(\"HS\" => 0.2, \"College\" => 0.5, \"Graduate\" => 0.3)])\nintervention = profile_margins(model, data, intervention_grid; type=:predictions)","category":"section"},{"location":"api/#Parameter-Reference","page":"API Reference","title":"Parameter Reference","text":"","category":"section"},{"location":"api/#Common-Parameters","page":"API Reference","title":"Common Parameters","text":"Quick Start Examples:\n\ntype=:effects → \"How much does the outcome change?\" (most common)  \ntype=:predictions → \"What outcome value should I expect?\"\nmeasure=:elasticity → \"What's the percentage effect?\" (useful for proportional changes)\nbackend=:ad → Default backend (highest accuracy, zero allocation)\nbackend=:fd → Alternative backend (zero allocation, numerical approximation)\n\nAll main functions support these core parameters:","category":"section"},{"location":"api/#Analysis-Type-(type)","page":"API Reference","title":"Analysis Type (type)","text":":effects - Marginal effects (derivatives for continuous, contrasts for categorical)\n:predictions - Adjusted predictions (fitted values)","category":"section"},{"location":"api/#Variable-Selection-(vars)","page":"API Reference","title":"Variable Selection (vars)","text":"nothing - Auto-detect continuous variables (default for effects)\n:all_continuous - Explicit selection of all continuous variables\n:variable_name - Single variable\n[:var1, :var2] - Multiple specific variables","category":"section"},{"location":"api/#Target-Scale-(scale)","page":"API Reference","title":"Target Scale (scale)","text":":response - Response scale (default, applies inverse link function)\n:link - Linear predictor scale (link scale)","category":"section"},{"location":"api/#Computational-Backend-(backend)","page":"API Reference","title":"Computational Backend (backend)","text":":ad - Automatic differentiation (default; higher accuracy, zero allocation after warmup)\n:fd - Finite differences (zero allocation, production-ready)","category":"section"},{"location":"api/#Effect-Measures-(measure)","page":"API Reference","title":"Effect Measures (measure)","text":":effect - Standard marginal effects (default)\n:elasticity - Elasticities (% change in Y per % change in X)\n:semielasticity_dyex - Semielasticity d(y)/d(ln x) (change in Y per % change in X)\n:semielasticity_eydx - Semielasticity d(ln y)/dx (% change in Y per unit change in X)","category":"section"},{"location":"api/#Profile-Specific-Parameters","page":"API Reference","title":"Profile-Specific Parameters","text":"","category":"section"},{"location":"api/#Reference-Grid-(positional-argument)","page":"API Reference","title":"Reference Grid (positional argument)","text":"Profile margins take a reference grid as the third positional argument. Use the built-in grid builders or pass a DataFrame directly:\n\n# At sample means (most common)\nprofile_margins(model, data, means_grid(data))\n\n# Cartesian product: 6 scenarios (3×2)\nprofile_margins(model, data, cartesian_grid(x=[0,1,2], group=[\"A\",\"B\"]))\n\n# Balanced grid (sample frequencies for categoricals, means for continuous)\nprofile_margins(model, data, balanced_grid(data))\n\n# DataFrame grid (full control)\ngrid = DataFrame(x=[0,1,2], group=[\"A\",\"A\",\"B\"])\nprofile_margins(model, data, grid)\n\nSee Reference Grids for full documentation of grid builders.","category":"section"},{"location":"api/#Population-Specific-Parameters","page":"API Reference","title":"Population-Specific Parameters","text":"","category":"section"},{"location":"api/#Grouping-(groups)","page":"API Reference","title":"Grouping (groups)","text":"Symbol - Single grouping variable\nVector{Symbol} - Multiple grouping variables  \nNamedTuple - Advanced grouping with value specifications\n\nExamples:\n\n# By single categorical variable\npopulation_margins(model, data; groups=:region)\n\n# Multiple grouping\npopulation_margins(model, data; groups=[:region, :year])\n\n# Advanced grouping (unified syntax)\npopulation_margins(model, data; groups=(:income, [20000, 50000, 80000]))","category":"section"},{"location":"api/#Counterfactual-Analysis-(scenarios)","page":"API Reference","title":"Counterfactual Analysis (scenarios)","text":"# Effects when treatment is set to 1 vs 0 for entire population\npopulation_margins(model, data; scenarios=(treatment=[0, 1],), type=:effects)","category":"section"},{"location":"api/#Usage-Patterns","page":"API Reference","title":"Usage Patterns","text":"","category":"section"},{"location":"api/#Basic-Workflow","page":"API Reference","title":"Basic Workflow","text":"# 1. Fit model\nmodel = lm(@formula(y ~ x1 + x2 + group), data)\n\n# 2. Population analysis (most common starting point)\name = population_margins(model, data; type=:effects)\naap = population_margins(model, data; type=:predictions)\n\n# 3. Profile analysis for specific scenarios\nmem = profile_margins(model, data, means_grid(data); type=:effects)\nscenario_results = profile_margins(model, data, cartesian_grid(x1=[0,1,2]); type=:effects)\n\n# 4. Convert to DataFrame for analysis\nDataFrame(ame)","category":"section"},{"location":"api/#Performance-Optimization","page":"API Reference","title":"Performance Optimization","text":"# Maximum performance configuration\nfast_result = population_margins(model, data; backend=:fd, scale=:link)\n\n# Profile analysis is O(1) - efficient regardless of data size\ngrid = cartesian_grid(x1=[0,1,2])\nprofile_result = profile_margins(model, huge_data, grid; type=:effects)  # ~300μs regardless of data size","category":"section"},{"location":"api/#Advanced-Analysis-Patterns","page":"API Reference","title":"Advanced Analysis Patterns","text":"# Elasticity analysis across scenarios (profile)\nscenarios = cartesian_grid(x1=[0, 1, 2])\nelasticities = profile_margins(model, data, scenarios; \n    measure=:elasticity, vars=[:x2])\n\n# Robust standard errors (with CovarianceMatrices.jl)\nusing CovarianceMatrices\nrobust_effects = population_margins(model, data; vcov=HC1(), type=:effects)\n\n# Complex categorical scenarios via reference grid\npolicy_grid = DataFrame(\n    treatment=[mix(0 => 0.3, 1 => 0.7)],           # 70% treatment rate\n    education=[mix(\"HS\" => 0.3, \"College\" => 0.7)] # Education composition\n)\npolicy_scenario = profile_margins(model, data, policy_grid; type=:predictions)","category":"section"},{"location":"api/#Error-Handling","page":"API Reference","title":"Error Handling","text":"","category":"section"},{"location":"api/#Common-Error-Patterns","page":"API Reference","title":"Common Error Patterns","text":"","category":"section"},{"location":"api/#Variable-Specification-Errors","page":"API Reference","title":"Variable Specification Errors","text":"# Error: Variable not found\npopulation_margins(model, data; vars=[:nonexistent_var])\n# → Clear error message with available variables\n\n# Error: Wrong variable type for effects  \npopulation_margins(model, data; vars=[:categorical_var], type=:effects)\n# → Suggests using categorical contrasts or predictions","category":"section"},{"location":"api/#Profile-Specification-Errors","page":"API Reference","title":"Profile Specification Errors","text":"# Error: Invalid reference grid argument (must be DataFrame or a grid builder output)\nprofile_margins(model, data, \"invalid\")\n# → Clear guidance on valid reference grid specifications\n\n# Error: Reference grid missing model variables\nincomplete_grid = DataFrame(x1=[0,1])  # Missing x2, group from model\nprofile_margins(model, data, incomplete_grid)\n# → Error with list of missing variables","category":"section"},{"location":"api/#Statistical-Validity-Errors","page":"API Reference","title":"Statistical Validity Errors","text":"# Error: Insufficient data for robust estimation\ntiny_data = data[1:5, :]\npopulation_margins(model, tiny_data)\n# → Warning about statistical reliability with small samples","category":"section"},{"location":"api/#Error-Recovery-Patterns","page":"API Reference","title":"Error Recovery Patterns","text":"# Input validation\nfunction validated_margins(model, data; vars=nothing, kwargs...)\n    # Validate variable existence\n    if vars !== nothing\n        data_vars = names(data)\n        missing_vars = setdiff(vars, Symbol.(data_vars))\n        if !isempty(missing_vars)\n            throw(ArgumentError(\"Variables not found in data: $missing_vars\"))\n        end\n    end\n    \n    return population_margins(model, data; vars=vars, kwargs...)\nend","category":"section"},{"location":"api/#Integration-Examples","page":"API Reference","title":"Integration Examples","text":"","category":"section"},{"location":"api/#With-GLM.jl-Ecosystem","page":"API Reference","title":"With GLM.jl Ecosystem","text":"using GLM, CategoricalArrays\n\n# Logistic regression\nmodel = glm(@formula(outcome ~ x1 + x2 + group), data, Binomial(), LogitLink())\n\n# Effects on probability scale\nprob_effects = population_margins(model, data; scale=:response, type=:effects)\n\n# Effects on log-odds scale  \nlogodds_effects = population_margins(model, data; scale=:link, type=:effects)","category":"section"},{"location":"api/#With-CovarianceMatrices.jl","page":"API Reference","title":"With CovarianceMatrices.jl","text":"using CovarianceMatrices\n\n# Apply different estimators via vcov parameter\name_hc1 = population_margins(model, data; vcov=HC1())\name_hc3 = population_margins(model, data; vcov=HC3())\name_clustered = population_margins(model, data; vcov=Clustered(:cluster_var))\name_hac = population_margins(model, data; vcov=HAC(Bartlett()))","category":"section"},{"location":"api/#With-DataFrames-Ecosystem","page":"API Reference","title":"With DataFrames Ecosystem","text":"using DataFrames, CSV, Chain\n\n# Complete analysis pipeline\nresults_df = @chain begin\n    population_margins(model, data; type=:effects)\n    DataFrame(_)\n    select(_, :term, :estimate, :se, :p_value)\n    filter(row -> row.p_value < 0.05, _)  # Significant effects only\nend\n\n# Export results\nCSV.write(\"significant_effects.csv\", results_df)\n\n\n\nThis API reference provides complete documentation for all Margins.jl functionality. For conceptual background on the 2×2 framework, see Mathematical Foundation. For performance optimization guidance, see Performance Guide. For advanced features including elasticities and robust inference, see Advanced Features.","category":"section"},{"location":"computational_architecture/#Computational-Architecture","page":"Computational Architecture","title":"Computational Architecture","text":"The foundational computational engine powering Margins.jl","category":"section"},{"location":"computational_architecture/#FormulaCompiler.jl:-The-Foundation","page":"Computational Architecture","title":"FormulaCompiler.jl: The Foundation","text":"Margins.jl is built on FormulaCompiler.jl, a high-performance formula evaluation and differentiation engine specifically designed for econometric analysis. This architectural foundation helps explain how Margins.jl achieves both statistical correctness and exceptional performance.","category":"section"},{"location":"computational_architecture/#Why-FormulaCompiler.jl-Matters","page":"Computational Architecture","title":"Why FormulaCompiler.jl Matters","text":"FormulaCompiler.jl provides the zero-allocation computational core that enables Margins.jl to:\n\nProcess large econometric datasets efficiently: O(1) profile margins regardless of dataset size\nMaintain statistical rigor: Exact derivatives for delta-method standard errors  \nSupport complex formulas: Nested functions like log(1 + income) with proper differentiation\nHandle mixed data types: Automatic Float64 conversion for derivatives without runtime cost\nEnsure numerical stability: Machine-precision accuracy for gradient computations\n\nWithout FormulaCompiler.jl, marginal effects computation would require either:\n\nSlow symbolic differentiation (intractable for large datasets)\nUnreliable numerical approximations (compromising statistical validity)\nMassive memory allocations (preventing large-scale analysis)","category":"section"},{"location":"computational_architecture/#The-Compilation-Strategy","page":"Computational Architecture","title":"The Compilation Strategy","text":"FormulaCompiler.jl transforms regression formulas into highly optimized computational kernels:","category":"section"},{"location":"computational_architecture/#Formula-Compilation","page":"Computational Architecture","title":"Formula Compilation","text":"# From StatsModels.jl formula...\n@formula(y ~ log(income) + age + education)\n\n# ...to zero-allocation evaluator\ncompiled = FormulaCompiler.compile_formula(model, data)\n# Single compilation, reused across all margin computations","category":"section"},{"location":"computational_architecture/#Derivative-System","page":"Computational Architecture","title":"Derivative System","text":"# Automatic derivative evaluators for marginal effects\nde = FormulaCompiler.build_derivative_evaluator(compiled, data; vars=[:income, :age])\n# Zero allocation per derivative computation","category":"section"},{"location":"computational_architecture/#Type-Safe-Overrides","page":"Computational Architecture","title":"Type-Safe Overrides","text":"# Efficient scenario analysis with fractional specifications\noverrides = Dict(:income => 50000, :treatment => 0.5)  # 50% treatment probability\nresult = FormulaCompiler.evaluate_scenario(compiled, overrides)\n# Supports categorical mixtures and continuous overrides seamlessly","category":"section"},{"location":"computational_architecture/#Computational-Kernels","page":"Computational Architecture","title":"Computational Kernels","text":"","category":"section"},{"location":"computational_architecture/#Zero-Allocation-Formula-Evaluation","page":"Computational Architecture","title":"Zero-Allocation Formula Evaluation","text":"The core of all marginal effects computation is formula evaluation. FormulaCompiler.jl achieves ~7 nanoseconds per evaluation with zero allocations:\n\n# Population margins: evaluate formula n times (once per observation)\nfor i in 1:n_observations\n    η[i] = compiled_evaluator(data_row[i])  # 7ns, 0 bytes\nend\n\n# Profile margins: evaluate formula k times (once per scenario)  \nfor j in 1:n_scenarios\n    η[j] = compiled_evaluator(scenario[j])   # 7ns, 0 bytes, independent of n_observations\nend\n\nKey insight: Profile margins achieve O(1) scaling because they evaluate formulas only at specified scenarios, not across the entire dataset.","category":"section"},{"location":"computational_architecture/#Derivative-Computation","page":"Computational Architecture","title":"Derivative Computation","text":"Marginal effects require computing ∂η/∂x for each variable. FormulaCompiler.jl provides two backends:","category":"section"},{"location":"computational_architecture/#Automatic-Differentiation-(:ad)-**RECOMMENDED**","page":"Computational Architecture","title":"Automatic Differentiation (:ad) - RECOMMENDED","text":"Accuracy: Machine precision (exact derivatives)\nAllocation: Zero bytes after warmup\nDomain safety: Handles log(), sqrt(), 1/x functions correctly\nUse case: Default choice for reliability and accuracy","category":"section"},{"location":"computational_architecture/#Finite-Differences-(:fd)","page":"Computational Architecture","title":"Finite Differences (:fd)","text":"Accuracy: Numerical approximation (typically sufficient)\nAllocation: Zero bytes in all cases\nPerformance: Slightly faster for simple formulas\nUse case: Production optimization when domain is well-behaved","category":"section"},{"location":"computational_architecture/#Buffer-Management-System","page":"Computational Architecture","title":"Buffer Management System","text":"Margins.jl pre-allocates computational buffers to achieve zero-allocation performance:\n\n# Pre-allocated in MarginsEngine struct\nstruct MarginsEngine\n    η_buf::Vector{Float64}          # Linear predictor evaluations\n    g_buf::Vector{Float64}          # Gradient computations  \n    gβ_accumulator::Vector{Float64} # Parameter gradient accumulation\n    # ... other buffers\nend\n\nThese buffers are reused across all computations, eliminating runtime allocations while maintaining thread safety.","category":"section"},{"location":"computational_architecture/#Data-Type-Architecture","page":"Computational Architecture","title":"Data Type Architecture","text":"","category":"section"},{"location":"computational_architecture/#Mixed-Type-Handling","page":"Computational Architecture","title":"Mixed Type Handling","text":"FormulaCompiler.jl automatically handles the mixed data types common in econometric analysis:","category":"section"},{"location":"computational_architecture/#Integer-Variables","page":"Computational Architecture","title":"Integer Variables","text":"Runtime behavior: Automatic Float64 conversion during derivative computation\nPerformance impact: Zero (conversion happens during compilation, not evaluation)\nMathematical correctness: Preserves exact derivative computation\nExample: age::Int64 treated as continuous for marginal effects","category":"section"},{"location":"computational_architecture/#Categorical-Variables","page":"Computational Architecture","title":"Categorical Variables","text":"Bool variables: Treated as categorical with fractional override support\nCategoricalArray: Supports both baseline and pairwise contrasts\nFrequency weighting: Unspecified categoricals use sample composition\nExample: treatment::Bool supports 0.7 for 70% treatment probability","category":"section"},{"location":"computational_architecture/#Continuous-Variables","page":"Computational Architecture","title":"Continuous Variables","text":"Float64: Native support with full arithmetic operations\nComplex expressions: log(1 + income), sqrt(age) handled correctly\nChain rule: Automatic differentiation through nested functions","category":"section"},{"location":"computational_architecture/#Type-Safe-Scenario-System","page":"Computational Architecture","title":"Type-Safe Scenario System","text":"FormulaCompiler.jl enables sophisticated scenario analysis while maintaining type safety:\n\n# Representative scenarios with mixed types\nscenarios = (\n    :income => [30000, 50000, 80000],        # Continuous override\n    :education => [\"High School\", \"College\"], # Categorical override  \n    :treatment => [0.2, 0.8]                 # Fractional Bool override\n)\n\n# Automatic Cartesian product: 3×2×2 = 12 scenarios\n# Each scenario maintains type consistency and statistical validity","category":"section"},{"location":"computational_architecture/#Statistical-Computation-Architecture","page":"Computational Architecture","title":"Statistical Computation Architecture","text":"","category":"section"},{"location":"computational_architecture/#Delta-Method-Standard-Errors","page":"Computational Architecture","title":"Delta-Method Standard Errors","text":"The statistical rigor of Margins.jl depends on proper delta-method computation:\n\n# Delta-method formula: Var(g(β)) = g'(β) Σ g'(β)ᵀ\n# Where g'(β) = ∂(marginal_effect)/∂β and Σ = vcov(model)\n\n# FormulaCompiler.jl computes g'(β) with zero allocation:\ngradient = FormulaCompiler.compute_parameter_gradient(compiled, β, data_point)\nvariance = gradient' * vcov_matrix * gradient\nstandard_error = sqrt(variance)\n\nCritical: This computation requires exact derivatives to ensure statistical validity. Approximate gradients would compromise the mathematical foundation of inference.","category":"section"},{"location":"computational_architecture/#Covariance-Matrix-Integration","page":"Computational Architecture","title":"Covariance Matrix Integration","text":"FormulaCompiler.jl integrates seamlessly with Julia's covariance matrix ecosystem:\n\nGLM.jl: Uses vcov(model) automatically\nCovarianceMatrices.jl: Supports robust/clustered standard errors\nMixedModels.jl: Compatible with mixed model covariance structures\nCustom matrices: Accepts user-provided covariance matrices","category":"section"},{"location":"computational_architecture/#Performance-Implications-of-Architecture","page":"Computational Architecture","title":"Performance Implications of Architecture","text":"","category":"section"},{"location":"computational_architecture/#Why-Profile-Margins-Are-O(1)","page":"Computational Architecture","title":"Why Profile Margins Are O(1)","text":"# Profile margins evaluate k scenarios (typically 1-50)\nn_scenarios = length(expand_scenarios(at_specification))\ncomputational_cost = n_scenarios * 7ns  # Independent of dataset size\n\n# Population margins evaluate n observations  \ncomputational_cost = n_observations * 7ns  # Scales with data\n\nArchitectural insight: Profile margins achieve constant-time performance because FormulaCompiler.jl decouples formula evaluation from data size.","category":"section"},{"location":"computational_architecture/#Memory-Architecture","page":"Computational Architecture","title":"Memory Architecture","text":"# Constant memory footprint regardless of dataset size:\nmemory_usage = sizeof(η_buf) + sizeof(g_buf) + sizeof(gβ_accumulator) + compilation_cache\n# Total: ~few KB, independent of whether you have 1k or 1M observations","category":"section"},{"location":"computational_architecture/#Compilation-Caching","page":"Computational Architecture","title":"Compilation Caching","text":"FormulaCompiler.jl automatically caches compiled evaluators:\n\n# First call: compilation cost\nresult1 = population_margins(model, data)  # ~milliseconds (compile + compute)\n\n# Subsequent calls: pure computation  \nresult2 = profile_margins(model, data, means_grid(data))  # ~microseconds (reuse compilation)","category":"section"},{"location":"computational_architecture/#Integration-with-JuliaStats-Ecosystem","page":"Computational Architecture","title":"Integration with JuliaStats Ecosystem","text":"","category":"section"},{"location":"computational_architecture/#StatsModels.jl-Integration","page":"Computational Architecture","title":"StatsModels.jl Integration","text":"FormulaCompiler.jl directly processes StatsModels.jl formulas:\n\n# From StatsModels formula specification...\nformula = @formula(log_wage ~ education + experience + education&experience)\n\n# ...to compiled computational kernel with proper derivatives\ncompiled = FormulaCompiler.compile_formula(formula, model, data)\n# Handles interaction terms, transformations, and categorical expansions","category":"section"},{"location":"computational_architecture/#GLM.jl-Integration","page":"Computational Architecture","title":"GLM.jl Integration","text":"Link functions are handled transparently:\n\n# For GLMs, chain rule automatically applied:\n# ∂μ/∂x = (∂μ/∂η) × (∂η/∂x)\n# Where μ = linkinv(η) and ∂μ/∂η computed by FormulaCompiler.jl\n\n# Both link scale (:link) and response scale (:response) supported\nmargin_link = compute_margin(compiled, :link)      # Direct derivative\nmargin_response = compute_margin(compiled, :response)  # Chain rule applied","category":"section"},{"location":"computational_architecture/#MixedModels.jl-Integration","page":"Computational Architecture","title":"MixedModels.jl Integration","text":"Mixed models require special covariance matrix handling:\n\n# FormulaCompiler.jl extracts fixed effects for differentiation:\nβ_fixed = fixef(mixed_model)\nV_fixed = vcov(mixed_model)  # Fixed effects covariance only\n\n# Marginal effects computed relative to fixed effects:\n# Random effects treated as integrated out (conditional on data)","category":"section"},{"location":"computational_architecture/#Extensibility-Architecture","page":"Computational Architecture","title":"Extensibility Architecture","text":"","category":"section"},{"location":"computational_architecture/#Custom-Function-Support","page":"Computational Architecture","title":"Custom Function Support","text":"FormulaCompiler.jl supports user-defined functions with automatic differentiation:\n\n# Custom transformations with exact derivatives\nmy_transform(x) = log(1 + exp(x))  # Softplus function\n\n# Automatic differentiation handles custom functions:\n@formula(y ~ my_transform(income) + age)  # Works seamlessly","category":"section"},{"location":"computational_architecture/#Backend-Extensibility","page":"Computational Architecture","title":"Backend Extensibility","text":"The architecture supports additional computational backends:\n\n# Current backends\npopulation_margins(model, data; backend=:ad)  # Automatic differentiation\npopulation_margins(model, data; backend=:fd)  # Finite differences\n\n# Future extensibility:  \n# population_margins(model, data; backend=:symbolic)  # Symbolic differentiation\n# population_margins(model, data; backend=:gpu)      # GPU acceleration","category":"section"},{"location":"computational_architecture/#Architectural-Principles","page":"Computational Architecture","title":"Architectural Principles","text":"","category":"section"},{"location":"computational_architecture/#1.-Separation-of-Concerns","page":"Computational Architecture","title":"1. Separation of Concerns","text":"FormulaCompiler.jl: Low-level computational primitives\nMargins.jl: High-level statistical interface and methodology\nResult: Clean abstraction boundaries and maintainable code","category":"section"},{"location":"computational_architecture/#2.-Performance-Without-Compromise","page":"Computational Architecture","title":"2. Performance Without Compromise","text":"Statistical integrity: Performance optimizations maintain statistical validity\nExact computation: Delta-method standard errors use exact derivatives\nMemory efficiency: Zero-allocation core with pre-allocated buffers","category":"section"},{"location":"computational_architecture/#3.-Type-Safety-and-Correctness","page":"Computational Architecture","title":"3. Type Safety and Correctness","text":"Compile-time checks: Type errors caught during formula compilation\nRuntime safety: Automatic type conversions preserve mathematical properties  \nStatistical validity: Architecture enforces proper delta-method computation","category":"section"},{"location":"computational_architecture/#4.-JuliaStats-Ecosystem-Compatibility","page":"Computational Architecture","title":"4. JuliaStats Ecosystem Compatibility","text":"Protocol adherence: Follows established conventions (vcov, predict, etc.)\nSeamless integration: Works with existing model types and data formats\nFuture compatibility: Architecture supports ecosystem evolution\n\n\n\nThis computational architecture enables Margins.jl to deliver both statistical rigor and exceptional performance for econometric analysis. For performance-specific guidance, see Performance Guide. For the mathematical foundation, see Mathematical Foundation.","category":"section"},{"location":"grouping/#Population-Grouping-Framework","page":"Population Grouping","title":"Population Grouping Framework","text":"Comprehensive hierarchical analysis for stratified marginal effects","category":"section"},{"location":"grouping/#Conceptual-Foundation","page":"Population Grouping","title":"Conceptual Foundation","text":"Margins.jl implements a population-based grouping framework that computes average marginal effects (AME) and average adjusted predictions (AAP) within stratified subgroups of the observed data.","category":"section"},{"location":"grouping/#Core-Design-Principles","page":"Population Grouping","title":"Core Design Principles","text":"","category":"section"},{"location":"grouping/#Population-Based-Analysis","page":"Population Grouping","title":"Population-Based Analysis","text":"All operations maintain population averaging semantics - computing effects by averaging across actual or modified populations, not evaluating at synthetic representative points.","category":"section"},{"location":"grouping/#Orthogonal-Parameters","page":"Population Grouping","title":"Orthogonal Parameters","text":"Three independent dimensions combine multiplicatively:\n\nvars: Which variables to compute marginal effects for\ngroups: How to stratify the analysis (data structure) \nscenarios: What counterfactual scenarios to consider (data modification)","category":"section"},{"location":"grouping/#Single-Fundamental-Operation","page":"Population Grouping","title":"Single Fundamental Operation","text":"All grouping reduces to: stratify data into subgroups, compute population margins within each subgroup.","category":"section"},{"location":"grouping/#Basic-Grouping-Patterns","page":"Population Grouping","title":"Basic Grouping Patterns","text":"","category":"section"},{"location":"grouping/#Simple-Categorical-Grouping","page":"Population Grouping","title":"Simple Categorical Grouping","text":"Compute effects separately within each category of a grouping variable:\n\nusing Margins, DataFrames, GLM\n\n# Effects by education level\neducation_effects = population_margins(model, data; \n                                     type=:effects, \n                                     groups=:education)\n\n# Results: separate effects for each education category\nDataFrame(education_effects)","category":"section"},{"location":"grouping/#Cross-Tabulated-Grouping","page":"Population Grouping","title":"Cross-Tabulated Grouping","text":"Analyze effects across combinations of multiple categorical variables:\n\n# Effects by education × gender combinations\ndemographic_effects = population_margins(model, data;\n                                        type=:effects,\n                                        groups=[:education, :gender])\n\n# Results: effects for (HS,Male), (HS,Female), (College,Male), (College,Female), etc.","category":"section"},{"location":"grouping/#Advanced-Hierarchical-Grouping","page":"Population Grouping","title":"Advanced Hierarchical Grouping","text":"","category":"section"},{"location":"grouping/#Nested-Grouping-with-Operator","page":"Population Grouping","title":"Nested Grouping with => Operator","text":"The => operator creates hierarchical nesting where the right side is computed within each level of the left side:\n\n# Region first, then education within each region\nnested_effects = population_margins(model, data;\n                                  type=:effects,\n                                  groups=:region => :education)\n\n# Results: (North,HS), (North,College), (South,HS), (South,College)","category":"section"},{"location":"grouping/#Deep-Hierarchical-Nesting","page":"Population Grouping","title":"Deep Hierarchical Nesting","text":"Multiple levels of nesting support complex organizational structures:\n\n# Three-level hierarchy: country → region → education\ndeep_hierarchy = population_margins(model, data;\n                                  type=:effects,\n                                  groups=:country => (:region => :education))\n\n# Four-level hierarchy: sector → company → department → position\norganizational = population_margins(model, data;\n                                  type=:effects, \n                                  groups=:sector => (:company => (:department => :position)))","category":"section"},{"location":"grouping/#Parallel-Grouping-Within-Hierarchy","page":"Population Grouping","title":"Parallel Grouping Within Hierarchy","text":"Complex patterns combining hierarchical and cross-tabulated structures:\n\n# Region first, then education×gender cross-tab within each region\nparallel_nested = population_margins(model, data;\n                                   type=:effects,\n                                   groups=(:region => [:education, :gender]))\n\n# Region first, then separate analyses for education levels AND income quartiles\nmixed_parallel = population_margins(model, data;\n                                  type=:effects,\n                                  groups=(:region => [:education, (:income, 4)]))","category":"section"},{"location":"grouping/#Continuous-Variable-Binning","page":"Population Grouping","title":"Continuous Variable Binning","text":"","category":"section"},{"location":"grouping/#Quantile-Based-Binning","page":"Population Grouping","title":"Quantile-Based Binning","text":"Automatic binning using quantiles with professional statistical terminology:\n\n# Quartile analysis (Q1, Q2, Q3, Q4)\nincome_quartiles = population_margins(model, data;\n                                    type=:effects,\n                                    groups=(:income, 4))\n\n# Tertile analysis (T1, T2, T3) \nscore_tertiles = population_margins(model, data;\n                                  type=:effects,\n                                  groups=(:test_score, 3))\n\n# Quintile analysis (P1, P2, P3, P4, P5)\nwealth_quintiles = population_margins(model, data;\n                                    type=:effects,\n                                    groups=(:wealth, 5))","category":"section"},{"location":"grouping/#Custom-Threshold-Binning","page":"Population Grouping","title":"Custom Threshold Binning","text":"Policy-relevant thresholds using mathematical interval notation:\n\n# Income brackets for tax policy analysis\ntax_brackets = population_margins(model, data;\n                                type=:effects,\n                                groups=(:income, [25000, 50000, 75000]))\n\n# Results: [\"< 25000\", \"[25000, 50000)\", \"[50000, 75000)\", \">= 75000\"]\n\n# Poverty line analysis\npoverty_analysis = population_margins(model, data;\n                                    type=:effects,\n                                    groups=(:income, [federal_poverty_line]))\n\n# Results: [\"< 12880\", \">= 12880\"] (using 2023 federal poverty guideline)","category":"section"},{"location":"grouping/#Mixed-Categorical-and-Continuous-Grouping","page":"Population Grouping","title":"Mixed Categorical and Continuous Grouping","text":"Combine categorical variables with binned continuous variables:\n\n# Education levels × income quartiles\neducation_income = population_margins(model, data;\n                                    type=:effects,\n                                    groups=[:education, (:income, 4)])\n\n# Results: (HS,Q1), (HS,Q2), (HS,Q3), (HS,Q4), (College,Q1), etc.\n\n# Geographic region × age quintiles × gender\ncomplex_demographics = population_margins(model, data;\n                                        type=:effects,\n                                        groups=[:region, (:age, 5), :gender])","category":"section"},{"location":"grouping/#Counterfactual-Scenario-Analysis","page":"Population Grouping","title":"Counterfactual Scenario Analysis","text":"See Population Scenarios for detailed semantics and implementation notes on scenarios in population analysis.","category":"section"},{"location":"grouping/#Policy-Scenario-Framework","page":"Population Grouping","title":"Policy Scenario Framework","text":"The scenarios parameter modifies variable values for the entire population, creating counterfactual analyses:\n\n# Binary treatment analysis\ntreatment_effects = population_margins(model, data;\n                                     type=:effects,\n                                     scenarios=(:treatment = [0, 1]))\n\n# Multi-level policy scenarios\npolicy_scenarios = population_margins(model, data;\n                                    type=:effects,\n                                    scenarios=(:policy_level = [\"none\", \"moderate\", \"aggressive\"]))","category":"section"},{"location":"grouping/#Multi-Variable-Scenarios","page":"Population Grouping","title":"Multi-Variable Scenarios","text":"Cartesian product expansion for complex policy analysis:\n\n# Treatment × policy combinations\ncomprehensive_policy = population_margins(model, data;\n                                        type=:effects,\n                                        scenarios=(:treatment = [0, 1], \n                                                      :policy = [\"current\", \"reform\"]))\n\n# Results: 4 scenarios (2×2 combinations)\n\n# Three-dimensional policy space\ncomplex_scenarios = population_margins(model, data;\n                                     type=:effects,\n                                     scenarios=(:treatment = [0, 1],\n                                                   :funding = [0.8, 1.0, 1.2],\n                                                   :regulation = [\"light\", \"standard\", \"strict\"]))\n\n# Results: 18 scenarios (2×3×3 combinations)","category":"section"},{"location":"grouping/#Combined-Groups-and-Scenarios","page":"Population Grouping","title":"Combined Groups and Scenarios","text":"","category":"section"},{"location":"grouping/#Comprehensive-Policy-Analysis","page":"Population Grouping","title":"Comprehensive Policy Analysis","text":"Groups and scenarios combine multiplicatively for complete analytical coverage:\n\n# Demographics × policy scenarios\nfull_analysis = population_margins(model, data;\n                                 type=:effects,\n                                 groups=[:education, :region],\n                                 scenarios=(:treatment = [0, 1]))\n\n# Results: Each education×region combination under both treatment scenarios","category":"section"},{"location":"grouping/#Advanced-Applications","page":"Population Grouping","title":"Advanced Applications","text":"# Healthcare policy evaluation\nhealthcare_comprehensive = population_margins(health_model, health_data;\n    type=:effects,\n    groups=(:state => (:urban_rural => [:insurance_type, (:income, 3)])),\n    scenarios=(:aca_expansion = [0, 1], :medicaid_funding = [0.8, 1.2])\n)\n\n# Results: State × Urban/Rural × (Insurance×Income-Tertiles) × ACA×Medicaid scenarios\n# Total combinations: 4 states × 2 urban/rural × 12 insurance×income × 4 policy scenarios = 384 results","category":"section"},{"location":"grouping/#Important:-Skip-Rule-for-Statistical-Validity","page":"Population Grouping","title":"Important: Skip Rule for Statistical Validity","text":"Critical Constraint: For population analysis, computing the effect of a variable while simultaneously holding it fixed (via scenarios) or using it to define subgroups (via groups) is contradictory and statistically meaningless.","category":"section"},{"location":"grouping/#The-Skip-Rule","page":"Population Grouping","title":"The Skip Rule","text":"To preserve statistical correctness and interpretability, population_margins() automatically skips variables that appear in vars if they also appear in groups or scenarios.\n\n# Example: x appears in both vars and scenarios\nresult = population_margins(model, data;\n    type=:effects,\n    vars=[:x, :z],           # Request effects for x and z\n    scenarios=(:x = [0, 1])  # But fix x at specific values\n)\n# Result: Only z effect is computed. x is skipped because it's in scenarios.\n# The package silently handles this to avoid statistical errors.","category":"section"},{"location":"grouping/#Why-This-Rule-Exists","page":"Population Grouping","title":"Why This Rule Exists","text":"Conceptual Problem:\n\nMarginal effect asks: \"What happens when x changes naturally?\"\nScenario/Group says: \"Hold x fixed at specific values\" or \"Stratify by x levels\"\nThese two concepts are mutually exclusive\n\nExamples of Invalid Requests:\n\n# INVALID: \"What's the effect of income while holding income fixed?\"\npopulation_margins(model, data;\n    vars=[:income],            # Effect of income changing\n    scenarios=(:income = [30000, 50000])  # But income is fixed\n)\n# → income is skipped from vars\n\n# INVALID: \"What's the effect of education within education groups?\"\npopulation_margins(model, data;\n    vars=[:education],         # Effect of education changing\n    groups=:education          # But stratified by education levels\n)\n# → education is skipped from vars","category":"section"},{"location":"grouping/#Practical-Alternatives","page":"Population Grouping","title":"Practical Alternatives","text":"","category":"section"},{"location":"grouping/#Alternative-1:-Profile-Analysis-(for-Stata-users)","page":"Population Grouping","title":"Alternative 1: Profile Analysis (for Stata users)","text":"If you want Stata-style dydx(x) over(x) (derivative of x at different values of x), use profile analysis:\n\n# Instead of: population_margins(model, data; vars=[:x], groups=:x)  # INVALID\n# Use profile margins:\nresult = profile_margins(model, data, cartesian_grid(x=[10, 20, 30, 40]);\n    type=:effects,\n    vars=[:x]\n)\n# Computes marginal effect of x AT each specific value of x","category":"section"},{"location":"grouping/#Alternative-2:-Effects-Within-Strata","page":"Population Grouping","title":"Alternative 2: Effects Within Strata","text":"If you want effects within strata of x, group by a different variable or compute effects of other variables:\n\n# GOOD: Effects of z within education groups\nresult = population_margins(model, data;\n    type=:effects,\n    vars=[:z],              # Effect of z (not education)\n    groups=:education       # Stratified by education\n)\n\n# GOOD: Effects within income quintiles\nresult = population_margins(model, data;\n    type=:effects,\n    vars=[:treatment],      # Effect of treatment (not income)\n    groups=(:income, 5)     # Within income quintiles\n)","category":"section"},{"location":"grouping/#Alternative-3:-Counterfactual-Predictions","page":"Population Grouping","title":"Alternative 3: Counterfactual Predictions","text":"If you want to see how outcomes change as x varies, use predictions with scenarios:\n\n# Instead of: population_margins(model, data; vars=[:x], scenarios=(:x = [...]))\n# Use predictions:\nresult = population_margins(model, data;\n    type=:predictions,         # Not effects!\n    scenarios=(:x = [10, 20, 30, 40])\n)\n# Shows predicted outcomes at each value of x","category":"section"},{"location":"grouping/#User-Notification","page":"Population Grouping","title":"User Notification","text":"Current Behavior: The skip rule operates silently - variables are removed from computation without warning.\n\nHow to Check: Compare requested vars against result:\n\nresult = population_margins(model, data;\n    vars=[:x, :z],\n    scenarios=(:x = [0, 1])\n)\ndf = DataFrame(result)\nunique(df.variable)  # Will only show \"z\" (x was skipped)","category":"section"},{"location":"grouping/#Performance","page":"Population Grouping","title":"Performance","text":"","category":"section"},{"location":"grouping/#Computational-Complexity","page":"Population Grouping","title":"Computational Complexity","text":"Population grouping maintains efficient O(n) scaling within each subgroup:\n\nusing BenchmarkTools\n\n# Simple grouping: O(n/k) per group for k groups\n@btime population_margins($model, $data; groups=:education)\n\n# Complex hierarchical grouping: O(n/k) per final subgroup\n@btime population_margins($model, $data; groups=(:region => (:education => :gender)))\n\n# With scenarios: same O(n/k) complexity repeated for each scenario\n@btime population_margins($model, $data; groups=:education, scenarios=(:treatment = [0, 1]))","category":"section"},{"location":"grouping/#Memory-Efficiency","page":"Population Grouping","title":"Memory Efficiency","text":"The grouping framework avoids data duplication through efficient indexing:\n\nSubgroup filtering: Uses DataFrame indexing, not data copying\nScenario modification: Temporary overrides without permanent data changes  \nResult aggregation: Minimal memory footprint for result compilation","category":"section"},{"location":"grouping/#Large-Dataset-Considerations","page":"Population Grouping","title":"Large Dataset Considerations","text":"# For datasets >100k observations with many groups\n# Consider selective analysis of key variables\nkey_analysis = population_margins(model, large_data;\n                                type=:effects,\n                                vars=[:primary_outcome],  # Limit variables\n                                groups=(:income, 4))      # Manageable grouping\n\n# Complex patterns still feasible for large n\ncomplex_large = population_margins(model, large_data;\n                                 type=:effects,\n                                 groups=(:region => [:education, (:income, 4)]))","category":"section"},{"location":"grouping/#Best-Practices","page":"Population Grouping","title":"Best Practices","text":"","category":"section"},{"location":"grouping/#When-to-Use-Different-Grouping-Patterns","page":"Population Grouping","title":"When to Use Different Grouping Patterns","text":"Simple Grouping (groups=:var):\n\nSingle dimension analysis\nClear categorical divisions\nStraightforward interpretation needs\n\nCross-Tabulation (groups=[:var1, :var2]):\n\nInteraction effects important\nPolicy targets multiple demographics simultaneously\nComprehensive coverage needed\n\nHierarchical Grouping (groups=:var1 => :var2):\n\nNatural organizational structure exists\nContext matters (e.g., regions have different education systems)\nNested decision-making processes\n\nContinuous Binning (groups=(:var, n)):\n\nPolicy-relevant thresholds exist\nDistribution-based analysis needed\nQuantile-based interpretation valuable","category":"section"},{"location":"grouping/#Avoiding-Common-Pitfalls","page":"Population Grouping","title":"Avoiding Common Pitfalls","text":"","category":"section"},{"location":"grouping/#Combination-Explosion","page":"Population Grouping","title":"Combination Explosion","text":"# Dangerous: could create 1000s of combinations\n# groups=[:var1, :var2, :var3, (:var4, 10), (:var5, 5)]\n\n# Better: use hierarchical structure\ngroups=:var1 => [:var2, (:var4, 4)]","category":"section"},{"location":"grouping/#Empty-Subgroups","page":"Population Grouping","title":"Empty Subgroups","text":"# The framework automatically detects and errors on empty subgroups\n# to maintain statistical validity","category":"section"},{"location":"grouping/#Skip-Rule-Reference","page":"Population Grouping","title":"Skip Rule Reference","text":"See the dedicated section \"Important: Skip Rule for Statistical Validity\" above for complete documentation on how population_margins() handles variables that appear in both vars and groups/scenarios.","category":"section"},{"location":"grouping/#Interpretation-Complexity","page":"Population Grouping","title":"Interpretation Complexity","text":"# For presentation, consider simpler patterns:\npresentation_analysis = population_margins(model, data;\n                                         groups=:education,\n                                         scenarios=(:policy = [0, 1]))\n\n# For comprehensive analysis, use full complexity:\nresearch_analysis = population_margins(model, data;\n                                     groups=(:region => [:education, (:income, 4)]),\n                                     scenarios=(:policy = [0, 1], :funding = [0.8, 1.2]))\n\n\n\nThe population grouping framework enables sophisticated econometric analysis while maintaining computational efficiency and statistical rigor. For related details on scenarios and reference grids, see Reference Grids and for performance optimization, see Performance Guide.","category":"section"},{"location":"population_scenarios/#Population-Scenarios-(Stata-at())","page":"Population Scenarios","title":"Population Scenarios (Stata at())","text":"Counterfactual analysis for population-averaged effects and predictions","category":"section"},{"location":"population_scenarios/#Overview","page":"Population Scenarios","title":"Overview","text":"Population scenarios enable \"what if\" analysis by computing marginal effects or predictions under counterfactual covariate values. This is analogous to Stata's at() option but with population averaging semantics.\n\nKey Concept: Scenarios modify variable values for the entire population while computing population-averaged quantities (AME or AAP), allowing you to answer questions like:\n\n\"What would the average treatment effect be if everyone were college-educated?\"\n\"How do predicted outcomes differ between policy scenarios?\"\n\"What's the effect of X when intervention Y is set to specific levels?\"","category":"section"},{"location":"population_scenarios/#Scope-and-Design","page":"Population Scenarios","title":"Scope and Design","text":"Supported in: population_margins() only (population-averaged analysis)\nNot supported in: profile_margins() (use reference grids like cartesian_grid() instead)\nDesign priorities: Statistical correctness (proper delta-method SEs with full covariance matrix Σ) and zero-allocation performance","category":"section"},{"location":"population_scenarios/#Conceptual-Model","page":"Population Scenarios","title":"Conceptual Model","text":"Given a fitted model and a dataset, a scenario specifies a set of variable overrides to evaluate counterfactuals. For population analysis, we:\n\nKeep the original rows and any grouping subset (if groups are used).\nEvaluate effects or predictions at the counterfactual covariates by overriding row values during evaluation (no data mutation).\nAverage over the selected rows (weighted or unweighted), compute the averaged parameter gradient, and apply the delta method with the full covariance matrix.","category":"section"},{"location":"population_scenarios/#Basic-Usage","page":"Population Scenarios","title":"Basic Usage","text":"","category":"section"},{"location":"population_scenarios/#Single-Variable-Scenarios","page":"Population Scenarios","title":"Single-Variable Scenarios","text":"Evaluate effects or predictions at different values of one variable:\n\n# Compare predictions under treatment vs control\nresult = population_margins(model, data;\n    type=:predictions,\n    scenarios=(treatment=[0, 1])\n)\n# Result: 2 rows showing AAP when treatment=0 and treatment=1\n\n# Effect of X under different policy environments\nresult = population_margins(model, data;\n    type=:effects,\n    vars=[:x],\n    scenarios=(policy=[\"baseline\", \"reform\"])\n)\n# Result: 2 rows showing AME of x under each policy scenario","category":"section"},{"location":"population_scenarios/#Multi-Variable-Scenarios-(Cartesian-Product)","page":"Population Scenarios","title":"Multi-Variable Scenarios (Cartesian Product)","text":"Multiple scenario variables create a Cartesian product of all combinations:\n\n# 2×3 = 6 scenarios\nresult = population_margins(model, data;\n    type=:predictions,\n    scenarios=(\n        treatment=[0, 1],\n        policy=[\"low\", \"medium\", \"high\"]\n    )\n)\n# Result: 6 rows for all (treatment, policy) combinations\n\n# 2×2×3 = 12 scenarios\nresult = population_margins(model, data;\n    type=:effects,\n    vars=[:income],\n    scenarios=(\n        education=[\"HS\", \"College\"],\n        region=[\"Urban\", \"Rural\"],\n        tax_rate=[0.15, 0.25, 0.35]\n    )\n)\n# Result: 12 rows showing income effect under all scenario combinations","category":"section"},{"location":"population_scenarios/#Scenarios-with-Grouping","page":"Population Scenarios","title":"Scenarios with Grouping","text":"Combine scenarios with grouping for within-group counterfactual analysis:\n\n# Effects of x within gender groups, under different policy scenarios\nresult = population_margins(model, data;\n    type=:effects,\n    vars=[:x],\n    groups=:gender,\n    scenarios=(policy=[\"none\", \"pilot\", \"full\"])\n)\n# Result: 6 rows (2 genders × 3 policies)\n\n# Complex: education×region groups × treatment scenarios\nresult = population_margins(model, data;\n    type=:predictions,\n    groups=[:education, :region],\n    scenarios=(treatment=[0, 1], dosage=[1, 2, 3])\n)\n# Result: (# education levels × # regions × 2 treatments × 3 dosages) rows","category":"section"},{"location":"population_scenarios/#Application-examples","page":"Population Scenarios","title":"Application examples","text":"","category":"section"},{"location":"population_scenarios/#Policy-Impact-Analysis","page":"Population Scenarios","title":"Policy Impact Analysis","text":"Question: How do predicted outcomes change under different policy interventions?\n\n# Healthcare: Compare predicted health outcomes under coverage scenarios\nusing GLM, DataFrames, Margins\n\nmodel = glm(@formula(health_score ~ age + income + insurance + education),\n            health_data, Normal(), IdentityLink())\n\npolicy_comparison = population_margins(model, health_data;\n    type=:predictions,\n    scenarios=(insurance=[\"none\", \"basic\", \"comprehensive\"])\n)\n\ndf = DataFrame(policy_comparison)\n# Shows average predicted health score under each insurance scenario\n\nResult Interpretation:\n\nEach row shows population-averaged predicted outcome under a specific policy\nStandard errors account for uncertainty in model parameters\nCan compare scenarios: df[df.at_insurance .== \"comprehensive\", :estimate] - df[df.at_insurance .== \"none\", :estimate]","category":"section"},{"location":"population_scenarios/#Treatment-Effect-Heterogeneity","page":"Population Scenarios","title":"Treatment Effect Heterogeneity","text":"Question: Does the treatment effect vary across subpopulations?\n\n# Education program: Effect of tutoring hours across SES groups\nmodel = lm(@formula(test_score ~ tutoring_hours + ses + prior_score), student_data)\n\nheterogeneous_effects = population_margins(model, student_data;\n    type=:effects,\n    vars=[:tutoring_hours],\n    groups=:ses,\n    scenarios=(prior_score=[40, 50, 60, 70, 80])  # Standardize baseline\n)\n\ndf = DataFrame(heterogeneous_effects)\n# Shows tutoring effect within each SES group, holding prior_score constant","category":"section"},{"location":"population_scenarios/#Intervention-Dosage-Analysis","page":"Population Scenarios","title":"Intervention Dosage Analysis","text":"Question: What's the optimal intervention level?\n\n# Medication study: Predicted outcomes at different dosages\ndosage_response = population_margins(medication_model, patient_data;\n    type=:predictions,\n    scenarios=(dosage=[0, 5, 10, 15, 20, 25])  # mg\n)\n\ndf = DataFrame(dosage_response)\n# Plot estimate vs at_dosage to visualize dose-response curve","category":"section"},{"location":"population_scenarios/#Demographic-Standardization","page":"Population Scenarios","title":"Demographic Standardization","text":"Question: What would effects be if population demographics were different?\n\n# Labor economics: Income effect standardized to college-educated population\ncollege_standardized = population_margins(wage_model, worker_data;\n    type=:effects,\n    vars=[:experience],\n    scenarios=(education=[\"College\"])  # Everyone has college degree\n)\n\n# Compare to actual population (mixed education)\nactual_population = population_margins(wage_model, worker_data;\n    type=:effects,\n    vars=[:experience]\n)","category":"section"},{"location":"population_scenarios/#Multi-Dimensional-Policy-Space","page":"Population Scenarios","title":"Multi-Dimensional Policy Space","text":"Question: How do multiple policies interact?\n\n# Tax policy: Joint effects of rates and deductions\ntax_scenarios = population_margins(income_model, taxpayer_data;\n    type=:predictions,\n    scenarios=(\n        tax_rate=[0.15, 0.22, 0.30],\n        deduction_cap=[5000, 10000, 25000],\n        credit_phase_out=[40000, 60000, 80000]\n    )\n)\n# Result: 3×3×3 = 27 scenarios showing all policy combinations","category":"section"},{"location":"population_scenarios/#Common-Patterns-and-Idioms","page":"Population Scenarios","title":"Common Patterns and Idioms","text":"","category":"section"},{"location":"population_scenarios/#Scenario-Naming-Convention","page":"Population Scenarios","title":"Scenario Naming Convention","text":"Scenarios appear in results with at_ prefix:\n\nresult = population_margins(model, data;\n    type=:predictions,\n    scenarios=(treatment=[0, 1], dosage=[10, 20])\n)\n\ndf = DataFrame(result)\nnames(df)  # Includes: :at_treatment, :at_dosage, :estimate, :se, ...","category":"section"},{"location":"population_scenarios/#Extracting-Specific-Scenarios","page":"Population Scenarios","title":"Extracting Specific Scenarios","text":"df = DataFrame(result)\n\n# Filter to treatment=1 scenarios only\ntreated = df[df.at_treatment .== 1, :]\n\n# Compare two specific scenarios\nbaseline = df[(df.at_treatment .== 0) .& (df.at_dosage .== 10), :]\nintervention = df[(df.at_treatment .== 1) .& (df.at_dosage .== 20), :]\ndifference = intervention.estimate .- baseline.estimate","category":"section"},{"location":"population_scenarios/#Scenario-Differences-(Contrasts)","page":"Population Scenarios","title":"Scenario Differences (Contrasts)","text":"To compute differences between scenarios, use predictions:\n\n# Wrong: Don't try to compute \"effect of treatment\" using effects + scenarios\n# population_margins(model, data; type=:effects, vars=[:treatment], scenarios=(...))\n# → treatment will be skipped (see skip rule in grouping.md)\n\n# Right: Use predictions with treatment scenarios\npredictions = population_margins(model, data;\n    type=:predictions,\n    scenarios=(treatment=[0, 1])\n)\n\ndf = DataFrame(predictions)\nate = df[df.at_treatment .== 1, :estimate][1] - df[df.at_treatment .== 0, :estimate][1]\n# Average Treatment Effect (ATE) = difference in predicted outcomes","category":"section"},{"location":"population_scenarios/#Combining-with-Weights","page":"Population Scenarios","title":"Combining with Weights","text":"Scenarios respect sampling weights:\n\n# Weighted scenarios (e.g., survey data)\nresult = population_margins(model, survey_data;\n    type=:predictions,\n    scenarios=(income=[30000, 50000, 70000]),\n    weights=:survey_weight\n)\n# Predictions are population-weighted averages","category":"section"},{"location":"population_scenarios/#Architecture-Overview","page":"Population Scenarios","title":"Architecture Overview","text":"Scenario handling is built around FormulaCompiler’s DataScenario system:\n\nDataScenario: a lightweight structure that maps variable overrides (e.g., :z => 0.5) and supplies them to the compiled evaluator per-row.\nCore evaluation calls (internal):\n_predict_with_scenario(compiled, scenario, row, scale, β, link, row_buf)\n_gradient_with_scenario!(out, compiled, scenario, row, scale, β, link, row_buf)\n\nKey properties:\n\nZero per-row allocations (reuse pre-allocated row and gradient buffers).\nO(1) memory per context (reuse scenarios; continuous FD constructs only minimal override sets).\nNo mutation of the data; categorical types remain safe (no re-pooling required).","category":"section"},{"location":"population_scenarios/#Computation-Details","page":"Population Scenarios","title":"Computation Details","text":"","category":"section"},{"location":"population_scenarios/#Continuous-Effects-under-Scenarios","page":"Population Scenarios","title":"Continuous Effects under Scenarios","text":"For each row i in the context and variable x, FD constructs centered differences around x_i while merging user overrides for other variables.\nAverage per-row effects and per-row gradients across the context (weighted or unweighted) and apply the delta method with the averaged gradient: se = sqrt(ḡ' Σ ḡ).","category":"section"},{"location":"population_scenarios/#Categorical/Boolean-Effects-under-Scenarios","page":"Population Scenarios","title":"Categorical/Boolean Effects under Scenarios","text":"Build contrasts (baseline or pairwise), merge overrides for non-effect variables into each level scenario, compute per-row differences and gradients, then average as above and apply the delta method.","category":"section"},{"location":"population_scenarios/#Predictions-under-Scenarios","page":"Population Scenarios","title":"Predictions under Scenarios","text":"Construct a single DataScenario per context, evaluate predictions and gradients per-row, average (weighted/unweighted), and apply the delta method with the averaged gradient.","category":"section"},{"location":"population_scenarios/#Grouping-(groups-...)","page":"Population Scenarios","title":"Grouping (groups = ...)","text":"Group subsets are determined first (categorical crosses, quantile bins, thresholds).\nScenario evaluation occurs within each subset using the same overrides.\nLarge combination protection prevents explosion; invalid combinations error (error-first policy).","category":"section"},{"location":"population_scenarios/#Weights","page":"Population Scenarios","title":"Weights","text":"Weighted contexts use proper normalization by total weight: Σw is used for both effects and averaged gradients.\nSampling and frequency weights are supported; weights can be provided as a column Symbol or a vector.","category":"section"},{"location":"population_scenarios/#Column-Naming-and-Ordering","page":"Population Scenarios","title":"Column Naming and Ordering","text":"Group variables appear unprefixed (e.g., education).\nScenario variables appear with at_ prefix (e.g., at_x).\nColumn order: context columns first (groups, then scenarios), then statistical columns.","category":"section"},{"location":"population_scenarios/#Programmatic-Identification","page":"Population Scenarios","title":"Programmatic Identification","text":"groups, scenarios = Margins.context_columns(result)\n# groups == [:education, ...], scenarios == [:x, :policy, ...]","category":"section"},{"location":"population_scenarios/#Best-Practices","page":"Population Scenarios","title":"Best Practices","text":"","category":"section"},{"location":"population_scenarios/#When-to-Use-Scenarios","page":"Population Scenarios","title":"When to Use Scenarios","text":"Use scenarios when you want:\n\nPopulation-averaged counterfactuals (\"What if everyone had X=value?\")\nPolicy impact assessment with population averaging\nStandardization to common covariate values\nTreatment effect estimation (via prediction differences)\n\nDon't use scenarios when you want:\n\nEffects at specific covariate points → Use profile_margins() with cartesian_grid()\nRepresentative individual analysis → Use profile_margins() with means_grid()\nDetailed covariate combinations → Use profile_margins() with reference grids","category":"section"},{"location":"population_scenarios/#Avoiding-Common-Mistakes","page":"Population Scenarios","title":"Avoiding Common Mistakes","text":"","category":"section"},{"location":"population_scenarios/#Mistake-1:-Confusing-Scenarios-with-Profile-Analysis","page":"Population Scenarios","title":"Mistake 1: Confusing Scenarios with Profile Analysis","text":"# Wrong: Using scenarios for \"effects at x=10\"\n# This computes population-average effect when everyone has x=10 (not what you want)\npopulation_margins(model, data; type=:effects, vars=[:z], scenarios=(x=[10]))\n\n# Right: Use profile analysis for \"effects at x=10\"\nprofile_margins(model, data, cartesian_grid(x=[10]); type=:effects, vars=[:z])","category":"section"},{"location":"population_scenarios/#Mistake-2:-Skip-Rule-Violation","page":"Population Scenarios","title":"Mistake 2: Skip Rule Violation","text":"# Wrong: x appears in both vars and scenarios\n# Result: x will be skipped, only other variables computed\npopulation_margins(model, data; type=:effects, vars=[:x, :z], scenarios=(x=[1, 2]))\n\n# Right: Use predictions to see how outcomes change as x varies\npopulation_margins(model, data; type=:predictions, scenarios=(x=[1, 2]))","category":"section"},{"location":"population_scenarios/#Mistake-3:-Too-Many-Scenario-Combinations","page":"Population Scenarios","title":"Mistake 3: Too Many Scenario Combinations","text":"# Dangerous: 10×10×10×10 = 10,000 scenarios\n# result = population_margins(model, data;\n#     type=:predictions,\n#     scenarios=(a=1:10, b=1:10, c=1:10, d=1:10)\n# )\n\n# Better: Focus on key scenarios\nresult = population_margins(model, data;\n    type=:predictions,\n    scenarios=(a=[1, 5, 10], b=[1, 5, 10])  # 3×3 = 9 scenarios\n)","category":"section"},{"location":"population_scenarios/#Performance-Considerations","page":"Population Scenarios","title":"Performance Considerations","text":"Scenario Count:\n\nEach scenario requires a full population pass\n100 scenarios on 10,000 observations = 1 million evaluations\nKeep scenario counts reasonable (< 100 for most applications)\n\nGrouping Interaction:\n\nScenarios are evaluated within each group\n10 groups × 20 scenarios = 200 computations\nComputational cost is O(groups × scenarios × observations)\n\nMemory:\n\nZero per-row allocations (efficient)\nMemory scales with number of unique scenarios, not observations\nLarge scenario counts use O(scenarios) memory for result storage","category":"section"},{"location":"population_scenarios/#Technical-Implementation-Notes","page":"Population Scenarios","title":"Technical Implementation Notes","text":"","category":"section"},{"location":"population_scenarios/#Statistical-Correctness","page":"Population Scenarios","title":"Statistical Correctness","text":"All scenarios maintain publication-grade statistical validity:\n\nFull delta-method standard errors using complete covariance matrix Σ\nProper gradient averaging across population\nNo independence assumptions\nAccounts for parameter uncertainty","category":"section"},{"location":"population_scenarios/#Computational-Architecture","page":"Population Scenarios","title":"Computational Architecture","text":"Zero-Allocation Design:\n\nScenarios use FormulaCompiler's DataScenario system for efficient variable overrides\nPre-allocated buffers reused across all scenario evaluations\nNo data mutation (original data remains unchanged)\nCategorical types safe (no re-pooling required)\n\nPerformance Characteristics:\n\nO(1) memory per scenario (reuse buffers)\nO(n) time per scenario where n = observations\nScenarios evaluated in parallel when possible\nFD backend: minimal override sets constructed\nAD backend: exact derivatives with zero allocation","category":"section"},{"location":"population_scenarios/#Column-Naming-Convention","page":"Population Scenarios","title":"Column Naming Convention","text":"Results use consistent naming:\n\nGroup variables: Unprefixed (e.g., education, region)\nScenario variables: at_ prefix (e.g., at_treatment, at_dosage)\nStatistical columns: estimate, se, ci_lower, ci_upper, etc.\n\nProgrammatic Access:\n\n# Identify which columns are groups vs scenarios\ngroups, scenarios = Margins.context_columns(result)\n# groups == [:education, ...], scenarios == [:treatment, :dosage, ...]","category":"section"},{"location":"population_scenarios/#Relationship-to-Profile-Analysis","page":"Population Scenarios","title":"Relationship to Profile Analysis","text":"Key Distinction:\n\nPopulation scenarios (population_margins + scenarios): Population-averaged quantities under counterfactual values\nProfile analysis (profile_margins + reference grids): Quantities at specific covariate combinations\n\nWhen to use each:\n\nGoal Method\n\"Average effect if everyone had X=10\" population_margins with scenarios=(X=[10])\n\"Effect at X=10 for a typical individual\" profile_margins with cartesian_grid(X=[10])\n\"Compare predictions across 3 policies\" population_margins with scenarios=(policy=[...])\n\"Effects at high/med/low values of X\" profile_margins with cartesian_grid(X=[...])\n\nRemember: profile_margins() does not accept scenarios parameter. Use reference grid builders (cartesian_grid, means_grid, etc.) instead.","category":"section"},{"location":"population_scenarios/#Further-Reading","page":"Population Scenarios","title":"Further Reading","text":"Grouping Framework - Combining scenarios with groups\nReference Grids - Profile analysis alternative\nAPI Reference - Complete parameter documentation\n\n\n\nPopulation scenarios enable sophisticated counterfactual analysis while maintaining computational efficiency and statistical rigor. They are a powerful tool for policy evaluation, treatment effect estimation, and \"what if\" analysis in econometric research.","category":"section"},{"location":"stata_migration/#Stata-Migration-Guide","page":"Migration Guide","title":"Stata Migration Guide","text":"Complete translation reference for economists migrating from Stata's margins command","category":"section"},{"location":"stata_migration/#Basic-Command-Translation","page":"Migration Guide","title":"Basic Command Translation","text":"","category":"section"},{"location":"stata_migration/#Core-Margins-Commands","page":"Migration Guide","title":"Core Margins Commands","text":"Stata Command Margins.jl Equivalent Notes\nmargins population_margins(model, data; type=:predictions) Average adjusted predictions\nmargins, dydx(*) population_margins(model, data; type=:effects) Average marginal effects (AME)\nmargins, at(means) profile_margins(model, data, means_grid(data); type=:predictions) Predictions at sample means\nmargins, at(means) dydx(*) profile_margins(model, data, means_grid(data); type=:effects) Marginal effects at means (MEM)\nmargins, dydx(*) atmeans profile_margins(model, data, means_grid(data); type=:effects) Alternative MEM syntax","category":"section"},{"location":"stata_migration/#Variable-Selection","page":"Migration Guide","title":"Variable Selection","text":"Stata Command Margins.jl Equivalent Notes\nmargins, dydx(x1 x2) population_margins(model, data; type=:effects, vars=[:x1, :x2]) Specific variables only\nmargins, dydx(_continuous) population_margins(model, data; type=:effects) All continuous variables (automatic)\nmargins, eyex(x1) population_margins(model, data; type=:effects, vars=[:x1], measure=:elasticity) Elasticities","category":"section"},{"location":"stata_migration/#Grouping-and-Stratification","page":"Migration Guide","title":"Grouping and Stratification","text":"","category":"section"},{"location":"stata_migration/#Basic-Grouping","page":"Migration Guide","title":"Basic Grouping","text":"Stata Command Margins.jl Equivalent Notes\nmargins education population_margins(model, data; groups=:education) Group predictions\nmargins education, dydx(*) population_margins(model, data; type=:effects, groups=:education) Group effects\nmargins, over(education) population_margins(model, data; groups=:education) Alternative syntax\nmargins education gender population_margins(model, data; groups=[:education, :gender]) Cross-tabulation\nmargins education#gender population_margins(model, data; groups=[:education, :gender]) Interaction syntax","category":"section"},{"location":"stata_migration/#Nested-Analysis","page":"Migration Guide","title":"Nested Analysis","text":"Stata Command Margins.jl Equivalent Notes\nby region: margins education population_margins(model, data; groups=:region => :education) Nested grouping\nmargins education, over(region) population_margins(model, data; groups=[:education, :region]) Cross-tabulation alternative","category":"section"},{"location":"stata_migration/#Scenario-Analysis-(at()-Specification)","page":"Migration Guide","title":"Scenario Analysis (at() Specification)","text":"","category":"section"},{"location":"stata_migration/#Basic-Scenarios","page":"Migration Guide","title":"Basic Scenarios","text":"Stata Command Margins.jl Equivalent Notes\nmargins, at(x=0) profile_margins(model, data, cartesian_grid(x=[0]); type=:predictions) Single scenario\nmargins, at(x=(0 1 2)) profile_margins(model, data, cartesian_grid(x=[0, 1, 2]); type=:predictions) Multiple values\nmargins, at(x=0 y=1) profile_margins(model, data, cartesian_grid(x=[0], y=[1]); type=:predictions) Multiple variables","category":"section"},{"location":"stata_migration/#Population-Level-Counterfactuals","page":"Migration Guide","title":"Population-Level Counterfactuals","text":"Key Difference: Stata's at() creates evaluation points, while Margins.jl's scenarios creates population counterfactuals.\n\nStata Approach Margins.jl Population Approach Notes\nmargins, at(treatment=(0 1)) population_margins(model, data; scenarios=(treatment=[0, 1])) Everyone untreated vs everyone treated\nmargins education, at(policy=(0 1)) population_margins(model, data; groups=:education, scenarios=(policy=[0, 1])) Policy effects by education","category":"section"},{"location":"stata_migration/#Profile-vs-Population-Interpretation","page":"Migration Guide","title":"Profile vs Population Interpretation","text":"# Stata: margins, at(treatment=(0 1))\n# → Effects at two evaluation points\n\n# Margins.jl Profile Equivalent\nprofile_results = profile_margins(model, data, \n    cartesian_grid(treatment=[0, 1]);\n    type=:effects)\n\n# Margins.jl Population Alternative (often more relevant)  \npopulation_results = population_margins(model, data;\n    scenarios=(treatment=[0, 1]),\n    type=:effects)","category":"section"},{"location":"stata_migration/#Skip-Rule:-dydx(x)-with-over(x)-(and-scenarios)","page":"Migration Guide","title":"Skip Rule: dydx(x) with over(x) (and scenarios)","text":"Unlike Stata, population_margins intentionally skips computing the effect of a variable when that same variable appears in groups (Stata over()) or in scenarios (Stata at()). This avoids the contradiction of “compute the effect of x while holding x fixed” or “using x both as an effect variable and a grouping key.”\n\nRecommended translations:\n\n# 1) Stata: margins, dydx(x) over(x)\n# → Profile-style alternative: evaluate derivatives at specific x values\nmem_like = profile_margins(model, data,\n    cartesian_grid(x=[-2.0, 0.0, 2.0]);\n    type=:effects,\n    vars=[:x])\n\n# 2) Population stratification by x without contradiction:\n#    Create a derived bin variable and group by it, not by :x directly\ndf.x_bin = cut(df.x, 4)  # quartiles via user code; or use groups=(:x, 4)\nby_xbins = population_margins(model, df;\n    type=:effects,\n    vars=[:x],\n    groups=:x_bin)  # allowed since groups variable ≠ :x\n\n# 3) Effects of other variables within x strata (population approach)\neffects_in_xbins = population_margins(model, data;\n    type=:effects,\n    vars=[:z, :w],\n    groups=(:x, 4))\n\n# 4) Counterfactual predictions as x changes (not effects of x)\npreds_under_x = population_margins(model, data;\n    type=:predictions,\n    scenarios=(x=[-2.0, 0.0, 2.0]))\n\nSee also: “Skip Rule” note in the Population Grouping docs for rationale and guidance.","category":"section"},{"location":"stata_migration/#Short-example:-grouping-by-x_bin-to-compute-dydx(x)-across-strata","page":"Migration Guide","title":"Short example: grouping by x_bin to compute dydx(x) across strata","text":"using Random\nusing DataFrames, CategoricalArrays\nusing Statistics  # for quantile\nusing GLM\nusing Margins\n\nRandom.seed!(42)\nn = 500\ndf = DataFrame(\n    y = rand(Bool, n),\n    x = randn(n),\n    z = randn(n)\n)\n\n# Fit a simple model\nm = glm(@formula(y ~ x + z), df, Binomial(), LogitLink())\n\n# Create quartile bins for x as a separate column \"x_bin\"\nedges = quantile(df.x, 0:0.25:1.0)\nlabels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\ndf.x_bin = cut(df.x, edges; labels=labels, extend=true)\n\n# Now compute population AME of x within x_bin strata (no contradiction)\nres = population_margins(m, df;\n    type=:effects,\n    vars=[:x],\n    groups=:x_bin)\n\nDataFrame(res)  # Shows dydx(x) by Q1..Q4","category":"section"},{"location":"stata_migration/#Combined-Grouping-and-Scenarios","page":"Migration Guide","title":"Combined Grouping and Scenarios","text":"","category":"section"},{"location":"stata_migration/#Complex-Analysis-Patterns","page":"Migration Guide","title":"Complex Analysis Patterns","text":"Stata Pattern Margins.jl Equivalent Notes\nmargins education, at(treatment=(0 1)) population_margins(model, data; groups=:education, scenarios=(treatment=[0, 1])) Group × scenario analysis\nMultiple margins commands Single comprehensive call More efficient in Julia","category":"section"},{"location":"stata_migration/#Advanced-Patterns-Beyond-Stata","page":"Migration Guide","title":"Advanced Patterns Beyond Stata","text":"Margins.jl extends far beyond Stata's capabilities with features unavailable in Stata:","category":"section"},{"location":"stata_migration/#Continuous-Variable-Binning","page":"Migration Guide","title":"Continuous Variable Binning","text":"* Stata approach (manual and cumbersome):\ngen income_q = .\n_pctile income, nq(4)\nreplace income_q = 1 if income <= r(r1)\nreplace income_q = 2 if income > r(r1) & income <= r(r2)\nreplace income_q = 3 if income > r(r2) & income <= r(r3)\nreplace income_q = 4 if income > r(r3)\nmargins income_q\n\n# Julia approach (automatic):\npopulation_margins(model, data; groups=(:income, 4))  # Automatic Q1-Q4 quartiles","category":"section"},{"location":"stata_migration/#Custom-Policy-Thresholds","page":"Migration Guide","title":"Custom Policy Thresholds","text":"* Stata approach:\ngen income_bracket = .\nreplace income_bracket = 1 if income < 25000\nreplace income_bracket = 2 if income >= 25000 & income < 50000\nreplace income_bracket = 3 if income >= 50000 & income < 75000\nreplace income_bracket = 4 if income >= 75000\nmargins income_bracket\n\n# Julia approach:\npopulation_margins(model, data; groups=(:income, [25000, 50000, 75000]))","category":"section"},{"location":"stata_migration/#Hierarchical-Grouping","page":"Migration Guide","title":"Hierarchical Grouping","text":"* Stata approach (requires multiple commands or complex by groups):\nby region: margins education\n* No native support for deep nesting\n\n# Julia approach (native hierarchical support):\npopulation_margins(model, data; groups=:region => :education)\npopulation_margins(model, data; groups=:country => (:region => :education))  # Deep nesting","category":"section"},{"location":"stata_migration/#Multi-Variable-Scenarios","page":"Migration Guide","title":"Multi-Variable Scenarios","text":"* Stata approach (requires multiple separate commands):\nmargins, at(treatment=0 policy=0)\nmargins, at(treatment=0 policy=1) \nmargins, at(treatment=1 policy=0)\nmargins, at(treatment=1 policy=1)\n\n# Julia approach (automatic Cartesian product):\npopulation_margins(model, data; scenarios=(treatment=[0, 1], policy=[0, 1]))\n\nNote: scenarios in Julia are population‑level counterfactuals (everyone receives each setting in turn). For Stata’s point‑evaluation semantics of at(), use profile_margins(model, data, reference_grid) with a grid builder (e.g., means_grid, cartesian_grid) or an explicit DataFrame.","category":"section"},{"location":"stata_migration/#Complete-Workflow-Examples","page":"Migration Guide","title":"Complete Workflow Examples","text":"","category":"section"},{"location":"stata_migration/#Example-1:-Education-Policy-Analysis","page":"Migration Guide","title":"Example 1: Education Policy Analysis","text":"","category":"section"},{"location":"stata_migration/#Stata-Workflow","page":"Migration Guide","title":"Stata Workflow","text":"* Fit model\nlogit outcome education income female urban policy_treatment\n\n* Basic effects\nmargins, dydx(*)\n\n* Effects by education\nmargins education, dydx(income)\n\n* Policy scenarios (multiple commands required)\nmargins education, at(policy_treatment=0)\nmargins education, at(policy_treatment=1)\n\n* Manual difference calculation needed for treatment effects","category":"section"},{"location":"stata_migration/#Julia-Workflow","page":"Migration Guide","title":"Julia Workflow","text":"# Fit model  \nmodel = glm(@formula(outcome ~ education + income + female + urban + policy_treatment),\n            data, Binomial(), LogitLink())\n\n# Basic effects\nbasic_effects = population_margins(model, data; type=:effects)\n\n# Effects by education\neducation_effects = population_margins(model, data; \n                                     type=:effects, \n                                     vars=[:income],\n                                     groups=:education)\n\n# Policy scenarios (automatic treatment effect calculation)\npolicy_analysis = population_margins(model, data;\n                                   type=:effects,\n                                   groups=:education,\n                                   scenarios=(:policy_treatment => [0, 1]))\n\n# All results readily available as DataFrames\nDataFrame(policy_analysis)","category":"section"},{"location":"stata_migration/#Example-2:-Complex-Demographic-Analysis","page":"Migration Guide","title":"Example 2: Complex Demographic Analysis","text":"","category":"section"},{"location":"stata_migration/#Stata-Approach-(Cumbersome)","page":"Migration Guide","title":"Stata Approach (Cumbersome)","text":"* Multiple manual commands needed:\nmargins education, over(region)\nmargins gender, over(region)  \nmargins education#gender, over(region)\n\n* Income quartiles require manual creation:\nxtile income_q4 = income, nq(4)\nmargins education, over(income_q4)\n\n* No native support for hierarchical analysis","category":"section"},{"location":"stata_migration/#Julia-Approach-(Comprehensive)","page":"Migration Guide","title":"Julia Approach (Comprehensive)","text":"# Single comprehensive analysis\ncomprehensive_results = population_margins(model, data;\n    type=:effects,\n    groups=:region => [:education, :gender, (:income, 4)]\n)\n\n# Results: Region × (Education + Gender + Income-Quartiles) automatically computed\n# Professional Q1-Q4 labeling included\nDataFrame(comprehensive_results)","category":"section"},{"location":"stata_migration/#Performance-Comparisons","page":"Migration Guide","title":"Performance Comparisons","text":"","category":"section"},{"location":"stata_migration/#Computational-Advantages","page":"Migration Guide","title":"Computational Advantages","text":"Aspect Stata Margins.jl\nComplex grouping Multiple manual commands Single comprehensive call\nScenario analysis Manual looping/multiple commands Automatic Cartesian products\nLarge datasets Memory limitations Efficient O(n) scaling\nCustom thresholds Manual variable creation Automatic binning with labels\nHierarchical analysis Limited native support Unlimited nesting depth","category":"section"},{"location":"stata_migration/#Stata-Command-Count-Reduction","page":"Migration Guide","title":"Stata Command Count Reduction","text":"# This single Julia command:\nresult = population_margins(model, data;\n    groups=:region => [:education, (:income, 4)],\n    scenarios=(treatment=[0, 1], policy=[\"old\", \"new\"])\n)\n\n# Replaces ~30 individual Stata margins commands:\n# 4 regions × 3 education × 4 income × 2 treatment × 2 policy = 192 combinations\n# Plus manual variable creation, looping, and results compilation","category":"section"},{"location":"stata_migration/#Migration-Best-Practices","page":"Migration Guide","title":"Migration Best Practices","text":"","category":"section"},{"location":"stata_migration/#Common-Translation-Patterns","page":"Migration Guide","title":"Common Translation Patterns","text":"# Pattern 1: Simple margins → population_margins\n# margins → population_margins(model, data; type=:predictions)\n\n# Pattern 2: Effects by groups → groups parameter  \n# margins education, dydx(*) → population_margins(model, data; type=:effects, groups=:education)\n\n# Pattern 3: Multiple at() values → scenarios or profile grids\n# margins, at(x=(0 1 2)) → profile_margins(model, data, cartesian_grid(x=[0, 1, 2]))\n# OR population_margins(model, data; scenarios=(x=[0, 1, 2]))  # for counterfactuals\n\n# Pattern 4: Complex manual analysis → comprehensive single call\n# Multiple Stata commands → single population_margins with groups + scenarios","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"Comprehensive workflow examples and implementation patterns","category":"section"},{"location":"examples/#Conceptual-Overview","page":"Examples","title":"Conceptual Overview","text":"","category":"section"},{"location":"examples/#Example-Organization","page":"Examples","title":"Example Organization","text":"This guide demonstrates practical implementation of the two-dimensional analytical framework through concrete examples. Examples progress from basic usage patterns to advanced specification techniques, illustrating both population and profile analysis approaches across diverse econometric applications.","category":"section"},{"location":"examples/#Basic-Implementation","page":"Examples","title":"Basic Implementation","text":"","category":"section"},{"location":"examples/#Fundamental-Usage-Pattern","page":"Examples","title":"Fundamental Usage Pattern","text":"using Random\nusing Margins, DataFrames, GLM\n\n# Generate sample data\nn = 1000\nRandom.seed!(06515)\ndf = DataFrame(\n    y = randn(n),\n    x1 = randn(n), \n    x2 = randn(n),\n    group = rand([\"A\", \"B\", \"C\"], n)\n)\n\n# Fit model\nmodel = lm(@formula(y ~ x1 + x2 + group), df)\n\n# Population average marginal effects (AME)\name_result = population_margins(model, df; type=:effects)\nDataFrame(ame_result)\n\n# Marginal effects at sample means (MEM)  \nmem_result = profile_margins(model, df, means_grid(df); type=:effects)\nDataFrame(mem_result)","category":"section"},{"location":"examples/#Advanced-Implementation-Patterns","page":"Examples","title":"Advanced Implementation Patterns","text":"","category":"section"},{"location":"examples/#Profile-Specification-Methods","page":"Examples","title":"Profile Specification Methods","text":"Margins.jl provides multiple approaches to specify evaluation profiles for profile_margins(), each optimized for different analytical requirements.","category":"section"},{"location":"examples/#1.-Table-Based-Reference-Grid-(Maximum-Control)","page":"Examples","title":"1. Table-Based Reference Grid (Maximum Control)","text":"For exact control over evaluation points, pass a DataFrame directly:\n\nusing DataFrames\n\n# Custom reference grid\nreference_grid = DataFrame(\n    x1 = [-1.0, 0.0, 1.0],\n    x2 = [10, 15, 20],\n    group = [\"A\", \"B\", \"A\"]  # Row order preserved as specified\n)\n\n# Predictions at specific points\npredictions = profile_margins(model, df, reference_grid; type=:predictions)\n\n# Effects at specific points  \neffects = profile_margins(model, df, reference_grid; type=:effects, vars=[:x1, :x2])","category":"section"},{"location":"examples/#2.-Cartesian-Product-Specification","page":"Examples","title":"2. Cartesian Product Specification","text":"For systematic scenario construction, use grid builders to specify value combinations:\n\n# Systematic scenario grid (Cartesian product)\nscenarios = cartesian_grid(x1=[-1.0, 0.0, 1.0], x2=[10, 20], group=[\"A\", \"B\"])  \n# Creates 3×2×2 = 12 evaluation points\n\n# Effects across all scenarios\nscenario_effects = profile_margins(model, df, scenarios; type=:effects, vars=[:x1])\nDataFrame(scenario_effects)","category":"section"},{"location":"examples/#3.-At-Sample-Means-(Most-Common)","page":"Examples","title":"3. At Sample Means (Most Common)","text":"For representative case analysis:\n\n# Effects at sample means - most interpretable approach\nmeans_effects = profile_margins(model, df, means_grid(df); type=:effects)\n\n# Predictions at sample means\nmeans_predictions = profile_margins(model, df, means_grid(df); type=:predictions)","category":"section"},{"location":"examples/#4.-Explicit-Profile-Tables","page":"Examples","title":"4. Explicit Profile Tables","text":"For irregular or custom evaluation points, pass an explicit DataFrame:\n\ncustom_profiles = DataFrame(\n    x1 = [-1.0, 0.0, 1.0],\n    x2 = [10, 15, 20],\n    group = [\"A\", \"B\", \"A\"]\n)\n\nresults = profile_margins(model, df, custom_profiles; type=:effects)","category":"section"},{"location":"examples/#5.-Categorical-Mixtures-for-Policy-Analysis","page":"Examples","title":"5. Categorical Mixtures for Policy Analysis","text":"For realistic population scenarios using categorical mixtures:\n\nusing CategoricalArrays\n\n# Realistic policy scenarios with population composition\nmixture_grid = DataFrame(group=[mix(\"A\" => 0.5, \"B\" => 0.3, \"C\" => 0.2)])\npolicy_scenario = profile_margins(model, df, mixture_grid; type=:predictions)\n\n# Multiple policy scenarios\npolicy_grid = DataFrame(\n    x1 = [0, 1],  # Policy intervention levels\n    group = [mix(\"A\" => 0.6, \"B\" => 0.4), mix(\"A\" => 0.6, \"B\" => 0.4)]\n)\npolicy_effects = profile_margins(model, df, policy_grid; type=:effects)","category":"section"},{"location":"examples/#Economic-Analysis-Workflow","page":"Examples","title":"Economic Analysis Workflow","text":"","category":"section"},{"location":"examples/#Wage-Determination-Analysis","page":"Examples","title":"Wage Determination Analysis","text":"Complete econometric workflow using human capital theory:\n\nusing GLM, CategoricalArrays, Random\n\n# Generate realistic econometric dataset\nRandom.seed!(06515)\nn = 2000\n\ndata = DataFrame(\n    # Demographics\n    age = rand(25:65, n),\n    female = rand([0, 1], n),\n    education = categorical(rand([\"HS\", \"College\", \"Graduate\"], n)),\n    \n    # Economic variables\n    experience = rand(0:40, n),\n    urban = rand([0, 1], n),\n    unemployment_rate = rand(3.0:0.1:12.0, n)\n)\n\n# Generate realistic log wages\neducation_effects = Dict(\"HS\" => 0.0, \"College\" => 0.4, \"Graduate\" => 0.8)\nedu_numeric = [education_effects[string(edu)] for edu in data.education]\n\ndata.log_wage = 1.5 .+ \n                0.05 .* data.age .+ \n                edu_numeric .+ \n                0.02 .* data.experience .- \n                0.15 .* data.female .+ \n                0.10 .* data.urban .- \n                0.03 .* data.unemployment_rate .+ \n                0.3 .* randn(n)\n\n# Fit wage equation\nwage_model = lm(@formula(log_wage ~ age + education + experience + \n                        female + urban + unemployment_rate), data)","category":"section"},{"location":"examples/#Population-Analysis","page":"Examples","title":"Population Analysis","text":"# Population average marginal effects\name_results = population_margins(wage_model, data; type=:effects)\nprintln(\"Population Average Marginal Effects:\")\nprintln(DataFrame(ame_results))\n\n# Effects by gender subgroups  \ngender_effects = population_margins(wage_model, data; \n                                  type=:effects, \n                                  groups=:female)\nprintln(\"Effects by gender:\")\nprintln(DataFrame(gender_effects))","category":"section"},{"location":"examples/#Profile-Analysis","page":"Examples","title":"Profile Analysis","text":"# Effects at sample means (representative person)\nmem_results = profile_margins(wage_model, data, means_grid(data); type=:effects)\nprintln(\"Effects for typical person:\")\nprintln(DataFrame(mem_results))\n\n# Policy scenarios: education and unemployment effects\npolicy_grid = cartesian_grid(education=[\"HS\", \"College\", \"Graduate\"],\n                             unemployment_rate=[3.0, 6.0, 9.0])\npolicy_analysis = profile_margins(wage_model, data, policy_grid; type=:predictions)\nprintln(\"Policy scenario predictions:\")\nprintln(DataFrame(policy_analysis))","category":"section"},{"location":"examples/#Logistic-Regression-Example","page":"Examples","title":"Logistic Regression Example","text":"Binary outcome analysis with proper probability interpretation:\n\n# Generate binary outcome data\ndata.manager = [rand() < (1/(1+exp(-(-1.0 + 0.03*age + 0.5*edu + 0.02*exp - 0.3*fem)))) ? 1 : 0 \n                for (age,edu,exp,fem) in zip(data.age, edu_numeric, data.experience, data.female)]\n\n# Fit logistic model\nlogit_model = glm(@formula(manager ~ age + education + experience + female), \n                  data, Binomial(), LogitLink())\n\n# Effects on probability scale (most interpretable)\nprob_effects = population_margins(logit_model, data; \n                                type=:effects, \n                                scale=:response)\nprintln(\"Effects on probability of management position:\")\nprintln(DataFrame(prob_effects))\n\n# Gender gap analysis across education levels\ngender_grid = cartesian_grid(education=[\"HS\", \"College\", \"Graduate\"], female=[0, 1])\ngender_gap = profile_margins(logit_model, data, gender_grid;\n    type=:predictions, scale=:response)\nprintln(\"Gender gap in management probability by education:\")\nprintln(DataFrame(gender_gap))","category":"section"},{"location":"examples/#Elasticity-Analysis","page":"Examples","title":"Elasticity Analysis","text":"","category":"section"},{"location":"examples/#Basic-Elasticity-Computation","page":"Examples","title":"Basic Elasticity Computation","text":"# Population average elasticities\nelasticities = population_margins(wage_model, data; \n                                type=:effects, \n                                measure=:elasticity,\n                                vars=[:age, :experience])\nprintln(\"Population average elasticities:\")\nprintln(DataFrame(elasticities))\n\n# Elasticities at different education levels\nedu_grid = cartesian_grid(education=[\"HS\", \"College\", \"Graduate\"]) \nedu_elasticities = profile_margins(wage_model, data, edu_grid;\n    type=:effects, measure=:elasticity, vars=[:age, :experience])\nprintln(\"Elasticities by education level:\")\nprintln(DataFrame(edu_elasticities))","category":"section"},{"location":"examples/#Semi-Elasticities","page":"Examples","title":"Semi-Elasticities","text":"# Semi-elasticity: % change in wages per unit change in unemployment\nunemployment_semi = population_margins(wage_model, data;\n                                     measure=:semielasticity_dyex,\n                                     vars=[:unemployment_rate])\nprintln(\"Unemployment semi-elasticity (% wage change per point):\")\nprintln(DataFrame(unemployment_semi))","category":"section"},{"location":"examples/#Advanced-Features","page":"Examples","title":"Advanced Features","text":"","category":"section"},{"location":"examples/#Advanced-Grouping-and-Stratification","page":"Examples","title":"Advanced Grouping and Stratification","text":"# Basic categorical grouping\nurban_analysis = population_margins(wage_model, data; \n                                  type=:effects, \n                                  groups=:urban)\n\n# Cross-tabulated grouping\neducation_urban = population_margins(wage_model, data; \n                                   type=:effects, \n                                   groups=[:education, :urban])\n\n# Hierarchical grouping: education → urban within each education level\nnested_analysis = population_margins(wage_model, data;\n                                   type=:effects,\n                                   groups=:education => :urban)\n\n# Continuous binning: age quartiles\nage_quartiles = population_margins(wage_model, data;\n                                 type=:effects,\n                                 groups=(:age, 4))\n\n# Custom thresholds for policy analysis\nincome_thresholds = population_margins(wage_model, data;\n                                     type=:effects,\n                                     groups=(:log_wage, [2.0, 2.5, 3.0]))\n\n# Mixed categorical and continuous\ncomplex_groups = population_margins(wage_model, data;\n                                  type=:effects,\n                                  groups=[:education, (:age, 4)])","category":"section"},{"location":"examples/#Counterfactual-Scenario-Analysis","page":"Examples","title":"Counterfactual Scenario Analysis","text":"","category":"section"},{"location":"examples/#Skip-Rule-Example:-dydx(x)-across-x-strata-using-a-derived-bin-variable","page":"Examples","title":"Skip Rule Example: dydx(x) across x strata using a derived bin variable","text":"using Statistics\nusing CategoricalArrays\n\n# Suppose we want dydx(age) across age strata without holding age fixed or using it as the grouping key directly.\n# Create an \"age_bin\" column (quartiles), then group by that derived column:\nedges = quantile(data.age, 0:0.25:1.0)\nlabels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\ndata.age_bin = cut(data.age, edges; labels=labels, extend=true)\n\nage_effects_by_bin = population_margins(wage_model, data;\n    type=:effects,\n    vars=[:age],\n    groups=:age_bin)\n\nDataFrame(age_effects_by_bin)\n\n# Policy scenarios: unemployment rate effects\nrecession_scenarios = population_margins(wage_model, data;\n                                       type=:effects,\n                                       scenarios=(:unemployment_rate => [3.0, 6.0, 12.0]))\n\n# Combined grouping and scenarios\neducation_recession = population_margins(wage_model, data;\n                                       type=:effects,\n                                       groups=:education,\n                                       scenarios=(:unemployment_rate => [3.0, 12.0]))\n\n# Multi-variable scenarios\ncomplex_policy = population_margins(wage_model, data;\n                                  type=:effects,\n                                  scenarios=(:urban => [0, 1], \n                                               :unemployment_rate => [3.0, 9.0]))","category":"section"},{"location":"examples/#Robust-Standard-Errors","page":"Examples","title":"Robust Standard Errors","text":"using CovarianceMatrices\n\n# Heteroskedasticity-robust standard errors (HC1)\nrobust_effects = population_margins(wage_model, data; vcov=HC1(), type=:effects)\nprintln(\"Robust standard errors:\")\nprintln(DataFrame(robust_effects))","category":"section"},{"location":"examples/#Performance-Comparison","page":"Examples","title":"Performance Comparison","text":"using BenchmarkTools\n\n# Profile margins: O(1) constant time\nprintln(\"Profile margins performance (constant time):\")\n@btime profile_margins($wage_model, $data, means_grid($data); type=:effects)\n\n# Population margins: O(n) scaling  \nprintln(\"Population margins performance (scales with n):\")\n@btime population_margins($wage_model, $data; type=:effects)\n\n# Complex scenario analysis (still O(1) for profiles)\ncomplex_scenarios = cartesian_grid(age=[25, 35, 45, 55],\n                                   education=[\"HS\", \"College\", \"Graduate\"],\n                                   urban=[0, 1])\nprintln(\"Complex scenario performance (24 profiles, still O(1)):\")\n@btime profile_margins($wage_model, $data, $complex_scenarios; type=:effects)","category":"section"},{"location":"examples/#Stata-Migration-Examples","page":"Examples","title":"Stata Migration Examples","text":"Direct equivalency for economists familiar with Stata:\n\n# Stata: margins, dydx(*)\nstata_ame = population_margins(wage_model, data; type=:effects)\n\n# Stata: margins, at(means) dydx(*)  \nstata_mem = profile_margins(wage_model, data, means_grid(data); type=:effects)\n\n# Stata: margins, at(age=(25 35 45) education=(1 2 3))\nstata_grid = cartesian_grid(age=[25, 35, 45], education=[\"HS\", \"College\", \"Graduate\"]) \nstata_scenarios = profile_margins(wage_model, data, stata_grid; type=:effects)\n\n# Stata: margins, over(female)\nstata_subgroups = population_margins(wage_model, data; \n                                   type=:effects, \n                                   groups=:female)","category":"section"},{"location":"examples/#MixedModels.jl-Examples","page":"Examples","title":"MixedModels.jl Examples","text":"Minimal linear and generalized linear mixed models with population analysis.\n\n# Illustrative example (not executed in docs CI): MixedModels integration\nusing Random\nusing DataFrames, CategoricalArrays, MixedModels, StatsModels, Margins\n\n# Synthetic random-intercept dataset\nRandom.seed!(42)\nn_groups = 20; n_per = 30; n = n_groups * n_per\ngroup = repeat(1:n_groups, inner=n_per)\nx = randn(n)\nu = randn(n_groups)  # random intercepts\ny = 1.0 .+ 0.5 .* x .+ u[group] .+ 0.2 .* randn(n)\ndf = DataFrame(y=y, x=x, group=categorical(string.(group)))\n\n# Linear mixed model\nlmm = fit(MixedModel, @formula(y ~ 1 + x + (1 | group)), df)\n\n# Population AME for x (averaged across sample distribution)\name_lmm = population_margins(lmm, df; type=:effects, vars=[:x])\n\n# Generalized linear mixed model (binary outcome)\nη = -0.5 .+ 1.2 .* x .+ u[group]\np = 1.0 ./ (1 .+ exp.(-η))\nybin = rand.(Bernoulli.(p))\ndf_bin = DataFrame(y=ybin, x=x, group=df.group)\n\nglmm = GeneralizedLinearMixedModel(@formula(y ~ 1 + x + (1 | group)), df_bin, Binomial()) |> fit!\n\n# Probability-scale effects\nprob_effects_glmm = population_margins(glmm, df_bin; type=:effects, vars=[:x], scale=:response)","category":"section"},{"location":"examples/#Best-Practices","page":"Examples","title":"Best Practices","text":"","category":"section"},{"location":"examples/#When-to-Use-Population-vs-Profile","page":"Examples","title":"When to Use Population vs Profile","text":"Choose Population Analysis When:\n\nEstimating true average effects across your sample\nSample heterogeneity is important for policy\nExternal validity to similar populations is the goal\nBroad policy applications affecting diverse groups\n\nChoose Profile Analysis When:\n\nUnderstanding specific, concrete scenarios  \nCommunicating results to non-technical audiences\nSample is relatively homogeneous\nPolicy targets specific demographic profiles","category":"section"},{"location":"examples/#Performance-Guidelines","page":"Examples","title":"Performance Guidelines","text":"# For large datasets (>100k observations)\n# Profile margins remain fast regardless of size\nlarge_data_profiles = profile_margins(model, large_data, means_grid(large_data); type=:effects)\n\n# Population margins scale linearly - use selectively for very large data\nkey_population_effects = population_margins(model, large_data; \n                                          vars=[:key_variable], \n                                          type=:effects)","category":"section"},{"location":"examples/#Error-Handling-Best-Practices","page":"Examples","title":"Error Handling Best Practices","text":"Important: Margins.jl follows an error-first philosophy - when statistical correctness cannot be guaranteed, the package errors explicitly rather than producing potentially invalid results. This ensures users are aware of problems rather than receiving plausible-but-wrong statistical output.\n\n# GOOD: Let errors propagate to inform users of issues\nfunction analyze_margins(model, data, vars)\n    # Errors will propagate with clear messages\n    result = population_margins(model, data; type=:effects, vars=vars)\n    return DataFrame(result)\nend\n\n# GOOD: Validate inputs before computation\nfunction safe_margins_analysis(model, data, vars)\n    # Check that variables exist in data\n    data_vars = Set(Symbol.(names(data)))\n    missing_vars = setdiff(Set(vars), data_vars)\n\n    if !isempty(missing_vars)\n        error(\"Variables not found in data: $(collect(missing_vars))\")\n    end\n\n    # Let computation errors propagate naturally\n    return population_margins(model, data; type=:effects, vars=vars)\nend\n\n# BAD: Silent fallbacks violate error-first philosophy\n# function bad_margins_analysis(model, data)\n#     try\n#         return population_margins(model, data; backend=:ad)\n#     catch e\n#         @warn \"AD failed, using FD\"  # User doesn't know why AD failed!\n#         return population_margins(model, data; backend=:fd)  # May produce different results!\n#     end\n# end\n#\n# Why this is bad:\n# 1. Silently switches backends without user awareness\n# 2. May hide underlying data quality issues\n# 3. Results from AD vs FD may differ slightly\n# 4. Violates principle: \"Error out rather than approximate\"\n\nGuideline: If you encounter errors during marginal effects computation, investigate and fix the root cause rather than implementing silent fallbacks. Common issues include:\n\nDomain errors (use variables that stay positive for log/sqrt)\nMissing variables (validate inputs before computation)\nModel convergence issues (check model fit quality)\nData quality problems (check for NaN/Inf values)\n\n\n\nThese examples demonstrate the full range of Margins.jl capabilities. For detailed API documentation, see API Reference. For performance optimization, see Performance Guide.","category":"section"},{"location":"#Margins.jl","page":"Introduction","title":"Margins.jl","text":"Marginal effects for Julia","category":"section"},{"location":"#Overview","page":"Introduction","title":"Overview","text":"Margins.jl provides a general and high-performance framework for marginal effects analysis in julia. The package is organized around both population- and profile-based approaches. The computational architecture achieves constant-time performance for profile analysis with correct uncertainty handling.\n\nThe package integrates seamlessly with the established JuliaStats ecosystem, providing compatibility with StatsModels.jl for model specification, GLM.jl for generalized linear models, and CovarianceMatrices.jl for robust standard errors. The implementation builds upon FormulaCompiler.jl to achieve efficient and accurate results.","category":"section"},{"location":"#Implementation-Overview","page":"Introduction","title":"Implementation Overview","text":"using Random\nusing CategoricalArrays, DataFrames, GLM, Margins\n\n# Generate sample data\nn = 1000\nRandom.seed!(06515)\ndf = DataFrame(\n    y = randn(n),\n    x1 = randn(n), \n    x2 = randn(n),\n    group = categorical(rand([\"A\", \"B\", \"C\"], n))\n)\n\n# Fit model\nmodel = lm(@formula(y ~ x1 + x2 + group), df)\n\n# Population analysis: effects averaged across sample distribution\name_result = population_margins(model, df; type=:effects)\nDataFrame(ame_result)\n\n# Profile analysis: effects at representative points\nmem_result = profile_margins(model, df, means_grid(df); type=:effects)\nDataFrame(mem_result)","category":"section"},{"location":"#Methodological-Framework","page":"Introduction","title":"Methodological Framework","text":"Marginal Effects Framework Overview  This package addresses two core questions in marginal effects analysis:\n\nEffects: \"How much does Y change when I change X?\" (like: \"How much do wages increase per year of education?\")\nPredictions: \"What value of Y should I expect for specific values of X?\" (like: \"What wage should I expect for someone with 16 years of education?\")\n\nThe methodological foundation of Margins.jl rests upon a two-dimensional framework that hinges on the evaluation context and the analytical target.","category":"section"},{"location":"#Evaluation-Context","page":"Introduction","title":"Evaluation Context","text":"The choice of evaluation context determines the distributional properties of the marginal effects estimates. Population analysis computes effects averaged across the observed sample distribution, yielding estimates that reflect the heterogeneity present in the data generating process. Profile analysis evaluates effects at specific covariate combinations, providing inference at representative or theoretically meaningful points in the covariate space.","category":"section"},{"location":"#Analytical-Targets","page":"Introduction","title":"Analytical Targets","text":"The analytical target specifies the statistical quantity of interest within the chosen evaluation context. Effects analysis computes marginal effects through appropriate differentiation of the conditional expectation function, utilizing analytical derivatives for continuous variables and discrete contrasts for categorical variables. Predictions analysis evaluates adjusted predictions, providing fitted values that incorporate the full uncertainty structure of the estimated model.","category":"section"},{"location":"#Analytical-Framework-Implementation","page":"Introduction","title":"Analytical Framework Implementation","text":"# Population Analysis: Sample Distribution Averaging\npopulation_margins(model, data; type=:effects) # Average Marginal Effects\npopulation_margins(model, data; type=:predictions) # Average Adjusted Predictions\n\n# Profile Analysis: Representative Point Evaluation\nprofile_margins(model, data, means_grid(data); type=:effects) # Effects at Sample Means\nprofile_margins(model, data, means_grid(data); type=:predictions) # Predictions at Sample Means","category":"section"},{"location":"#Computational-Architecture-and-Statistical-Properties","page":"Introduction","title":"Computational Architecture and Statistical Properties","text":"","category":"section"},{"location":"#Performance-Characteristics","page":"Introduction","title":"Performance Characteristics","text":"The computational implementation achieves constant-time complexity for profile analysis through optimized evaluation algorithms that scale independently of dataset size. Population analysis exhibits linear scaling with respect to sample size while maintaining minimal per-observation computational overhead through zero-allocation implementations built upon FormulaCompiler.jl. The architecture has been empirically validated across datasets ranging from small-scale experimental studies to large administrative datasets exceeding one million observations. Detailed performance analysis is provided in the Performance Guide.","category":"section"},{"location":"#Statistical-Inference-Framework","page":"Introduction","title":"Statistical Inference Framework","text":"Standard errors are calculated via the delta-method with full integration of the model's covariance matrix structure. Statistical computations have been validated with tests.","category":"section"},{"location":"#Extended-Analytical-Capabilities","page":"Introduction","title":"Extended Analytical Capabilities","text":"We include methods for second differences (interaction analyses) that allow researchers to smoothly analyze effect heterogeneity across moderator levels. The package supports elasticity analysis through specification of effect measures, and includes both standard elasticities and semi-elasticity variants for both dependent and independent variable transformations. We also support categorical mixture specifications for handling typical values of categorical and binary variables. Furthermore, the framework accommodates robust and clustered standard error computation through integration with CovarianceMatrices.jl. while flexible subgroup analysis capabilities facilitate stratified inference across multiple dimensions of heterogeneity.\n\ncf. Advanced Features and Second Differences for these and other features.","category":"section"},{"location":"#Implementation-Examples","page":"Introduction","title":"Implementation Examples","text":"","category":"section"},{"location":"#Profile-Specification","page":"Introduction","title":"Profile Specification","text":"Multiple ways to specify evaluation points:\n\n# At sample means (most common)\nprofile_margins(model, data, means_grid(data); type = :effects)\n\n# Custom scenarios\nscenarios = cartesian_grid(x1 = [0, 1, 2], group = [\"A\", \"B\"]) \nprofile_margins(model, data, scenarios; type=:effects)\n\n# Complex realistic scenarios with categorical mixtures\nusing CategoricalArrays\nmixture_grid = DataFrame(group = [mix(\"A\" => 0.5, \"B\" => 0.3, \"C\" => 0.2)])\nprofile_margins(model, data, mixture_grid; type = :predictions)\n\n# Pre-built reference grids (maximum control)\nreference_grid = DataFrame(\n  x1 = [0, 1], x2 = [0, 0], group = [\"A\", \"A\"]\n)\nprofile_margins(model, data, reference_grid; type = :effects)","category":"section"},{"location":"#Elasticity-Analysis","page":"Introduction","title":"Elasticity Analysis","text":"# Population average elasticities\npopulation_margins(model, data; type = :effects, measure = :elasticity)\n\n# Elasticities at representative scenarios  \nprofile_margins(\n  model, data, means_grid(data); type = :effects, measure = :elasticity\n)\n\n# Semi-elasticities:\n# change Y per % change X\npopulation_margins(model, data; type = :effects, measure = :semielasticity_dyex)\n# % change Y per unit X\npopulation_margins(model, data; type = :effects, measure = :semielasticity_eydx)  ","category":"section"},{"location":"#Subgroup-Analysis","page":"Introduction","title":"Subgroup Analysis","text":"# Effects by categorical groups\npopulation_margins(model, data; type = :effects, groups = :region)\n\n# Multiple grouping variables\npopulation_margins(\n  model, data; type = :effects, groups = [:region, :year]\n)\n\n# Complex nested grouping\npopulation_margins(\n  model, data; type = :effects, groups = [:region, :income_quartile]\n)","category":"section"},{"location":"#Second-Differences-(Interaction-Effects)","page":"Introduction","title":"Second Differences (Interaction Effects)","text":"# Compute AMEs across modifier levels\names = population_margins(\n  model, data; scenarios = (treated = [0,1],), type = :effects\n)\n\n# Calculate second differences (does X's effect depend on treatment?)\nsd = second_differences(ames, :age, :treated, vcov(model))\nDataFrame(sd)\n\n# Categorical moderators with multiple levels\names = population_margins(\n  model, data;\n  scenarios = (education = [\"hs\",\"college\",\"grad\"],),\n  type = :effects\n)\nsd = second_differences(ames, :income, :education, vcov(model))","category":"section"},{"location":"#Integration-with-JuliaStats","page":"Introduction","title":"Integration with JuliaStats","text":"","category":"section"},{"location":"#Model-Compatibility","page":"Introduction","title":"Model Compatibility","text":"The package provides comprehensive compatibility with models following the StatsModels.jl interface specifications. Support encompasses linear models, logistic regression, Poisson models, and other generalized linear model families through GLM.jl integration. Mixed effects modeling is accommodated through MixedModels.jl compatibility for both linear and generalized linear mixed model specifications. Extensions to custom model types are supported provided they implement the standard coef() and vcov() accessor methods and rely on the @formula approach.","category":"section"},{"location":"#Data-Integration-Framework","page":"Introduction","title":"Data Integration Framework","text":"Data handling utilizes the Tables.jl interface to ensure compatibility with diverse data sources including DataFrames, CSV files, and database result sets. The specialized result types (EffectsResult and PredictionsResult) implement the Tables.jl protocol with conversion to DataFrame format for downstream analysis and reporting.","category":"section"},{"location":"#Robust-Standard-Errors","page":"Introduction","title":"Robust Standard Errors","text":"# Using CovarianceMatrices.jl\nusing CovarianceMatrices\n\n# Robust standard errors (HC1)\npopulation_margins(model, data; vcov = HC1())\n\n# Clustered standard errors\npopulation_margins(\n  model, data; vcov = Clustered(:firm_id)\n)","category":"section"},{"location":"#Computational-Performance-Analysis","page":"Introduction","title":"Computational Performance Analysis","text":"","category":"section"},{"location":"#Constant-Time-Profile-Evaluation","page":"Introduction","title":"Constant-Time Profile Evaluation","text":"Profile analysis achieves computational complexity independent of dataset size through optimized algorithms that evaluate marginal effects at specified covariate combinations without full dataset traversal. This constant-time property holds across diverse scenario specifications, enabling efficient analysis of complex policy counterfactuals and sensitivity analyses regardless of the underlying sample size.\n\n# Profile margins exhibit O(1) complexity characteristics\n# baseline timing:\n@time profile_margins(model, small_data, means_grid(small_data))\n# identical complexity:\n@time profile_margins(model, large_data, means_grid(large_data))\n\n# Complex scenario specifications maintain constant-time properties:\nscenarios = (\n  x1 = [0, 1, 2], x2 = [10, 20, 30], group = [\"A\", \"B\"]\n) # 18 profiles\nscenarios = cartesian_grid(\n  x1 = [0, 1, 2], x2 = [10, 20, 30], group = [\"A\",\"B\"]\n) # 18 profiles\n# remains constant time: \n@time profile_margins(model, huge_data, scenarios) ","category":"section"},{"location":"#Linear-Scaling-in-Population-Analysis","page":"Introduction","title":"Linear Scaling in Population Analysis","text":"Population analysis exhibits linear scaling characteristics with respect to sample size while maintaining minimal per-observation computational overhead through zero-allocation implementations. The computational architecture ensures predictable performance scaling suitable for large-scale statistical applications and administrative dataset analysis.\n\n# Population margins demonstrate O(n) scaling with optimized\n# per-row processing:\n@time population_margins(model, data_1k) # baseline linear scaling\n@time population_margins(model, data_10k) # proportional computational cost\n@time population_margins(model, data_100k) # maintained efficiency at scale","category":"section"},{"location":"#Documentation-Organization","page":"Introduction","title":"Documentation Organization","text":"","category":"section"},{"location":"#Conceptual-Foundation","page":"Introduction","title":"Conceptual Foundation","text":"Mathematical Foundation: Theoretical basis and statistical properties\nSecond Differences: Interaction effects and effect heterogeneity\nComparison Guide: Methodological comparison with alternative approaches","category":"section"},{"location":"#Implementation-Reference","page":"Introduction","title":"Implementation Reference","text":"API Reference: Complete function specifications and parameters\nPerformance Guide: Computational characteristics and benchmarks\nExamples: Executable workflows and application demonstrations","category":"section"},{"location":"#Migration-and-Integration","page":"Introduction","title":"Migration and Integration","text":"Stata Migration: Command equivalence and workflow translation\nAdvanced Features: Extended analytical capabilities\n\nTechnical support and bug reports should be directed to the GitHub Issues repository.","category":"section"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"using Pkg\nPkg.add(\"Margins\")\n\nRequirements: Julia ≥ 1.10\n\nNote on documentation versions:\n\nThe “dev” docs reflect the latest commits on main and update after each successful docs build.\nThe “stable” docs update when a new tagged release is published. If you don’t see your recent changes on “stable”, check the “dev” docs or tag a release to promote changes to “stable”.","category":"section"},{"location":"#Citation","page":"Introduction","title":"Citation","text":"If you use Margins.jl in your research, please cite:\n\n@misc{feltham_formulacompilerjl_2026,\n  title = {{FormulaCompiler}.Jl and {Margins}.Jl: {Efficient Marginal Effects} in {Julia}},\n  shorttitle = {{FormulaCompiler}.Jl and {Margins}.Jl},\n  author = {Feltham, Eric},\n  year = {2026},\n  month = jan,\n  number = {arXiv:2601.07065},\n  eprint = {2601.07065},\n  primaryclass = {stat},\n  publisher = {arXiv},\n  doi = {10.48550/arXiv.2601.07065},\n  urldate = {2026-01-13},\n  abstract = {Marginal effects analysis is fundamental to interpreting statistical models, yet existing implementations face computational constraints that limit analysis at scale. We introduce two Julia packages that address this gap. Margins.jl provides a clean two-function API organizing analysis around a 2-by-2 framework: evaluation context (population vs profile) by analytical target (effects vs predictions). The package supports interaction analysis through second differences, elasticity measures, categorical mixtures for representative profiles, and robust standard errors. FormulaCompiler.jl provides the computational foundation, transforming statistical formulas into zero-allocation, type-specialized evaluators that enable O(p) per-row computation independent of dataset size. Together, these packages achieve 622x average speedup and 460x memory reduction compared to R's marginaleffects package, with successful computation of average marginal effects and delta-method standard errors on 500,000 observations where R fails due to memory exhaustion, providing the first comprehensive and efficient marginal effects implementation for Julia's statistical ecosystem.},\n  archiveprefix = {arXiv},\n  keywords = {Statistics - Computation},\n}","category":"section"}]
}
